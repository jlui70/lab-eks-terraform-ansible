# EKS Express - Infraestrutura AWS Production Grade

Infraestrutura completa para provisionar um **Cluster Amazon EKS production-grade** utilizando **Terraform**, com stacks modulares para gerenciamento de recursos AWS.

Este projeto inclui:
- ‚úÖ **EKS Cluster 1.32** com Node Groups gerenciados
- ‚úÖ **Karpenter** para auto-scaling din√¢mico de nodes
- ‚úÖ **AWS Load Balancer Controller** para Ingress
- ‚úÖ **External DNS** para gerenciamento autom√°tico de DNS
- ‚úÖ **WAF** para prote√ß√£o do Application Load Balancer
- ‚úÖ **Amazon Managed Prometheus + Grafana** para observabilidade
- ‚úÖ **6 stacks Terraform** modulares e reutiliz√°veis
- ‚úÖ **Scripts de automa√ß√£o** para deploy e destroy

---

## üÜï Novidade: Integra√ß√£o com Ansible

Este projeto foi expandido com **documenta√ß√£o completa** para integra√ß√£o com **Ansible**, automatizando a configura√ß√£o de servi√ßos ap√≥s o deployment Terraform.

### **üìö Documenta√ß√£o Ansible Dispon√≠vel:**

1. **[ANALISE-ANSIBLE-INTEGRACAO.md](./docs/ANALISE-ANSIBLE-INTEGRACAO.md)**  
   - An√°lise t√©cnica completa das 5 √°reas onde Ansible agrega valor
   - Pr√°ticas de mercado (Netflix, Spotify, Airbnb)
   - ROI e estimativa de esfor√ßo

2. **[GUIA-IMPLEMENTACAO-ANSIBLE.md](./docs/GUIA-IMPLEMENTACAO-ANSIBLE.md)**  
   - C√≥digo pronto para uso (roles, playbooks)
   - Setup passo a passo
   - Exemplos pr√°ticos

### **üéØ Benef√≠cios da Integra√ß√£o Ansible:**

| Tarefa | Sem Ansible | Com Ansible | Economia |
|--------|-------------|-------------|----------|
| Configurar Grafana | 15-20 min (manual) | 2 min (autom√°tico) | **90%** |
| Deploy sample apps | 10 min (manual) | 1 min (autom√°tico) | **90%** |
| Valida√ß√£o cluster | 15 min (manual) | 1 min (autom√°tico) | **93%** |
| **3 ambientes completos** | **~10 horas** | **~2.5 horas** | **75%** |

__name__="up", instance="10.0.0.100:61678", job="pod_exporter"}
{__name__="up", instance="10.0.0.100:80", job="pod_exporter"}
{__name__="up", instance="10.0.0.100:8162", job="pod_exporter"}
{__name__="up", instance="10.0.0.100:9100", job="pod_exporter"}
{__name__="up", instance="10.0.0.106:3003", job="pod_exporter"}
{__name__="up", instance="10.0.0.107:53", job="pod_exporter"}
{__name__="up", instance="10.0.0.107:9153", job="pod_exporter"}
{__name__="up", instance="10.0.0.109:4000", job="pod_exporter"}
{__name__="up", instance="10.0.0.110:8080", job="pod_exporter"}
{__name__="up", instance="10.0.0.111:80", job="pod_exporter"}
{__name__="up", instance="10.0.0.113:8080", job="pod_exporter"}
{__name__="up", instance="10.0.0.113:8081", job="pod_exporter"}
{__name__="up", instance="10.0.0.115:9090", job="pod_exporter"}
{__name__="up", instance="10.0.0.116:10251", job="pod_exporter"}
{__name__="up", instance="10.0.0.117:80", job="pod_exporter"}---

## üöÄ Fluxo de Deployment Recomendado

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ FASE 1: Terraform (60-90 min)                                   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 1. Stack 00 (Backend)        ‚Üí S3 + DynamoDB                    ‚îÇ
‚îÇ 2. Stack 01 (Networking)     ‚Üí VPC + Subnets + NAT              ‚îÇ
‚îÇ 3. Stack 02 (EKS Cluster)    ‚Üí EKS + Node Group + ALB           ‚îÇ
‚îÇ 4. Stack 03 (Karpenter)      ‚Üí Auto-scaling                     ‚îÇ
‚îÇ 5. Stack 04 (Security/WAF)   ‚Üí WAF WebACL (OPCIONAL - requer apps) ‚îÇ
‚îÇ 6. Stack 05 (Monitoring)     ‚Üí Grafana + Prometheus + API Key   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ FASE 2: Configura√ß√£o Grafana SSO (5-10 min) ‚ö†Ô∏è OBRIGAT√ìRIO     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 1. Habilitar IAM Identity Center (SSO)                          ‚îÇ
‚îÇ 2. Criar usu√°rio SSO                                            ‚îÇ
‚îÇ 3. Atribuir usu√°rio ao Grafana Workspace                        ‚îÇ
‚îÇ 4. ‚ö†Ô∏è MUDAR PARA ADMIN (cr√≠tico!)                               ‚îÇ
‚îÇ 5. Acessar Grafana via AWS Access Portal                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ FASE 3A: Ansible (2 min) - RECOMENDADO                         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ ansible-playbook playbooks/01-configure-grafana.yml             ‚îÇ
‚îÇ   ‚Üí ‚úÖ Data Source Prometheus configurado automaticamente       ‚îÇ
‚îÇ   ‚Üí ‚úÖ Dashboard Node Exporter importado automaticamente        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              OU
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ FASE 3B: Manual (10-15 min) - Alternativa                      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 1. Configurar Data Source Prometheus manualmente                ‚îÇ
‚îÇ 2. Importar Dashboard 1860 manualmente                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ FASE 4: Deploy E-commerce App (OPCIONAL - Demonstra√ß√£o)        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Stack 06 - Aplica√ß√£o real com 7 microservi√ßos                  ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ OP√á√ÉO A - Ansible (3 min): ‚ö° 85% mais r√°pido                  ‚îÇ
‚îÇ   ansible-playbook playbooks/03-deploy-ecommerce.yml            ‚îÇ
‚îÇ   ansible-playbook playbooks/04-configure-ecommerce-monitoring.yml ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ OP√á√ÉO B - Manual (20 min): kubectl apply -f ...                ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ Resultado: App acess√≠vel em eks.devopsproject.com.br           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ ‚úÖ AMBIENTE PRONTO PARA USO + APLICA√á√ÉO DEMO                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**‚ö†Ô∏è PONTOS CR√çTICOS:**
- üî¥ **Stack 05 deve incluir API Key** para Ansible funcionar (ver se√ß√£o "Stack 05")
- üî¥ **Usu√°rio SSO DEVE ser ADMIN** sen√£o Ansible falhar√° com 403 Forbidden
- üî¥ **N√£o pule a Fase 2** (SSO) - Grafana workspace √© criado vazio sem autentica√ß√£o

---

## üìã Pr√©-requisitos

Antes de iniciar o deployment, certifique-se de ter:

- **AWS Account** com permiss√µes administrativas
- **AWS CLI** configurado (vers√£o 2.x recomendada)
- **Terraform** instalado (vers√£o 1.12.x ou superior)
- **kubectl** instalado (vers√£o compat√≠vel com EKS 1.32)
- **Helm** instalado (vers√£o 3.x)
- **Conta AWS Paid Plan** ou cr√©ditos suficientes (Free Tier n√£o suporta inst√¢ncias t3.medium)

> ‚ö†Ô∏è **IMPORTANTE:** O projeto utiliza inst√¢ncias **t3.medium** para os worker nodes. Contas AWS Free Tier s√£o limitadas a t3.micro/t3.small. Certifique-se de ter upgrade para Paid Plan ou cr√©ditos AWS dispon√≠veis.
>
> üí∞ **ESTIMATIVA DE CUSTO PARA LABORAT√ìRIO:**
> - **30 minutos de teste:** ~$0.50 USD
> - **2 horas completas (deploy + valida√ß√£o):** ~$2.00 USD
> - **8 horas (dia de estudo):** ~$8.00 USD
> 
> **üí° DICA:** Execute `terraform destroy` imediatamente ap√≥s os testes para evitar cobran√ßas cont√≠nuas. O custo de ~$280/m√™s mencionado abaixo √© apenas se voc√™ mantiver a infraestrutura rodando 24/7.

---

## üõ†Ô∏è Configura√ß√£o Inicial

### 1. Criar IAM User para Terraform

Crie um usu√°rio IAM na sua conta AWS para realizar o deployment:

**Aten√ß√£o:** Substitua `<YOUR_USER>` pelo nome desejado (ex: `terraform-deploy`).

```bash
aws iam create-user --user-name <YOUR_USER>
```

---

### 2. Criar e Configurar a Role do Terraform

Crie uma Role na sua conta AWS que ser√° assumida pelo Terraform:

**Aten√ß√£o:** Substitua `<YOUR_ACCOUNT>` pelo ID da sua conta AWS e `<YOUR_USER>` pelo usu√°rio criado no passo anterior.

```bash
aws iam create-role \
    --role-name terraform-role \
    --assume-role-policy-document '{
        "Version": "2012-10-17",
        "Statement": [{
            "Effect": "Allow",
            "Principal": {
                "AWS": "arn:aws:iam::<YOUR_ACCOUNT>:user/<YOUR_USER>"
            },
            "Action": "sts:AssumeRole",
            "Condition": {
                "StringEquals": {
                    "sts:ExternalId": "3b94ec31-9d0d-4b22-9bce-72b6ab95fe1a"
                }
            }
        }]
    }'
```

üìå **Observa√ß√£o:** O External ID `3b94ec31-9d0d-4b22-9bce-72b6ab95fe1a` j√° est√° configurado em todos os arquivos do projeto. Voc√™ pode alter√°-lo, mas precisar√° atualizar todos os arquivos `variables.tf`.

---

### 3. Anexar Permiss√µes Administrativas √† Role

```bash
aws iam attach-role-policy \
    --role-name terraform-role \
    --policy-arn arn:aws:iam::aws:policy/AdministratorAccess
```

---

### 4. Configurar AWS CLI Profile

Configure um profile espec√≠fico para o Terraform assumir a role:

**Aten√ß√£o:** Substitua `<YOUR_ACCOUNT>` pelo ID da sua conta AWS.

```bash
aws configure set role_arn arn:aws:iam::<YOUR_ACCOUNT>:role/terraform-role --profile terraform
aws configure set source_profile default --profile terraform
aws configure set external_id 3b94ec31-9d0d-4b22-9bce-72b6ab95fe1a --profile terraform
aws configure set region us-east-1 --profile terraform
```

Teste a configura√ß√£o:

```bash
aws sts get-caller-identity --profile terraform
```

---

## üîß Substitui√ß√µes Necess√°rias nos Arquivos

### 5.1. Substituir `<YOUR_ACCOUNT>` pelo seu Account ID

**CR√çTICO:** Todos os arquivos `.tf` cont√™m o placeholder `<YOUR_ACCOUNT>` que **deve** ser substitu√≠do pelo ID da sua conta AWS.

#### **Obter seu Account ID:**

```bash
aws sts get-caller-identity --query Account --output text --profile terraform
```

Anote o n√∫mero retornado (ex: `123456789012`).

#### üêß **(WSL/Linux)**

```bash
find . -type f -name "*.tf" -exec sed -i \
    's|<YOUR_ACCOUNT>|123456789012|g' {} +
```

#### üçé **(MacOS)**

```bash
find . -type f -name "*.tf" -exec sed -i '' \
    's|<YOUR_ACCOUNT>|123456789012|g' {} +
```

> ‚ö†Ô∏è **ATEN√á√ÉO:** Substitua `123456789012` pelo seu Account ID real obtido no comando acima.

**O que ser√° substitu√≠do:**
- ‚úÖ IAM Role ARN: `arn:aws:iam::<YOUR_ACCOUNT>:role/terraform-role`
- ‚úÖ Bucket S3: `eks-devopsproject-state-files-<YOUR_ACCOUNT>`
- ‚úÖ EKS Access entries (cluster admin)

**Total:** 16 ocorr√™ncias em 10 arquivos `.tf`

---

### 5.2. Verificar EKS Access Configuration (Autom√°tico)

‚úÖ **NENHUMA A√á√ÉO NECESS√ÅRIA!** O arquivo `02-eks-cluster/eks.cluster.access.tf` j√° est√° configurado corretamente para usar a `terraform-role`.

O Terraform automaticamente:
- Detecta seu Account ID via `data.aws_caller_identity`
- Configura access entry para `arn:aws:iam::{ACCOUNT_ID}:role/terraform-role`
- Garante permiss√µes de Cluster Admin para kubectl funcionar

> üí° **Nota:** Se voc√™ encontrar erro `"the server has asked for the client to provide credentials"` ao usar kubectl, verifique se voc√™ est√° usando o profile correto:
> ```bash
> aws sts get-caller-identity --profile terraform
> # Deve retornar AssumedRoleUser com terraform-role
> ```

---

## üöÄ Sequ√™ncia de Deploy

### Stack 00 - Backend (S3 + DynamoDB)

A stack `backend` cria o bucket S3 e a tabela DynamoDB para o Terraform state locking e remote backend:

```bash
cd ./00-backend
terraform init
terraform apply -auto-approve
```

**Recursos criados:** 3 (S3 bucket, S3 versioning, DynamoDB table)

üìå **Observa√ß√£o:** O comando considera que voc√™ est√° na pasta root do projeto.

---

### Stack 01 - Networking (VPC, Subnets, NAT)

Crie a base de redes para as pr√≥ximas stacks:

```bash
cd ../01-networking
terraform init
terraform apply -auto-approve
```

**Recursos criados:** 21 (VPC, Internet Gateway, 6 Subnets, NAT Gateways, Route Tables, EIPs)

**‚è±Ô∏è Tempo estimado:** 2-3 minutos

---

### Stack 02 - EKS Cluster

Crie um Cluster EKS com addons instalados.

**ANTES DE APLICAR:**

1. ‚úÖ Substitua `<YOUR_ACCOUNT>` em todos os arquivos `.tf` (veja se√ß√£o 5.1)
2. ‚úÖ EKS Access j√° est√° configurado automaticamente com terraform-role (veja se√ß√£o 5.2)
3. (Opcional) Ajuste quantidade de worker nodes em `variables.tf` se necess√°rio

```bash
cd ../02-eks-cluster
terraform init
terraform apply -auto-approve
```

**Recursos criados:** 21 (EKS Cluster, Node Group, IAM Roles, Addons, OIDC Provider, ALB Controller, External DNS)

**‚è±Ô∏è Tempo estimado:** 15-20 minutos (inclui provisionamento dos node groups)

---

### Configurar kubectl (OBRIGAT√ìRIO)

Ap√≥s o deploy do Stack 02, configure o kubectl para acessar o cluster:

```bash
aws eks update-kubeconfig \
    --name <CLUSTER_NAME> \
    --region us-east-1 \
    --profile terraform
```

> üìù **Nota:** Substitua `<CLUSTER_NAME>` pelo nome do seu cluster. Se voc√™ n√£o alterou as vari√°veis do Terraform, o nome padr√£o √© `eks-devopsproject-cluster`.

**Exemplo:**
```bash
aws eks update-kubeconfig \
    --name eks-devopsproject-cluster \
    --region us-east-1 \
    --profile terraform
```

Teste o acesso:

```bash
kubectl get nodes
kubectl get pods -A
```

**‚úÖ Valida√ß√£o esperada:**
- 3 nodes no estado `Ready`
- Pods do kube-system rodando
- Pods do aws-load-balancer-controller (2/2 Ready)
- Pods do external-dns (1/1 Ready)

---

### Stack 03 - Karpenter Auto Scaling

Torne o Cluster EKS din√¢mico, adicionando e removendo n√≥s sob demanda utilizando Karpenter:

```bash
cd ../03-karpenter-auto-scaling
terraform init
terraform apply -auto-approve
```

**Recursos criados:** 10 (Karpenter Controller, IAM Roles, Security Group, CRDs, NodePool, EC2NodeClass)

**‚è±Ô∏è Tempo estimado:** 3-5 minutos

**‚úÖ Valida√ß√£o:**

```bash
kubectl get pods -n kube-system | grep karpenter
# Deve mostrar: karpenter-xxxxx  2/2  Running

kubectl get nodepools
# Deve mostrar: default-node-pool  Ready

kubectl get ec2nodeclasses
# Deve mostrar: default  Ready  True
```

---

### Stack 04 - Security (WAF) - OPCIONAL

> üí° **IMPORTANTE:** Este stack √© **opcional** e s√≥ faz sentido ap√≥s deployar aplica√ß√µes que criam ALBs. 
> 
> O WAF protege Application Load Balancers, mas eles s√≥ s√£o criados quando voc√™ cria recursos Ingress no Kubernetes. Se voc√™ ainda n√£o tem aplica√ß√µes deployadas, pode **pular este stack** e voltar depois.

**Quando usar:**
- ‚úÖ Voc√™ j√° deployou aplica√ß√µes com Ingress (que criam ALBs)
- ‚úÖ Voc√™ quer proteger seus ALBs contra ataques web (SQL injection, XSS, rate limiting)

**Se voc√™ n√£o tem aplica√ß√µes ainda:**
- ‚è≠Ô∏è Pule para Stack 05 (Monitoring)
- üîÑ Volte aqui depois de deployar apps

---

#### Passo 4.1: Criar WAF WebACL

#### Passo 4.1: Criar WAF WebACL

```bash
cd ../04-security
terraform init
terraform apply -auto-approve
```

**Recursos criados:** 1 (WAF WebACL)

**‚è±Ô∏è Tempo estimado:** 30 segundos

---

#### Passo 4.2: Criar Ingress Sample (provisionar√° o ALB)

> üìù **Nota:** Este passo cria uma aplica√ß√£o de exemplo apenas para demonstrar a integra√ß√£o WAF + ALB. 
> Em produ√ß√£o, voc√™ associaria o WAF aos ALBs das suas aplica√ß√µes reais.

Antes de associar o WAF ao ALB, √© necess√°rio que um ALB exista. Vamos criar um deployment de teste:

```bash
kubectl apply -f ../02-eks-cluster/samples/ingress-sample-deployment.yml
```

**Aguarde o ALB ser provisionado (~2-3 minutos):**

```bash
kubectl get ingress eks-devopsproject-ingress -n sample-app -w
```

Quando aparecer o endere√ßo do ALB na coluna `ADDRESS`, pressione Ctrl+C.

**Teste o ALB (aguarde DNS propagar ~60-90 segundos):**

```bash
ALB_URL=$(kubectl get ingress eks-devopsproject-ingress -n sample-app -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
curl -I http://$ALB_URL
```

**‚úÖ Esperado:** `HTTP/1.1 200 OK`

> üí° **Automa√ß√£o com Ansible:** Em ambientes de produ√ß√£o, recomendamos automatizar o deploy de aplica√ß√µes e associa√ß√£o do WAF usando Ansible. Veja [GUIA-IMPLEMENTACAO-ANSIBLE.md](./docs/GUIA-IMPLEMENTACAO-ANSIBLE.md) para exemplos.

---

#### Passo 4.3: Associar WAF ao ALB

Agora que o ALB existe, associe o WAF adicionando uma anota√ß√£o ao Ingress.

**Obtenha o ARN do WAF:**

```bash
cd ../04-security
WAF_ARN=$(terraform state show aws_wafv2_web_acl.this | grep "arn " | awk '{print $3}' | tr -d '"')
echo "WAF ARN: $WAF_ARN"
```

**Adicione a anota√ß√£o do WAF ao Ingress:**

```bash
kubectl annotate ingress eks-devopsproject-ingress \
  -n sample-app \
  alb.ingress.kubernetes.io/wafv2-acl-arn="$WAF_ARN" \
  --overwrite
```

**Aguarde o ALB Controller processar (~30-60 segundos):**

```bash
kubectl get ingress eks-devopsproject-ingress -n sample-app -w
```

Quando a coluna `ADDRESS` aparecer novamente (pode piscar), pressione Ctrl+C.

**‚úÖ Valida√ß√£o:**

Verifique se a associa√ß√£o foi criada:

```bash
# Obter ARN do ALB
ALB_ARN=$(aws elbv2 describe-load-balancers \
  --query "LoadBalancers[?contains(LoadBalancerName, 'k8s-sampleap')].LoadBalancerArn" \
  --output text --profile terraform)

# Verificar associa√ß√£o WAF
aws wafv2 get-web-acl-for-resource \
  --resource-arn "$ALB_ARN" \
  --region us-east-1 \
  --profile terraform \
  --query 'WebACL.Name' \
  --output text
```

**Esperado:** `waf-eks-devopsproject-webacl`

Ou verifique no AWS Console:
1. Acesse: https://console.aws.amazon.com/wafv2/home?region=us-east-1
2. Clique em **Web ACLs** ‚Üí `waf-eks-devopsproject-webacl`
3. Na aba **Associated AWS resources**, voc√™ ver√° o ALB listado

---

### ü§ñ Automatizando WAF com Ansible (Recomendado para Produ√ß√£o)

Os passos manuais acima s√£o √∫teis para **demonstra√ß√£o e aprendizado**, mas em produ√ß√£o recomendamos automatizar:

**Por que automatizar?**
- ‚úÖ Evita passos manuais repetitivos
- ‚úÖ Garante consist√™ncia entre ambientes (dev/staging/prod)
- ‚úÖ Permite CI/CD completo
- ‚úÖ Reduz erros humanos

**Como fazer:**

Crie um playbook Ansible que:
1. Deploya sua aplica√ß√£o com Ingress
2. Aguarda o ALB ser provisionado
3. Associa automaticamente o WAF ao ALB

**Exemplo b√°sico:**

```yaml
# ansible/playbooks/deploy-app-with-waf.yml
- name: Deploy aplica√ß√£o com WAF
  hosts: localhost
  tasks:
    - name: Deploy aplica√ß√£o
      kubernetes.core.k8s:
        state: present
        src: ../k8s/my-app-ingress.yml
    
    - name: Aguardar ALB ser criado
      kubernetes.core.k8s_info:
        kind: Ingress
        name: my-app-ingress
        namespace: production
      register: ingress
      until: ingress.resources[0].status.loadBalancer.ingress is defined
      retries: 30
      delay: 10
    
    - name: Obter ARN do WAF
      shell: |
        cd ../04-security
        terraform output -raw waf_arn
      register: waf_arn
    
    - name: Associar WAF ao Ingress
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: networking.k8s.io/v1
          kind: Ingress
          metadata:
            name: my-app-ingress
            namespace: production
            annotations:
              alb.ingress.kubernetes.io/wafv2-acl-arn: "{{ waf_arn.stdout }}"
```

üìñ **Para implementa√ß√£o completa, veja:** [GUIA-IMPLEMENTACAO-ANSIBLE.md](./docs/GUIA-IMPLEMENTACAO-ANSIBLE.md)

---

### Stack 05 - Monitoring (Prometheus + Grafana)

Configure Amazon Managed Prometheus e Amazon Managed Grafana para monitorar o Cluster EKS.

**ANTES DE APLICAR:**

1. Verifique se `05-monitoring/data.cluster.remote-state.tf` usa o bucket correto com seu Account ID
2. O arquivo `05-monitoring/grafana.workspace.tf` j√° est√° configurado com `authentication_providers = ["AWS_SSO"]`
   - ‚úÖ **AWS_SSO √© RECOMENDADO** (gratuito, integrado com AWS)
   - ‚ö†Ô∏è Se voc√™ usa IdP externo (Okta, Azure AD), altere para `["SAML"]` e configure federation metadata ap√≥s o deploy
3. Ap√≥s o `terraform apply`, voc√™ **deve** configurar o acesso ao Grafana (ver se√ß√£o "Configura√ß√£o do Grafana" abaixo)

```bash
cd ../05-monitoring
terraform init
terraform apply -auto-approve
```

**Recursos criados:** 7 (Prometheus Workspace, Prometheus Scraper, Grafana Workspace, IAM Roles, CloudWatch Log Group, EKS Addon)

**‚è±Ô∏è Tempo estimado:** 20-25 minutos (Prometheus Scraper ~17min, Grafana Workspace ~6min)

**‚úÖ Outputs importantes:**

```bash
terraform output
```

Voc√™ receber√°:
- `grafana_workspace_url`: URL de acesso ao Grafana
- `prometheus_workspace_endpoint`: Endpoint do Prometheus
- `grafana_workspace_id`: ID do workspace Grafana
- `prometheus_workspace_id`: ID do workspace Prometheus
- `grafana_api_key`: API Key para automa√ß√£o Ansible (sensitive)

**‚ö†Ô∏è PR√ìXIMO PASSO OBRIGAT√ìRIO:** V√° para a se√ß√£o "üìä Configura√ß√£o do Grafana" mais abaixo antes de usar o Grafana

---

## ‚úÖ Valida√ß√£o Final da Infraestrutura

Ap√≥s completar todos os stacks, valide a infraestrutura completa:

```bash
# 1. Verificar nodes do cluster
kubectl get nodes
# Esperado: 3 nodes Ready

# 2. Verificar pods de sistema
kubectl get pods -A
# Esperado: Todos Running

# 3. Verificar Karpenter
kubectl get nodepools
kubectl get ec2nodeclasses
# Esperado: Status Ready

# 4. Verificar Ingress e ALB
kubectl get ingress
# Esperado: ADDRESS preenchido

# 5. Testar acesso HTTP
ALB_URL=$(kubectl get ingress eks-devopsproject-ingress -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
curl -I http://$ALB_URL
# Esperado: HTTP/1.1 200 OK

# 6. Verificar addons EKS
aws eks list-addons --cluster-name eks-devopsproject-cluster --profile terraform
# Esperado: vpc-cni, kube-proxy, coredns, aws-ebs-csi-driver, eks-pod-identity-agent, prometheus-node-exporter
```

**üìä Resumo de Recursos Provisionados:**

| Stack | Recursos | Tempo Estimado | Notas |
|-------|----------|----------------|-------|
| 00 - Backend | 3 | < 1 min | Obrigat√≥rio |
| 01 - Networking | 21 | 2-3 min | Obrigat√≥rio |
| 02 - EKS Cluster | 21 | 15-20 min | Obrigat√≥rio |
| 03 - Karpenter | 10 | 3-5 min | Obrigat√≥rio |
| 04 - Security/WAF | 2 | 1 min | **Opcional*** |
| 05 - Monitoring | 7 | 20-25 min | Obrigat√≥rio |
| 06 - E-commerce App | 15 (K8s) | 3 min (Ansible) / 20 min (Manual) | **Opcional**‚Ä†‚Ä† |
| **TOTAL (sem Stacks opcionais)** | **62** | **~39-54 min** | Cluster funcional |
| **TOTAL (com Stack 04)** | **64** | **~40-55 min** | + WAF |
| **TOTAL (completo com app)** | **79** | **~42-58 min** | + Aplica√ß√£o demo |

> **\* Stack 04 (WAF) √© opcional** porque:
> - WAF protege ALBs, que s√≥ existem quando voc√™ deploya aplica√ß√µes com Ingress
> - Se voc√™ ainda n√£o tem apps, pode pular este stack
> - Voc√™ pode voltar e aplicar Stack 04 depois de deployar suas aplica√ß√µes
> - Para automa√ß√£o completa de apps + WAF, veja [GUIA-IMPLEMENTACAO-ANSIBLE.md](./docs/GUIA-IMPLEMENTACAO-ANSIBLE.md)
> 
> **‚Ä†‚Ä† Stack 06 (E-commerce App) √© opcional** porque:
> - √â uma aplica√ß√£o de demonstra√ß√£o para mostrar cluster em funcionamento
> - Demonstra o valor do Ansible (3 min vs 20 min manual - economia de 85%)
> - Ideal para apresenta√ß√µes e valida√ß√£o de observabilidade
> - Pode ser removida a qualquer momento sem afetar infraestrutura

---

### Stack 06 - E-commerce Application (Demonstra√ß√£o) - OPCIONAL

Deploy de uma aplica√ß√£o real (e-commerce com microservi√ßos) para demonstrar o cluster em funcionamento com observabilidade completa.

> üí° **NOVO DIFERENCIAL:** Este stack demonstra a **superioridade do Ansible** sobre processos manuais!
> 
> | Abordagem | Tempo | Comandos | Erros Poss√≠veis |
> |-----------|-------|----------|-----------------|
> | **Manual** | 15-20 min | ~15 kubectl apply + valida√ß√µes | Alta chance de erro |
> | **Ansible** | 2-3 min | 1 comando | Zero erros (idempotente) |
> | **Economia** | **~85%** | **93% menos comandos** | **100% confi√°vel** |

**Sobre a Aplica√ß√£o:**
- **7 microservi√ßos** (Frontend React + 6 APIs backend)
- Arquitetura moderna (microservices pattern)
- Imagens Docker prontas (rslim087/*)
- **Ingress com ALB** (reutiliza Stack 02)
- **Auto-scaling** (usa Karpenter da Stack 03)
- **WAF opcional** (pode usar Stack 04)
- **Monitoramento autom√°tico** (integrado com Stack 05)

**Pr√©-requisitos:**
- ‚úÖ Stacks 00-03 deployadas (obrigat√≥rio)
- ‚úÖ Stack 05 deployada (recomendado para monitoramento)
- ‚úÖ Ansible instalado (para automa√ß√£o)

---

#### Op√ß√£o A: Deploy Automatizado com Ansible (RECOMENDADO) üöÄ

```bash
# Deploy completo da aplica√ß√£o (namespace + deployments + services + ingress + valida√ß√µes)
ansible-playbook ansible/playbooks/03-deploy-ecommerce.yml
```

**O que o playbook faz automaticamente:**
1. ‚úÖ Valida conex√£o com cluster e ALB Controller
2. ‚úÖ Cria namespace `ecommerce`
3. ‚úÖ Deploy de 7 microservi√ßos (Deployments + Services)
4. ‚úÖ Aguarda pods ficarem prontos (health checks)
5. ‚úÖ Cria Ingress e provisiona ALB
6. ‚úÖ Aguarda ALB ficar acess√≠vel
7. ‚úÖ Executa testes de conectividade
8. ‚úÖ Salva informa√ß√µes de acesso em arquivo

**Tempo total:** ~3 minutos ‚è±Ô∏è

**Configurar Monitoramento (Opcional mas Recomendado):**

```bash
# Importa dashboards Grafana espec√≠ficos para monitorar a aplica√ß√£o
ansible-playbook ansible/playbooks/04-configure-ecommerce-monitoring.yml
```

**O que o playbook faz:**
1. ‚úÖ Importa 3 dashboards Grafana (Kubernetes App Metrics, Pods, Deployments)
2. ‚úÖ Cria dashboard customizado para e-commerce
3. ‚úÖ Configura queries Prometheus para m√©tricas dos microservi√ßos
4. ‚úÖ Documenta alertas recomendados

**Tempo total:** ~2 minutos ‚è±Ô∏è

---

#### Op√ß√£o B: Deploy Manual (Para Compara√ß√£o Educacional)

Se quiser ver a diferen√ßa e entender o valor do Ansible:

```bash
# 1. Criar namespace
kubectl create namespace ecommerce

# 2. Deploy dos microservi√ßos (7 arquivos)
kubectl apply -f 06-ecommerce-app/manifests/ecommerce-ui.yaml
kubectl apply -f 06-ecommerce-app/manifests/product-catalog.yaml
kubectl apply -f 06-ecommerce-app/manifests/order-management.yaml
kubectl apply -f 06-ecommerce-app/manifests/product-inventory.yaml
kubectl apply -f 06-ecommerce-app/manifests/profile-management.yaml
kubectl apply -f 06-ecommerce-app/manifests/shipping-and-handling.yaml
kubectl apply -f 06-ecommerce-app/manifests/team-contact-support.yaml

# 3. Aguardar pods ficarem prontos
kubectl wait --for=condition=ready pod --all -n ecommerce --timeout=300s

# 4. Deploy do Ingress
kubectl apply -f 06-ecommerce-app/manifests/ingress.yaml

# 5. Aguardar ALB ser provisionado (2-5 minutos)
kubectl get ingress ecommerce-ingress -n ecommerce -w

# 6. Obter URL do ALB
ALB_URL=$(kubectl get ingress ecommerce-ingress -n ecommerce -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
echo "Aplica√ß√£o dispon√≠vel em: http://$ALB_URL"

# 7. Testar acesso
curl -I http://$ALB_URL

# 8. Configurar DNS no Hostgator (manual via painel)
# CNAME: eks ‚Üí [ALB_URL]
```

**Tempo total:** ~15-20 minutos ‚è±Ô∏è

**Problemas comuns do processo manual:**
- ‚ùå Esquecer algum microservi√ßo
- ‚ùå N√£o aguardar pods ficarem prontos
- ‚ùå Testar ALB antes de propagar DNS
- ‚ùå N√£o salvar informa√ß√µes de acesso

---

#### Acessar a Aplica√ß√£o

Ap√≥s o deploy (Ansible ou manual):

**Via ALB Direto:**
```bash
# Obter URL
kubectl get ingress ecommerce-ingress -n ecommerce

# Acessar no navegador
http://[ALB-URL]
```

**Via DNS Personalizado (Recomendado):**

1. Acesse o painel DNS do Hostgator
2. Crie/Edite registro CNAME:
   - **Nome:** `eks`
   - **Tipo:** `CNAME`
   - **Destino:** `[ALB-URL]`
   - **TTL:** `300`

3. Aguarde propaga√ß√£o (~5-10 minutos)

4. Acesse: **http://eks.devopsproject.com.br**

---

#### Validar Aplica√ß√£o

```bash
# Status dos pods
kubectl get pods -n ecommerce

# Logs do frontend
kubectl logs -f deployment/ecommerce-ui -n ecommerce

# Logs de um microservi√ßo espec√≠fico
kubectl logs -f deployment/product-catalog -n ecommerce

# Informa√ß√µes do Ingress
kubectl describe ingress ecommerce-ingress -n ecommerce

# Health check
curl -I http://[ALB-URL]
```

---

#### Monitoramento no Grafana

Se voc√™ executou o playbook de monitoramento, acesse o Grafana e veja:

1. **Dashboard "Kubernetes App Metrics"**
   - CPU/Memory por microservi√ßo
   - Network I/O
   - Pod status

2. **Dashboard "E-commerce Application - Overview"**
   - M√©tricas espec√≠ficas dos 7 microservi√ßos
   - Contagem de restarts
   - Status de health checks

3. **Queries √∫teis para criar alertas:**
   ```promql
   # Pods running
   count(kube_pod_status_phase{namespace="ecommerce", phase="Running"})
   
   # CPU usage por pod
   sum(rate(container_cpu_usage_seconds_total{namespace="ecommerce"}[5m])) by (pod)
   
   # Restarts nas √∫ltimas 24h
   sum(increase(kube_pod_container_status_restarts_total{namespace="ecommerce"}[24h]))
   ```

---

#### Associar WAF ao E-commerce (Opcional)

Se voc√™ deployou Stack 04 (WAF), pode proteger a aplica√ß√£o:

```bash
# Obter ARN do WAF
cd 04-security
WAF_ARN=$(terraform output -raw waf_arn)

# Adicionar annotation ao Ingress
kubectl annotate ingress ecommerce-ingress \
  -n ecommerce \
  alb.ingress.kubernetes.io/wafv2-acl-arn="$WAF_ARN" \
  --overwrite

# Verificar associa√ß√£o
kubectl describe ingress ecommerce-ingress -n ecommerce | grep waf
```

**Prote√ß√µes ativadas:**
- ‚úÖ Rate limiting (200 req/5min por IP)
- ‚úÖ SQL Injection detection
- ‚úÖ Cross-Site Scripting (XSS) protection
- ‚úÖ Geographic blocking (se configurado)

---

#### Remover Aplica√ß√£o

**Via Ansible:**
```bash
kubectl delete namespace ecommerce
```

**Manual:**
```bash
kubectl delete -f 06-ecommerce-app/manifests/ -n ecommerce
kubectl delete namespace ecommerce
```

O ALB ser√° automaticamente removido.

---

#### üìä Comparativo Final: Ansible vs Manual

| Tarefa | Manual | Ansible | Diferen√ßa |
|--------|--------|---------|-----------|
| **Deploy aplica√ß√£o** | 15-20 min | 3 min | ‚ö° **83% mais r√°pido** |
| **Configurar monitoramento** | 15 min | 2 min | ‚ö° **87% mais r√°pido** |
| **Valida√ß√µes** | Manual (5 min) | Autom√°tico | ‚ö° **100% automatizado** |
| **Documenta√ß√£o** | Manual | Auto-gerada | ‚ö° **Zero esfor√ßo** |
| **Comandos executados** | ~15 | 1 | ‚ö° **93% menos comandos** |
| **Chance de erro** | Alta | Zero | ‚ö° **100% confi√°vel** |
| **Reprodutibilidade** | Baixa | Perfeita | ‚ö° **Idempotente** |
| **Total (deploy + monitor)** | **30-35 min** | **5 min** | ‚ö° **85% mais r√°pido** |

**Conclus√£o:** Ansible economiza ~30 minutos por deploy e elimina completamente erros humanos! üéØ

---

## ÔøΩÔøΩ Troubleshooting - Erros Comuns

### Erro 1: "the server has asked for the client to provide credentials" (kubectl)

**Causa:** Access entry da terraform-role n√£o foi criado no EKS.

**Solu√ß√£o:** 
1. Verifique se `02-eks-cluster/eks.cluster.access.tf` cont√©m o bloco terraform_role (veja se√ß√£o 5.4)
2. Reaplique Stack 02: `terraform apply -auto-approve`
3. Atualize kubeconfig: `aws eks update-kubeconfig --name eks-devopsproject-cluster --region us-east-1 --profile terraform`

---

### Erro 2: "S3 bucket eks-devopsproject-state-files does not exist"

**Causa:** Nome do bucket S3 n√£o inclui o Account ID ou n√£o foi substitu√≠do corretamente.

**Solu√ß√£o:**
1. Verifique o nome do bucket no Stack 00: `cat 00-backend/variables.tf | grep bucket`
2. Deve ser: `eks-devopsproject-state-files-<YOUR_ACCOUNT>`
3. Corrija todos os arquivos `main.tf` e `data.cluster.remote-state.tf` nos stacks 01-05
4. Execute o comando de substitui√ß√£o da se√ß√£o 5.2 novamente

---

### Erro 3: "SSO is not enabled in any region" (Grafana)

**Causa:** Tentativa de usar `AWS_SSO` como autentica√ß√£o do Grafana sem SSO configurado.

**Solu√ß√£o:**
1. Edite `05-monitoring/grafana.workspace.tf`
2. Altere: `authentication_providers = ["SAML"]`
3. Reaplique: `terraform apply -auto-approve`

---

### Erro 4: "The specified instance type is not eligible for Free Tier"

**Causa:** Conta AWS Free Tier n√£o suporta inst√¢ncias t3.medium.

**Solu√ß√£o:**
- **Op√ß√£o 1 (Recomendada):** Fa√ßa upgrade da conta AWS para Paid Plan
- **Op√ß√£o 2:** Altere em `02-eks-cluster/variables.tf`:
  ```hcl
  instance_types = ["t3.small"]  # ou ["t3.micro"]
  ```
  > ‚ö†Ô∏è **ATEN√á√ÉO:** Inst√¢ncias menores podem causar problemas de performance no cluster.

---

### Erro 5: "Error creating WAF Web ACL Association" (Stack 04)

**Causa:** Tentativa de associar WAF antes do ALB existir.

**Solu√ß√£o:** Siga a sequ√™ncia correta da se√ß√£o Stack 04:
1. Criar WAF (`terraform apply`)
2. Criar Ingress (`kubectl apply -f ingress-sample-deployment.yml`)
3. Aguardar ALB ser provisionado (`kubectl get ingress -w`)
4. Renomear arquivos `.disabled` para `.tf`
5. Aplicar associa√ß√£o (`terraform apply`)

---

### Erro 6: "InvalidParameterException: bash_user_arn not found" ou "invalid principal"

**Causa:** Nome de usu√°rio IAM em `locals.tf` n√£o foi atualizado ou voc√™ est√° usando role assumida.

**Solu√ß√£o para usu√°rio IAM direto:**
1. Edite `02-eks-cluster/locals.tf`
2. Substitua `<YOUR_IAM_USER>` pelo nome do seu usu√°rio IAM
3. Reaplique: `terraform apply -auto-approve`

**Solu√ß√£o se estiver usando terraform-role (AWS profile com assume role):**
1. Comente o `bash_user` em `02-eks-cluster/eks.cluster.access.tf`
2. Atualize depend√™ncias em `eks.cluster.external.alb.tf`:
   ```hcl
   depends_on = [
     aws_iam_role_policy_attachment.load_balancer_controller,
     aws_eks_node_group.this,
     aws_eks_access_policy_association.terraform_role  # alterado de bash_user
   ]
   ```
3. Reaplique: `terraform apply -auto-approve`

---

### Erro 7: Helm provider version conflicts

**Causa:** Incompatibilidade entre vers√µes do provider Helm.

**Solu√ß√£o:**
O projeto j√° est√° fixado no Helm provider v2.17.0. Se encontrar problemas:
```bash
cd 02-eks-cluster
terraform init -upgrade
```

---

### Erro 8: "VPC has dependencies and cannot be deleted" (Destroy)

**Causa:** ENIs (Network Interfaces) do Prometheus Scraper ainda anexadas √† VPC.

**Sintomas:**
- `terraform destroy` da Stack 01 falha com erro de VPC dependente
- Subnets n√£o podem ser deletadas
- Ap√≥s destroy do Stack 05, VPC permanece com recursos

**Explica√ß√£o T√©cnica:**
O Prometheus Scraper cria ENIs gerenciadas pela AWS (tipo `amp_collector`) nas subnets privadas. Quando voc√™ executa `terraform destroy`, o Terraform solicita a dele√ß√£o do scraper, mas as ENIs levam **5-10 minutos** para serem liberadas automaticamente pela AWS. Durante este per√≠odo, a VPC e subnets n√£o podem ser deletadas.

**Solu√ß√£o Autom√°tica (destroy-all.sh):**
O script `destroy-all.sh` J√Å TEM prote√ß√£o autom√°tica que aguarda at√© 10 minutos pelas ENIs. **Simplesmente execute:**
```bash
./destroy-all.sh
```

**Solu√ß√£o Manual (se destroy-all.sh falhar):**
```bash
# 1. Verificar se h√° ENIs do Prometheus ainda anexadas
aws ec2 describe-network-interfaces \
  --filters "Name=interface-type,Values=amp_collector" \
  --profile terraform

# 2. Se ainda houver ENIs, aguardar 5-10 minutos

# 3. Executar script de limpeza final
./cleanup-vpc-final.sh
```

**Preven√ß√£o:**
- ‚úÖ Sempre use `./destroy-all.sh` ao inv√©s de destroy manual
- ‚úÖ Execute `./pre-destroy-check.sh` antes para ver warnings
- ‚úÖ NUNCA force delete ENIs do tipo `amp_collector` (s√£o gerenciadas pela AWS)

**Por que acontece:**
- AWS Prometheus Scraper (AMP) cria ENIs gerenciadas nas subnets do EKS
- Estas ENIs s√£o "owned" pela AWS (`InstanceOwnerId: amazon-aws`)
- Quando voc√™ deleta o scraper, a AWS precisa de tempo para cleanup interno
- O Terraform n√£o espera automaticamente, causando falha no destroy da VPC

**Como o c√≥digo foi corrigido:**
1. **`05-monitoring/prometheus.scraper.tf`**: Adicionado lifecycle hook
2. **`destroy-all.sh`**: Adicionado wait loop de 10min verificando ENIs
3. **`cleanup-vpc-final.sh`**: Script de fallback caso ainda falhe

---

### Erro 9: "ALB still exists, cannot delete Security Groups" (Destroy)

**Causa:** Load Balancer criado por Ingress n√£o foi deletado antes do destroy.

**Solu√ß√£o:**
O `destroy-all.sh` j√° deleta recursos Kubernetes primeiro. Se ainda encontrar:
```bash
# Deletar ALBs manualmente
kubectl delete ingress --all --all-namespaces
kubectl delete namespace ecommerce
kubectl delete namespace sample-app

# Aguardar 45s para ALB ser removido
sleep 45

# Tentar destroy novamente
cd 02-eks-cluster
terraform destroy -auto-approve
```

---

## üóëÔ∏è Destruir Infraestrutura

### M√©todo 1: Autom√°tico (RECOMENDADO) üöÄ

**Pr√©-valida√ß√£o (Opcional):**
```bash
# Verifica recursos que podem causar problemas antes do destroy
./pre-destroy-check.sh
```

**Destroy Completo:**
```bash
# Destr√≥i TODAS as stacks automaticamente na ordem correta
./destroy-all.sh
```

**O script faz automaticamente:**
1. ‚úÖ Deleta recursos Kubernetes (namespaces, Ingress ‚Üí ALB)
2. ‚úÖ Aguarda ALBs serem removidos pela AWS
3. ‚úÖ Destr√≥i Stack 05 (Monitoring: Grafana + Prometheus)
4. ‚úÖ **PROTE√á√ÉO AUTOM√ÅTICA:** Aguarda at√© 10min para ENIs do Prometheus serem liberadas
5. ‚úÖ Remove recursos √≥rf√£os do state (WAF, Helm releases)
6. ‚úÖ Destr√≥i Stack 04 ‚Üí 03 ‚Üí 02 ‚Üí 01
7. ‚úÖ Pergunta se quer destruir Stack 00 (backend)

**‚è±Ô∏è Tempo total:** ~15-25 minutos (inclui espera de ENIs)

**Se VPC n√£o deletar (raro):**
```bash
# Script de emerg√™ncia que limpa ENIs √≥rf√£s e finaliza VPC
./cleanup-vpc-final.sh
```

---

### M√©todo 2: Manual (Para Troubleshooting)

Para destruir os recursos manualmente, siga **EXATAMENTE** esta ordem:

```bash
# Stack 05 - Monitoring
cd ./05-monitoring
terraform destroy -auto-approve

# ‚ö†Ô∏è CR√çTICO: Aguardar ENIs do Prometheus serem liberadas (5-10 min)
# Verificar se ENIs ainda existem:
aws ec2 describe-network-interfaces --filters "Name=interface-type,Values=amp_collector" --profile terraform

# Stack 04 - Security (WAF)
cd ../04-security
terraform destroy -auto-approve

# Stack 03 - Karpenter
cd ../03-karpenter-auto-scaling
terraform destroy -auto-approve

# Stack 02 - EKS Cluster
cd ../02-eks-cluster
terraform destroy -auto-approve

# Stack 01 - Networking
cd ../01-networking
terraform destroy -auto-approve

# Stack 00 - Backend (OPCIONAL - mant√©m hist√≥rico de state)
# cd ../00-backend
# terraform destroy -auto-approve
```

**‚ö†Ô∏è ATEN√á√ÉO:** 
- **N√£o destrua** o Stack 00 se quiser manter o hist√≥rico de state do Terraform
- **CR√çTICO:** Sempre aguarde ENIs do Prometheus serem liberadas antes de destruir VPC (Stack 01)
- Aguarde cada comando concluir antes de executar o pr√≥ximo
- Se houver erro, verifique se√ß√£o **Troubleshooting de Destroy** abaixo

**‚è±Ô∏è Tempo total de destrui√ß√£o:** ~15-25 minutos

---

## üí∞ Estimativa de Custos

**Custos mensais aproximados (us-east-1):**

| Servi√ßo | Custo Estimado |
|---------|----------------|
| EKS Control Plane | $73/m√™s |
| EC2 (3x t3.medium) | ~$90/m√™s |
| NAT Gateways (2x) | ~$65/m√™s |
| EBS Volumes | ~$10/m√™s |
| ALB | ~$23/m√™s |
| Prometheus | ~$10/m√™s |
| Grafana | ~$9/m√™s |
| **TOTAL** | **~$280/m√™s** |

**üí° Economia:** Destrua os recursos quando n√£o estiver usando para economizar ~$9-10 por noite.

---


---

## üìä Configura√ß√£o do Grafana

**‚ö†Ô∏è OBRIGAT√ìRIO:** Ap√≥s aplicar a Stack 05, o Grafana Workspace √© criado **vazio** e **sem acesso configurado**. Voc√™ deve seguir esta se√ß√£o para configurar autentica√ß√£o e dashboards.

### Vis√£o Geral do Processo

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ ETAPA 1: Configurar Autentica√ß√£o SSO (OBRIGAT√ìRIA)             ‚îÇ
‚îÇ ‚è±Ô∏è Tempo: 5-10 minutos                                          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 1. Habilitar IAM Identity Center (SSO)                          ‚îÇ
‚îÇ 2. Criar usu√°rio SSO                                            ‚îÇ
‚îÇ 3. Atribuir usu√°rio ao Grafana Workspace                        ‚îÇ
‚îÇ 4. Promover usu√°rio para ADMIN (cr√≠tico!)                       ‚îÇ
‚îÇ 5. Acessar Grafana via AWS Access Portal                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ ETAPA 2: Configurar Data Source + Dashboards                   ‚îÇ
‚îÇ Escolha UMA das op√ß√µes abaixo:                                  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ OP√á√ÉO A (RECOMENDADA): Ansible Automation                      ‚îÇ
‚îÇ ‚è±Ô∏è Tempo: 2 minutos                                             ‚îÇ
‚îÇ ‚úÖ Data Source Prometheus configurado automaticamente           ‚îÇ
‚îÇ ‚úÖ Dashboard Node Exporter importado automaticamente            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ OP√á√ÉO B: Configura√ß√£o Manual                                   ‚îÇ
‚îÇ ‚è±Ô∏è Tempo: 10-15 minutos                                         ‚îÇ
‚îÇ ‚öôÔ∏è Configurar Data Source Prometheus manualmente                ‚îÇ
‚îÇ ‚öôÔ∏è Importar Dashboard 1860 (Node Exporter Full) manualmente     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## ETAPA 1: Configurar Autentica√ß√£o SSO (Obrigat√≥ria para Ambas Op√ß√µes)

### Passo 1: Habilitar AWS IAM Identity Center (SSO)

1. Acesse o console AWS: https://console.aws.amazon.com/singlesignon
2. Clique em **"Enable"** para ativar o IAM Identity Center
3. Anote o **Instance ID** que ser√° criado (formato: `ssoins-xxxxxxxxxxxx`)

### Passo 2: Criar Usu√°rio SSO

1. No IAM Identity Center, v√° em **Users** (menu lateral)
2. Clique em **"Add user"**
3. Preencha:
   - **Username**: `grafana-admin` (ou nome de sua prefer√™ncia)
   - **Email**: seu e-mail corporativo
   - **First name**: Seu nome
   - **Last name**: Seu sobrenome
4. Clique em **"Next"**
5. Em "Add user to groups": Pule esta etapa (Next)
6. Clique em **"Add user"**
7. Verifique seu e-mail e clique no link de verifica√ß√£o
8. Defina uma senha quando solicitado

### Passo 3: Obter URLs Importantes

```bash
cd 05-monitoring

# URL do Grafana Workspace
terraform output -raw grafana_workspace_url

# ID do Grafana Workspace
terraform output -raw grafana_workspace_id

# Endpoint do Prometheus (voc√™ usar√° no Passo 7)
terraform output -raw prometheus_workspace_endpoint
```

**Anote esses valores!** Voc√™ precisar√°:
- **grafana_workspace_id**: Para encontrar o workspace no console AWS
- **prometheus_workspace_endpoint**: Para configurar o Data Source no Passo 7

**Exemplo de output esperado:**
```
Grafana URL: https://g-7b4f900d4a.grafana-workspace.us-east-1.amazonaws.com/
Grafana ID: g-7b4f900d4a
Prometheus Endpoint: https://aps-workspaces.us-east-1.amazonaws.com/workspaces/ws-12345678-abcd-1234-efgh-123456789012
```

### Passo 4: Atribuir Usu√°rio ao Grafana Workspace

1. Acesse: https://console.aws.amazon.com/grafana/home?region=us-east-1
2. Clique no workspace que foi criado (ex: `g-8e1225a34f`)
3. V√° na aba **"Authentication"**
4. Na se√ß√£o **"AWS IAM Identity Center"**, clique em **"Assign new user or group"**
5. Selecione:
   - **Type**: User
   - **User**: Selecione o usu√°rio que criou (ex: `grafana-admin`)
6. Clique em **"Assign users and groups"**

### Passo 5: Alterar Permiss√£o para ADMIN ‚ö†Ô∏è OBRIGAT√ìRIO

1. Na mesma aba **"Authentication"**, localize o usu√°rio na tabela
2. Selecione o usu√°rio (marque o checkbox ao lado do nome)
3. Clique no bot√£o **"Actions"** (no topo da tabela)
4. Selecione **"Make admin"**
5. Confirme a altera√ß√£o

> ‚ö†Ô∏è **CR√çTICO:** Sem permiss√£o ADMIN, voc√™ N√ÉO conseguir√°:
> - Adicionar Data Sources (manual ou via Ansible)
> - Importar Dashboards (manual ou via Ansible)
> - Executar playbook Ansible (falhar√° com erro 403 Forbidden)

> üìù **Nota:** A interface AWS foi atualizada. Se voc√™ ainda v√™ os 3 pontinhos **[...]**, use essa op√ß√£o. Caso contr√°rio, use o bot√£o **Actions** ‚Üí **Make admin**.

---

### ‚úÖ Checkpoint: Autentica√ß√£o SSO Configurada

**Parab√©ns!** Voc√™ completou a ETAPA 1. Agora voc√™ tem:
- ‚úÖ IAM Identity Center (SSO) habilitado
- ‚úÖ Usu√°rio SSO criado e verificado
- ‚úÖ Usu√°rio atribu√≠do ao Grafana Workspace com permiss√£o ADMIN
- ‚úÖ Acesso ao Grafana via AWS Access Portal

**üéØ Pr√≥ximo Passo:** Configure o Grafana com Ansible (automa√ß√£o)

---

## ETAPA 2: Configura√ß√£o Autom√°tica com Ansible ‚≠ê

**‚è±Ô∏è Tempo:** 2 minutos  
**üìã Pr√©-requisitos:**
- ‚úÖ ETAPA 1 completa (SSO configurado com usu√°rio ADMIN)
- ‚úÖ Ansible instalado (ver [QUICK-START-ANSIBLE.md](./docs/QUICK-START-ANSIBLE.md))

**üöÄ Execu√ß√£o:**

```bash
cd ansible
ansible-playbook playbooks/01-configure-grafana.yml
```

**‚úÖ Resultado esperado:**
```
PLAY RECAP *********************************************************************
localhost : ok=3 changed=2 unreachable=0 failed=0

‚úÖ Data Source Prometheus configurado automaticamente
‚úÖ Dashboard Node Exporter Full (ID 1860) importado automaticamente
‚úÖ Grafana 100% pronto para uso
```

**üéâ Pronto!** Prossiga para a "Valida√ß√£o Final" abaixo.

---

### üîß Preferiu Configurar Manualmente?

Se voc√™ **n√£o pode** usar Ansible ou quer entender o processo passo a passo:

üìñ **Guia Completo:** [CONFIGURACAO-MANUAL-GRAFANA.md](./docs/CONFIGURACAO-MANUAL-GRAFANA.md)

**Tempo estimado:** 10-15 minutos (vs 2 minutos com Ansible)

O guia manual inclui:
- Passo a passo detalhado para configurar Data Source Prometheus
- Instru√ß√µes para importar Dashboard Node Exporter (ID 1860)
- Troubleshooting de erros comuns
- Queries PromQL para testes

---

## ‚úÖ Valida√ß√£o Final do Grafana

Ap√≥s executar o playbook Ansible, valide se tudo est√° funcionando:

**1. Verificar Data Source:**
- Menu lateral ‚Üí **Connections** ‚Üí **Data sources**
- Deve aparecer: **Prometheus** (verde, ativo)

**2. Verificar Dashboard:**
- Menu lateral ‚Üí **Dashboards**
- Deve aparecer: **Node Exporter Full**
- Clique no dashboard e verifique se os gr√°ficos est√£o mostrando dados

**3. Verificar M√©tricas:**
- No dashboard, voc√™ deve ver m√©tricas dos 3 nodes do EKS
- Gr√°ficos de CPU, Mem√≥ria, Disco devem estar populados com dados

üéâ **Sucesso!** Seu Grafana est√° 100% configurado e monitorando o cluster!

### üìä M√©tricas Dispon√≠veis no Dashboard Node Exporter Full

- üìä **CPU**: Usage, cores, idle, system, user, iowait
- üíæ **Mem√≥ria**: Total, usado, dispon√≠vel, cache, buffers
- üíø **Disco**: I/O read/write, utiliza√ß√£o, espa√ßo livre
- üåê **Rede**: Tr√°fego RX/TX, pacotes, erros, drops
- ‚ö° **Sistema**: Load average (1m, 5m, 15m), uptime, processes
- üìÅ **File System**: Inodes, mount points, file descriptors

### Troubleshooting

#### ‚ùå Grafana vazio (sem data sources, sem dashboards)
**Causa:** Isso √© **esperado**! O Terraform provisiona apenas o workspace Grafana vazio.

**Solu√ß√£o:** Voc√™ **deve** configurar manualmente:
1. **Data Source Prometheus**: Siga o Passo 7 acima
   - Menu lateral ‚Üí Connections ‚Üí Add data source ‚Üí Prometheus
   - Configure URL do Prometheus (obtido via `terraform output`)
   - Habilite SigV4 auth
2. **Dashboards**: Siga o Passo 8 acima
   - Menu lateral ‚Üí Dashboards ‚Üí New ‚Üí Import
   - Digite ID **1860** (Node Exporter Full)

**Tempo estimado:** 5 minutos para configura√ß√£o completa

---

#### ‚ùå Erro "sso.auth.access-denied" ao tentar acessar Grafana
**Causa:** Usu√°rio SSO existe, mas n√£o est√° atribu√≠do ao workspace Grafana ou tem permiss√£o VIEWER.

**Solu√ß√£o:**
1. Acesse: https://console.aws.amazon.com/grafana/home?region=us-east-1
2. Clique no workspace criado (ex: `g-7b4f900d4a`)
3. V√° na aba **"Authentication"**
4. Verifique se seu usu√°rio SSO est√° na lista
   - Se **N√ÉO**: Clique em "Assign new user or group" e adicione
   - Se **SIM**: Verifique se a role √© **ADMIN** (n√£o VIEWER)
5. Aguarde 1-2 minutos e tente novamente

---

#### ‚ùå Erro "403 Forbidden" ao executar Ansible
**Causa:** Usu√°rio SSO tem permiss√£o VIEWER ao inv√©s de ADMIN.

**Solu√ß√£o:**
1. Acesse: https://console.aws.amazon.com/grafana/home?region=us-east-1
2. Clique no workspace ‚Üí aba "Authentication"
3. Selecione o usu√°rio ‚Üí Actions ‚Üí Make admin
4. Aguarde 1-2 minutos
5. Re-execute o playbook Ansible

---

#### ‚ùå Erro 404 ao clicar "Go to connections"
**Solu√ß√£o**: Acesse diretamente via menu lateral ‚Üí Connections

#### ‚ùå Bot√£o "Add data source" desabilitado
**Solu√ß√£o**: Usu√°rio est√° com role VIEWER. Altere para ADMIN (Passo 5)

#### ‚ùå "Missing Authentication Token" ao testar Prometheus
**Solu√ß√£o**: Certifique-se de:
- Marcar **SigV4 auth**
- Preencher **Service: aps**
- URL sem barra `/` no final

#### ‚ùå "Page not found" ou "HttpNotFoundException"
**Solu√ß√£o**: Verifique se a URL do Prometheus est√° correta (sem `/api/v1/query` no final)

### Queries PromQL √öteis

Teste no **Explore** do Grafana:

```promql
# CPU usage por node
100 - (avg by (instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)

# Mem√≥ria dispon√≠vel em %
node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes * 100

# Disco usado em %
(node_filesystem_size_bytes - node_filesystem_free_bytes) / node_filesystem_size_bytes * 100

# Load average 5 minutos
node_load5

# Tr√°fego de rede (recebido)
rate(node_network_receive_bytes_total[5m])
```


## üìö Recursos Adicionais

### Testes e Valida√ß√£o

O projeto inclui arquivos de exemplo (YAML manifests) para valida√ß√£o manual dos componentes:

üìñ **Guia Completo de Testes:** [TESTES-VALIDACAO-MANUAL.md](./docs/TESTES-VALIDACAO-MANUAL.md)

O guia inclui:
- ‚úÖ Valida√ß√£o de EBS CSI Driver (Persistent Volumes)
- ‚úÖ Valida√ß√£o de ALB Ingress Controller + WAF
- ‚úÖ Valida√ß√£o de Karpenter Auto-Scaling
- ‚úÖ Valida√ß√£o de External DNS
- ‚úÖ Valida√ß√£o de Prometheus Node Exporter
- üìä Checklist completo de valida√ß√£o

> üí° **Dica:** Para ambientes de produ√ß√£o, considere automatizar estes testes com Ansible ou CI/CD pipelines ao inv√©s de execut√°-los manualmente.

### Comandos √öteis

```bash
# Verificar vers√£o do cluster
aws eks describe-cluster \
    --name eks-devopsproject-cluster \
    --query 'cluster.version' \
    --profile terraform

# Listar todos os addons instalados
aws eks list-addons \
    --cluster-name eks-devopsproject-cluster \
    --profile terraform

# Verificar logs do Karpenter
kubectl logs -n kube-system -l app.kubernetes.io/name=karpenter --tail=100 -f

# Ver detalhes do NodePool do Karpenter
kubectl describe nodepool default-node-pool

# Verificar WAF rules aplicadas
aws wafv2 list-web-acls --scope REGIONAL --region us-east-1 --profile terraform

# Acessar Grafana (ap√≥s deploy do Stack 05)
cd 05-monitoring
terraform output -raw grafana_workspace_url

# Ou obter URL do Prometheus
terraform output -raw prometheus_workspace_endpoint
```

### Links da Documenta√ß√£o Oficial

- [Amazon EKS User Guide](https://docs.aws.amazon.com/eks/latest/userguide/)
- [Karpenter Documentation](https://karpenter.sh/)
- [AWS WAF Developer Guide](https://docs.aws.amazon.com/waf/latest/developerguide/)
- [Amazon Managed Prometheus](https://docs.aws.amazon.com/prometheus/)
- [Amazon Managed Grafana](https://docs.aws.amazon.com/grafana/)

---

## ü§ù Suporte

Se encontrar problemas durante o deployment:

1. Verifique a se√ß√£o **Troubleshooting** acima
2. Confirme que seguiu **exatamente** a sequ√™ncia de deployment
3. Verifique se todas as substitui√ß√µes de vari√°veis foram feitas (Account ID, Bucket S3, IAM User)
4. Consulte os logs do Terraform: `terraform apply` sem `-auto-approve` para ver detalhes
5. Verifique se sua conta AWS tem os limites de servi√ßo adequados

---

## üìù Notas Importantes

- ‚úÖ Projeto testado e validado com Terraform 1.12.2
- ‚úÖ Compat√≠vel com EKS 1.32
- ‚úÖ Helm provider fixado em v2.17.0 para evitar breaking changes
- ‚úÖ Todos os stacks usam remote state em S3 com state locking em DynamoDB
- ‚úÖ IAM Roles seguem princ√≠pio de least privilege exceto AdministratorAccess na terraform-role
- ‚ö†Ô∏è Requer AWS Paid Plan ou cr√©ditos suficientes para inst√¢ncias t3.medium
- ‚ö†Ô∏è Custo estimado: ~$280/m√™s se mantido ligado 24/7
- üí° Economia: ~$9-10/noite destruindo recursos fora do hor√°rio de uso

---

## ‚úÖ CAPACIDADE DE IPs OTIMIZADA

### üìä Configura√ß√£o Atual

Este projeto j√° est√° **otimizado automaticamente** para evitar problemas de esgotamento de IPs:

**Subnets Privadas:**
- **CIDR:** /26 (ao inv√©s de /27)
- **Capacidade:** 59 IPs √∫teis por subnet (vs 27 anteriormente)
- **Total:** ~118 IPs dispon√≠veis para workloads

**AWS VPC CNI Otimizado:**
- `WARM_ENI_TARGET=0` - N√£o pr√©-aloca ENIs desnecess√°rias
- `WARM_IP_TARGET=5` - Mant√©m apenas 5 IPs warm por node
- `MINIMUM_IP_TARGET=10` - Garante m√≠nimo de 10 IPs por node
- **Economia:** ~15-20% de IPs comparado com configura√ß√£o padr√£o

### üéØ Capacidade de Workload

Com esta configura√ß√£o, voc√™ pode executar:
- ‚úÖ **5-8 nodes t3.medium** confortavelmente
- ‚úÖ **40-60 pods** distribu√≠dos no cluster
- ‚úÖ **Monitoramento completo** (Prometheus + Grafana + Node Exporter)
- ‚úÖ **Aplica√ß√£o e-commerce** (7 microservi√ßos)
- ‚úÖ **Karpenter auto-scaling** com margem para crescimento

### üîç Monitoramento de IPs (Opcional)

Se quiser verificar quantos IPs est√£o dispon√≠veis:

```bash
aws ec2 describe-subnets \
    --filters "Name=tag:Name,Values=*private-subnet*" \
    --query 'Subnets[].[Tags[?Key==`Name`].Value|[0],CidrBlock,AvailableIpAddressCount]' \
    --output table \
    --profile terraform
```

**Valores esperados ap√≥s deploy:**
- `private-subnet-us-east-1a`: ~50-55 IPs dispon√≠veis
- `private-subnet-us-east-1b`: ~50-55 IPs dispon√≠veis

> üí° **Nota:** Se voc√™ precisar expandir ainda mais (produ√ß√£o de grande escala), considere usar subnets /25 (123 IPs √∫teis) editando `01-networking/variables.tf` antes do primeiro deploy.

---

## ‚ö†Ô∏è TROUBLESHOOTING: Problemas Hist√≥ricos Resolvidos

### Esgotamento de IPs nas Subnets (RESOLVIDO ‚úÖ)

**Vers√µes antigas** deste projeto (antes de 01/12/2025) usavam subnets /27 (27 IPs √∫teis), o que causava esgotamento em ambientes com muitos pods.

**Solu√ß√£o aplicada automaticamente:**
- ‚úÖ Subnets expandidas para /26 (59 IPs √∫teis)
- ‚úÖ AWS VPC CNI otimizado por padr√£o
- ‚úÖ Sem necessidade de configura√ß√£o manual

Se voc√™ ainda encontrar o erro `InsufficientFreeAddresses`, verifique se est√° usando a vers√£o atualizada:

#### Diagn√≥stico R√°pido (apenas para troubleshooting)

Verifique quantos IPs est√£o dispon√≠veis:

```bash
# 1. Listar todas as subnets privadas
aws ec2 describe-subnets \
    --filters "Name=tag:Name,Values=*private*" \
    --query 'Subnets[].[SubnetId,CidrBlock,AvailableIpAddressCount,Tags[?Key==`Name`].Value|[0]]' \
    --output table \
    --profile terraform

# 2. Ver detalhes de uma subnet espec√≠fica
aws ec2 describe-subnets \
    --subnet-ids subnet-xxxxxxxxx \
    --query 'Subnets[0].[SubnetId,CidrBlock,AvailableIpAddressCount]' \
    --output table \
    --profile terraform

# 3. Contar ENIs e IPs secund√°rios por node
aws ec2 describe-network-interfaces \
    --filters "Name=subnet-id,Values=subnet-xxxxxxxxx" \
    --query 'NetworkInterfaces[].[NetworkInterfaceId,PrivateIpAddress,PrivateIpAddresses[].PrivateIpAddress|length(@),Description]' \
    --output table \
    --profile terraform
```

**Indicadores de problema:**
- ‚úÖ **Saud√°vel:** AvailableIpAddressCount > 10 (>40% da capacidade)
- ‚ö†Ô∏è **Aten√ß√£o:** AvailableIpAddressCount 5-10 (20-40% da capacidade)
- üî¥ **Cr√≠tico:** AvailableIpAddressCount < 5 (<20% da capacidade)

---

```bash
aws ec2 describe-subnets \
    --filters "Name=tag:Name,Values=*private-subnet*" \
    --query 'Subnets[].[Tags[?Key==`Name`].Value|[0],CidrBlock,AvailableIpAddressCount]' \
    --output table \
    --profile terraform
```

**Valores esperados:**
- ‚úÖ **Saud√°vel:** AvailableIpAddressCount > 40 (com subnets /26)
- ‚ö†Ô∏è **Aten√ß√£o:** AvailableIpAddressCount < 20 (subnet sob press√£o)
- üî¥ **Cr√≠tico:** AvailableIpAddressCount < 10 (precisa expans√£o urgente)

**Se voc√™ ainda usar subnets /27 antigas:**
Edite `01-networking/variables.tf` e mude os CIDRs para:
- `10.0.1.0/26` e `10.0.1.64/26` (staging - 59 IPs cada)
- `10.0.2.0/25` e `10.0.2.128/25` (produ√ß√£o - 123 IPs cada)

Depois execute `./rebuild-all.sh` para recriar a infraestrutura.

---

## üìã Checklist de Valida√ß√£o P√≥s-Deploy

**Quando usar:** Ambiente de desenvolvimento/testes, subnet /27, poucos nodes (2-4)

**Pr√≥s:**
- ‚úÖ **Custo:** $0 (zero investimento)
- ‚úÖ **Downtime:** Zero (configura√ß√£o online)
- ‚úÖ **Complexidade:** Baixa (5 minutos)
- ‚úÖ **Ganho:** Reduz consumo de IPs em ~15-20%
- ‚úÖ **Revers√≠vel:** Sim, facilmente

**Contras:**
- ‚ö†Ô∏è **Ganho limitado:** Libera apenas 4-5 IPs em subnet /27
- ‚ö†Ô∏è **Necessita reciclagem de nodes:** Para efeito imediato
- ‚ö†Ô∏è **N√£o escala:** Solu√ß√£o paliativa, n√£o resolve crescimento futuro

**Passo a Passo:**

```bash
# 1. Aplicar configura√ß√£o otimizada no AWS VPC CNI
kubectl set env daemonset aws-node -n kube-system \
  WARM_ENI_TARGET=0 \
  WARM_IP_TARGET=5 \
  MINIMUM_IP_TARGET=10

# 2. Verificar rollout
kubectl rollout status daemonset aws-node -n kube-system --timeout=3m

# 3. Confirmar configura√ß√£o aplicada
kubectl get daemonset aws-node -n kube-system -o jsonpath='{.spec.template.spec.containers[0].env[?(@.name=="WARM_ENI_TARGET")].value}' && echo
kubectl get daemonset aws-node -n kube-system -o jsonpath='{.spec.template.spec.containers[0].env[?(@.name=="WARM_IP_TARGET")].value}' && echo

# 4. OPCIONAL: Reciclar nodes para efeito imediato (ou aguardar libera√ß√£o natural)
# ATEN√á√ÉO: Isto causar√° recria√ß√£o dos nodes e reagendamento de todos os pods

# 4a. Obter lista de nodes
kubectl get nodes -o wide

# 4b. Obter IDs das inst√¢ncias EC2 dos nodes
aws ec2 describe-instances \
    --filters "Name=tag:eks:cluster-name,Values=eks-devopsproject-cluster-<YOUR_ACCOUNT>" \
              "Name=instance-state-name,Values=running" \
    --query 'Reservations[].Instances[].[InstanceId,PrivateIpAddress]' \
    --output table \
    --profile terraform

# 4c. Terminar as inst√¢ncias (ASG criar√° novas automaticamente com CNI otimizado)
aws ec2 terminate-instances \
    --instance-ids i-xxxxxxxxx i-yyyyyyyyy i-zzzzzzzzz \
    --profile terraform

# 4d. Aguardar novos nodes (2-3 minutos)
watch kubectl get nodes

# 5. Validar IPs liberados (ap√≥s 3-5 minutos)
aws ec2 describe-subnets \
    --subnet-ids subnet-xxxxxxxxx \
    --query 'Subnets[0].AvailableIpAddressCount' \
    --profile terraform
```

**Resultado esperado:** De 1-2 IPs dispon√≠veis para 5-7 IPs dispon√≠veis (ganho de +400%)

**Reverter (se necess√°rio):**
```bash
kubectl set env daemonset aws-node -n kube-system \
  WARM_ENI_TARGET=1 \
  WARM_IP_TARGET- \
  MINIMUM_IP_TARGET-
```

---

#### **OP√á√ÉO 2: Expandir Subnet para /26 ou /25 (SOLU√á√ÉO DEFINITIVA)** ‚≠ê‚≠ê‚≠ê

**Quando usar:** Produ√ß√£o, staging, ou qualquer ambiente que precisar√° escalar

**Pr√≥s:**
- ‚úÖ **Ganho significativo:** /26 = 59 IPs √∫teis (+118%) | /25 = 123 IPs √∫teis (+355%)
- ‚úÖ **Escalabilidade:** Suporta crescimento futuro
- ‚úÖ **Estabilidade:** Solu√ß√£o definitiva, n√£o paliativa
- ‚úÖ **Sem reconfigura√ß√µes:** N√£o precisa otimizar CNI

**Contras:**
- ‚ö†Ô∏è **Requer recria√ß√£o da subnet:** Necess√°rio destruir e recriar Stack 01 e seguintes
- ‚ö†Ô∏è **Downtime:** ~30-40 minutos (destrui√ß√£o + recria√ß√£o)
- ‚ö†Ô∏è **Trabalhoso:** Precisa recriar todas as stacks dependentes
- ‚ö†Ô∏è **Perda de dados tempor√°rios:** Pods e volumes ef√™meros s√£o perdidos

**Passo a Passo:**

```bash
# 1. Destruir stacks (ordem inversa)
# Certifique-se de estar na raiz do projeto
cd ./05-monitoring && terraform destroy -auto-approve
cd ../04-security && terraform destroy -auto-approve
cd ../03-karpenter-auto-scaling && terraform destroy -auto-approve
cd ../02-eks-cluster && terraform destroy -target=helm_release.external_dns -auto-approve
cd ../02-eks-cluster && terraform destroy -target=helm_release.load_balancer_controller -auto-approve
cd ../02-eks-cluster && terraform destroy -auto-approve
cd ../01-networking && terraform destroy -auto-approve

# 2. Editar arquivo de subnets privadas
# Abrir: 01-networking/vpc.private-subnets.tf
# Alterar os CIDRs das 3 subnets privadas:

# DE (subnet /27 = 27 IPs √∫teis):
# private-subnet-us-east-1a = "10.0.0.32/27"   # 10.0.0.32 - 10.0.0.63
# private-subnet-us-east-1b = "10.0.0.96/27"   # 10.0.0.96 - 10.0.0.127
# private-subnet-us-east-1c = "10.0.0.160/27"  # 10.0.0.160 - 10.0.0.191

# PARA /26 (59 IPs √∫teis - RECOMENDADO PARA STAGING):
# private-subnet-us-east-1a = "10.0.1.0/26"    # 10.0.1.0 - 10.0.1.63
# private-subnet-us-east-1b = "10.0.1.64/26"   # 10.0.1.64 - 10.0.1.127
# private-subnet-us-east-1c = "10.0.1.128/26"  # 10.0.1.128 - 10.0.1.191

# OU PARA /25 (123 IPs √∫teis - RECOMENDADO PARA PRODU√á√ÉO):
# private-subnet-us-east-1a = "10.0.2.0/25"    # 10.0.2.0 - 10.0.2.127
# private-subnet-us-east-1b = "10.0.2.128/25"  # 10.0.2.128 - 10.0.2.255
# private-subnet-us-east-1c = "10.0.3.0/25"    # 10.0.3.0 - 10.0.3.127

# 3. Recriar todas as stacks (seguir sequ√™ncia de deploy completa)
cd ../01-networking && terraform init && terraform apply -auto-approve
cd ../02-eks-cluster && terraform init && terraform apply -auto-approve
# ... continuar com stacks 03, 04, 05

# 4. Validar capacidade da nova subnet
aws ec2 describe-subnets \
    --filters "Name=tag:Name,Values=*private-subnet-us-east-1b*" \
    --query 'Subnets[].[SubnetId,CidrBlock,AvailableIpAddressCount]' \
    --output table \
    --profile terraform
```

**Resultado esperado:**
- /26: ~55-57 IPs dispon√≠veis (de 27 para 59 IPs √∫teis)
- /25: ~119-121 IPs dispon√≠veis (de 27 para 123 IPs √∫teis)

---

#### **OP√á√ÉO 3: Adicionar Mais Availability Zones (M√âDIA COMPLEXIDADE)**

**Quando usar:** Precisa de alta disponibilidade em m√∫ltiplas AZs, mas n√£o quer recriar subnets

**Pr√≥s:**
- ‚úÖ **Aumenta capacidade total:** Distribui carga entre mais subnets
- ‚úÖ **Alta disponibilidade:** Mais AZs = mais resili√™ncia
- ‚úÖ **Mant√©m subnets existentes:** N√£o precisa destruir stacks

**Contras:**
- ‚ö†Ô∏è **N√£o resolve subnet espec√≠fica:** Se us-east-1b est√° cheia, continua cheia
- ‚ö†Ô∏è **Custo:** +$32/m√™s por NAT Gateway adicional
- ‚ö†Ô∏è **Complexidade moderada:** Requer edi√ß√£o de m√∫ltiplos arquivos

**Passo a Passo:**

```bash
# 1. Adicionar us-east-1d, us-east-1e, ou us-east-1f em:
#    - 01-networking/vpc.private-subnets.tf
#    - 01-networking/vpc.public-subnets.tf  
#    - 01-networking/vpc.nat-gateways.tf
#    - 01-networking/vpc.private-route-tables.tf

# 2. Aplicar mudan√ßas
cd ./01-networking && terraform apply -auto-approve

# 3. Node Group do EKS automaticamente distribuir√° nodes nas novas subnets
```

**Resultado esperado:** Carga distribu√≠da, mas custo adicional de ~$32/m√™s por AZ

---

### üéØ Matriz de Decis√£o: Qual Op√ß√£o Escolher?

| Cen√°rio | Op√ß√£o Recomendada | Justificativa |
|---------|-------------------|---------------|
| **Dev/Testes com poucos pods** | Op√ß√£o 1 (CNI) | R√°pido, gr√°tis, resolve temporariamente |
| **Staging com crescimento** | Op√ß√£o 2 (/26) | Balanceia capacidade e custo |
| **Produ√ß√£o cr√≠tica** | Op√ß√£o 2 (/25) + Op√ß√£o 1 | M√°xima capacidade + otimiza√ß√£o |
| **Multi-regi√£o HA** | Op√ß√£o 3 + Op√ß√£o 1 | Resili√™ncia + efici√™ncia |
| **Or√ßamento zero** | Op√ß√£o 1 (CNI) | √önica op√ß√£o sem custo |
| **Problema urgente** | Op√ß√£o 1 (CNI) | Resolve em 5 minutos |

---

Ap√≥s aplicar qualquer op√ß√£o, valide:

```bash
# ‚úÖ 1. Subnet tem IPs suficientes (>40 com /26)
aws ec2 describe-subnets \
    --filters "Name=tag:Name,Values=*private-subnet*" \
    --query 'Subnets[].[Tags[?Key==`Name`].Value|[0],CidrBlock,AvailableIpAddressCount]' \
    --output table \
    --profile terraform

# ‚úÖ 2. Todos os nodes est√£o Ready
kubectl get nodes

# ‚úÖ 3. Todos os pods est√£o Running (nenhum ContainerCreating)
kubectl get pods -A | grep -v Running | grep -v Completed
```

---

## üôè Cr√©ditos

Este projeto √© um fork do trabalho original de **[Kenerry Serain](https://github.com/kenerry-serain)**, desenvolvido como material do curso **DevOps na Nuvem**.

Agradecimentos especiais pela estrutura e conhecimento compartilhado que tornou este projeto poss√≠vel.

**Reposit√≥rio Original:** [kenerry-serain (GitHub)](https://github.com/kenerry-serain)

---

## üìÑ Licen√ßa

Este projeto √© fornecido como material educacional. Uso livre para fins de estudo e desenvolvimento pessoal.

---

**Desenvolvido com ‚ù§Ô∏è para aprendizado de DevOps e Infraestrutura como C√≥digo**

