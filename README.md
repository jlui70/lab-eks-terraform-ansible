# EKS Express - Infraestrutura AWS Production Grade

Infraestrutura completa para provisionar um **Cluster Amazon EKS production-grade** utilizando **Terraform**, com stacks modulares para gerenciamento de recursos AWS.

Este projeto inclui:
- ‚úÖ **EKS Cluster 1.32** com Node Groups gerenciados
- ‚úÖ **Karpenter** para auto-scaling din√¢mico de nodes
- ‚úÖ **AWS Load Balancer Controller** para Ingress
- ‚úÖ **External DNS** para gerenciamento autom√°tico de DNS
- ‚úÖ **WAF** para prote√ß√£o do Application Load Balancer
- ‚úÖ **Amazon Managed Prometheus + Grafana** para observabilidade
- ‚úÖ **6 stacks Terraform** modulares e reutiliz√°veis
- ‚úÖ **Scripts de automa√ß√£o** para deploy e destroy

---

## üÜï Novidade: Integra√ß√£o com Ansible

Este projeto foi expandido com **documenta√ß√£o completa** para integra√ß√£o com **Ansible**, automatizando a configura√ß√£o de servi√ßos ap√≥s o deployment Terraform.

### **üìö Documenta√ß√£o Ansible Dispon√≠vel:**

1. **[ANALISE-ANSIBLE-INTEGRACAO.md](./docs/ANALISE-ANSIBLE-INTEGRACAO.md)**  
   - An√°lise t√©cnica completa das 5 √°reas onde Ansible agrega valor
   - Pr√°ticas de mercado (Netflix, Spotify, Airbnb)
   - ROI e estimativa de esfor√ßo

2. **[GUIA-IMPLEMENTACAO-ANSIBLE.md](./docs/GUIA-IMPLEMENTACAO-ANSIBLE.md)**  
   - C√≥digo pronto para uso (roles, playbooks)
   - Setup passo a passo
   - Exemplos pr√°ticos

### **üéØ Benef√≠cios da Integra√ß√£o Ansible:**

| Tarefa | Sem Ansible | Com Ansible | Economia |
|--------|-------------|-------------|----------|
| Configurar Grafana | 15-20 min (manual) | 2 min (autom√°tico) | **90%** |
| Deploy sample apps | 10 min (manual) | 1 min (autom√°tico) | **90%** |
| Valida√ß√£o cluster | 15 min (manual) | 1 min (autom√°tico) | **93%** |
| **3 ambientes completos** | **~10 horas** | **~2.5 horas** | **75%** |

---

## üöÄ Fluxo de Deployment Recomendado

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ FASE 1: Terraform (60-90 min)                                   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 1. Stack 00 (Backend)        ‚Üí S3 + DynamoDB                    ‚îÇ
‚îÇ 2. Stack 01 (Networking)     ‚Üí VPC + Subnets + NAT              ‚îÇ
‚îÇ 3. Stack 02 (EKS Cluster)    ‚Üí EKS + Node Group + ALB           ‚îÇ
‚îÇ 4. Stack 03 (Karpenter)      ‚Üí Auto-scaling                     ‚îÇ
‚îÇ 5. Stack 04 (Security/WAF)   ‚Üí WAF WebACL (OPCIONAL - requer apps) ‚îÇ
‚îÇ 6. Stack 05 (Monitoring)     ‚Üí Grafana + Prometheus + API Key   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ FASE 2: Configura√ß√£o Grafana SSO (5-10 min) ‚ö†Ô∏è OBRIGAT√ìRIO     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 1. Habilitar IAM Identity Center (SSO)                          ‚îÇ
‚îÇ 2. Criar usu√°rio SSO                                            ‚îÇ
‚îÇ 3. Atribuir usu√°rio ao Grafana Workspace                        ‚îÇ
‚îÇ 4. ‚ö†Ô∏è MUDAR PARA ADMIN (cr√≠tico!)                               ‚îÇ
‚îÇ 5. Acessar Grafana via AWS Access Portal                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ FASE 3A: Ansible (2 min) - RECOMENDADO                         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ ansible-playbook playbooks/01-configure-grafana.yml             ‚îÇ
‚îÇ   ‚Üí ‚úÖ Data Source Prometheus configurado automaticamente       ‚îÇ
‚îÇ   ‚Üí ‚úÖ Dashboard Node Exporter importado automaticamente        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              OU
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ FASE 3B: Manual (10-15 min) - Alternativa                      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 1. Configurar Data Source Prometheus manualmente                ‚îÇ
‚îÇ 2. Importar Dashboard 1860 manualmente                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ FASE 4: Deploy E-commerce App (OPCIONAL - Demonstra√ß√£o)        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Stack 06 - Aplica√ß√£o real com 7 microservi√ßos                  ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ OP√á√ÉO A - Ansible (3 min): ‚ö° 85% mais r√°pido                  ‚îÇ
‚îÇ   ansible-playbook playbooks/03-deploy-ecommerce.yml            ‚îÇ
‚îÇ   ansible-playbook playbooks/04-configure-ecommerce-monitoring.yml ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ OP√á√ÉO B - Manual (20 min): kubectl apply -f ...                ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ Resultado: App acess√≠vel em eks.devopsproject.com.br           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ ‚úÖ AMBIENTE PRONTO PARA USO + APLICA√á√ÉO DEMO                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**‚ö†Ô∏è PONTOS CR√çTICOS:**
- üî¥ **Stack 05 deve incluir API Key** para Ansible funcionar (ver se√ß√£o "Stack 05")
- üî¥ **Usu√°rio SSO DEVE ser ADMIN** sen√£o Ansible falhar√° com 403 Forbidden
- üî¥ **N√£o pule a Fase 2** (SSO) - Grafana workspace √© criado vazio sem autentica√ß√£o

---

## üìã Pr√©-requisitos

Antes de iniciar o deployment, certifique-se de ter:

- **AWS Account** com permiss√µes administrativas
- **AWS CLI** configurado (vers√£o 2.x recomendada)
- **Terraform** instalado (vers√£o 1.12.x ou superior)
- **kubectl** instalado (vers√£o compat√≠vel com EKS 1.32)
- **Helm** instalado (vers√£o 3.x)
- **Conta AWS Paid Plan** ou cr√©ditos suficientes (Free Tier n√£o suporta inst√¢ncias t3.medium)

> ‚ö†Ô∏è **IMPORTANTE:** O projeto utiliza inst√¢ncias **t3.medium** para os worker nodes. Contas AWS Free Tier s√£o limitadas a t3.micro/t3.small. Certifique-se de ter upgrade para Paid Plan ou cr√©ditos AWS dispon√≠veis.
>
> üí∞ **ESTIMATIVA DE CUSTO PARA LABORAT√ìRIO:**
> - **30 minutos de teste:** ~$0.50 USD
> - **2 horas completas (deploy + valida√ß√£o):** ~$2.00 USD
> - **8 horas (dia de estudo):** ~$8.00 USD
> 
> **üí° DICA:** Execute `terraform destroy` imediatamente ap√≥s os testes para evitar cobran√ßas cont√≠nuas. O custo de ~$280/m√™s mencionado abaixo √© apenas se voc√™ mantiver a infraestrutura rodando 24/7.

---

## üõ†Ô∏è Configura√ß√£o Inicial

### 1. Criar IAM User para Terraform

Crie um usu√°rio IAM na sua conta AWS para realizar o deployment:

**Aten√ß√£o:** Substitua `<YOUR_USER>` pelo nome desejado (ex: `terraform-deploy`).

```bash
aws iam create-user --user-name <YOUR_USER>
```

---

### 2. Criar e Configurar a Role do Terraform

Crie uma Role na sua conta AWS que ser√° assumida pelo Terraform:

**Aten√ß√£o:** Substitua `<YOUR_ACCOUNT>` pelo ID da sua conta AWS e `<YOUR_USER>` pelo usu√°rio criado no passo anterior.

```bash
aws iam create-role \
    --role-name terraform-role \
    --assume-role-policy-document '{
        "Version": "2012-10-17",
        "Statement": [{
            "Effect": "Allow",
            "Principal": {
                "AWS": "arn:aws:iam::<YOUR_ACCOUNT>:user/<YOUR_USER>"
            },
            "Action": "sts:AssumeRole",
            "Condition": {
                "StringEquals": {
                    "sts:ExternalId": "3b94ec31-9d0d-4b22-9bce-72b6ab95fe1a"
                }
            }
        }]
    }'
```

üìå **Observa√ß√£o:** O External ID `3b94ec31-9d0d-4b22-9bce-72b6ab95fe1a` j√° est√° configurado em todos os arquivos do projeto. Voc√™ pode alter√°-lo, mas precisar√° atualizar todos os arquivos `variables.tf`.

---

### 3. Anexar Permiss√µes Administrativas √† Role

```bash
aws iam attach-role-policy \
    --role-name terraform-role \
    --policy-arn arn:aws:iam::aws:policy/AdministratorAccess
```

---

### 4. Configurar AWS CLI Profile

Configure um profile espec√≠fico para o Terraform assumir a role:

**Aten√ß√£o:** Substitua `<YOUR_ACCOUNT>` pelo ID da sua conta AWS.

```bash
aws configure set role_arn arn:aws:iam::<YOUR_ACCOUNT>:role/terraform-role --profile terraform
aws configure set source_profile default --profile terraform
aws configure set external_id 3b94ec31-9d0d-4b22-9bce-72b6ab95fe1a --profile terraform
aws configure set region us-east-1 --profile terraform
```

Teste a configura√ß√£o:

```bash
aws sts get-caller-identity --profile terraform
```

---

## üîß Substitui√ß√µes Necess√°rias nos Arquivos

### 5.1. Substituir `<YOUR_ACCOUNT>` pelo seu Account ID

**CR√çTICO:** Todos os arquivos `.tf` cont√™m o placeholder `<YOUR_ACCOUNT>` que **deve** ser substitu√≠do pelo ID da sua conta AWS.

#### **Obter seu Account ID:**

```bash
aws sts get-caller-identity --query Account --output text --profile terraform
```

Anote o n√∫mero retornado (ex: `123456789012`).

#### üêß **(WSL/Linux)**

```bash
find . -type f -name "*.tf" -exec sed -i \
    's|<YOUR_ACCOUNT>|123456789012|g' {} +
```

#### üçé **(MacOS)**

```bash
find . -type f -name "*.tf" -exec sed -i '' \
    's|<YOUR_ACCOUNT>|123456789012|g' {} +
```

> ‚ö†Ô∏è **ATEN√á√ÉO:** Substitua `123456789012` pelo seu Account ID real obtido no comando acima.

**O que ser√° substitu√≠do:**
- ‚úÖ IAM Role ARN: `arn:aws:iam::<YOUR_ACCOUNT>:role/terraform-role`
- ‚úÖ Bucket S3: `eks-devopsproject-state-files-<YOUR_ACCOUNT>`
- ‚úÖ EKS Access entries (cluster admin)

**Total:** 16 ocorr√™ncias em 10 arquivos `.tf`

---

### 5.2. Configurar Usu√°rio IAM no locals.tf (Stack 02)

**OBRIGAT√ìRIO:** Edite o arquivo `02-eks-cluster/locals.tf` e substitua o nome do usu√°rio IAM:

```hcl
locals {
  bash_user_arn    = "arn:aws:iam::${data.aws_caller_identity.current.account_id}:user/<YOUR_USER>"
  console_user_arn = "arn:aws:iam::${data.aws_caller_identity.current.account_id}:role/aws-reserved/sso.amazonaws.com/AWSReservedSSO_AdministratorAccess_xxxxx"
  eks_oidc_url     = replace(aws_eks_cluster.this.identity[0].oidc[0].issuer, "https://", "")
}
```

Substitua:
- `<YOUR_USER>` pelo nome do usu√°rio IAM criado no passo 1
- `console_user_arn` pelo ARN do seu SSO role (se aplic√°vel), ou comente a linha se n√£o usar SSO

---

### 5.3. Adicionar terraform-role ao EKS Access (Stack 02)

**CR√çTICO:** O arquivo `02-eks-cluster/eks.cluster.access.tf` **deve** conter o access entry para a terraform-role, caso contr√°rio `kubectl` n√£o funcionar√°:

Verifique se o arquivo cont√©m:

```hcl
# Terraform Role Access
resource "aws_eks_access_entry" "terraform_role" {
  cluster_name  = aws_eks_cluster.this.name
  principal_arn = "arn:aws:iam::<YOUR_ACCOUNT>:role/terraform-role"
  type          = "STANDARD"
}

resource "aws_eks_access_policy_association" "terraform_role" {
  cluster_name  = aws_eks_cluster.this.name
  policy_arn    = "arn:aws:eks::aws:cluster-access-policy/AmazonEKSClusterAdminPolicy"
  principal_arn = "arn:aws:iam::<YOUR_ACCOUNT>:role/terraform-role"

  access_scope {
    type = "cluster"
  }
}
```

> ‚ö†Ô∏è **ATEN√á√ÉO:** Sem este access entry, voc√™ receber√° erro `"the server has asked for the client to provide credentials"` ao tentar usar kubectl.

---

## üöÄ Sequ√™ncia de Deploy

### Stack 00 - Backend (S3 + DynamoDB)

A stack `backend` cria o bucket S3 e a tabela DynamoDB para o Terraform state locking e remote backend:

```bash
cd ./00-backend
terraform init
terraform apply -auto-approve
```

**Recursos criados:** 3 (S3 bucket, S3 versioning, DynamoDB table)

üìå **Observa√ß√£o:** O comando considera que voc√™ est√° na pasta root do projeto.

---

### Stack 01 - Networking (VPC, Subnets, NAT)

Crie a base de redes para as pr√≥ximas stacks:

```bash
cd ../01-networking
terraform init
terraform apply -auto-approve
```

**Recursos criados:** 21 (VPC, Internet Gateway, 6 Subnets, NAT Gateways, Route Tables, EIPs)

**‚è±Ô∏è Tempo estimado:** 2-3 minutos

---

### Stack 02 - EKS Cluster

Crie um Cluster EKS com addons instalados.

**ANTES DE APLICAR:**

1. Edite `02-eks-cluster/locals.tf` e configure seu usu√°rio IAM (veja se√ß√£o 5.3)
2. Verifique `02-eks-cluster/eks.cluster.access.tf` cont√©m terraform-role access entry (veja se√ß√£o 5.4)
3. (Opcional) Ajuste quantidade de worker nodes em `variables.tf` se necess√°rio

```bash
cd ../02-eks-cluster
terraform init
terraform apply -auto-approve
```

**Recursos criados:** 21 (EKS Cluster, Node Group, IAM Roles, Addons, OIDC Provider, ALB Controller, External DNS)

**‚è±Ô∏è Tempo estimado:** 15-20 minutos (inclui provisionamento dos node groups)

---

### Configurar kubectl (OBRIGAT√ìRIO)

Ap√≥s o deploy do Stack 02, configure o kubectl para acessar o cluster:

```bash
aws eks update-kubeconfig \
    --name <CLUSTER_NAME> \
    --region us-east-1 \
    --profile terraform
```

> üìù **Nota:** Substitua `<CLUSTER_NAME>` pelo nome do seu cluster. Se voc√™ n√£o alterou as vari√°veis do Terraform, o nome padr√£o √© `eks-devopsproject-cluster`.

**Exemplo:**
```bash
aws eks update-kubeconfig \
    --name eks-devopsproject-cluster \
    --region us-east-1 \
    --profile terraform
```

Teste o acesso:

```bash
kubectl get nodes
kubectl get pods -A
```

**‚úÖ Valida√ß√£o esperada:**
- 3 nodes no estado `Ready`
- Pods do kube-system rodando
- Pods do aws-load-balancer-controller (2/2 Ready)
- Pods do external-dns (1/1 Ready)

---

### Stack 03 - Karpenter Auto Scaling

Torne o Cluster EKS din√¢mico, adicionando e removendo n√≥s sob demanda utilizando Karpenter:

```bash
cd ../03-karpenter-auto-scaling
terraform init
terraform apply -auto-approve
```

**Recursos criados:** 10 (Karpenter Controller, IAM Roles, Security Group, CRDs, NodePool, EC2NodeClass)

**‚è±Ô∏è Tempo estimado:** 3-5 minutos

**‚úÖ Valida√ß√£o:**

```bash
kubectl get pods -n kube-system | grep karpenter
# Deve mostrar: karpenter-xxxxx  2/2  Running

kubectl get nodepools
# Deve mostrar: default-node-pool  Ready

kubectl get ec2nodeclasses
# Deve mostrar: default  Ready  True
```

---

### Stack 04 - Security (WAF) - OPCIONAL

> üí° **IMPORTANTE:** Este stack √© **opcional** e s√≥ faz sentido ap√≥s deployar aplica√ß√µes que criam ALBs. 
> 
> O WAF protege Application Load Balancers, mas eles s√≥ s√£o criados quando voc√™ cria recursos Ingress no Kubernetes. Se voc√™ ainda n√£o tem aplica√ß√µes deployadas, pode **pular este stack** e voltar depois.

**Quando usar:**
- ‚úÖ Voc√™ j√° deployou aplica√ß√µes com Ingress (que criam ALBs)
- ‚úÖ Voc√™ quer proteger seus ALBs contra ataques web (SQL injection, XSS, rate limiting)

**Se voc√™ n√£o tem aplica√ß√µes ainda:**
- ‚è≠Ô∏è Pule para Stack 05 (Monitoring)
- üîÑ Volte aqui depois de deployar apps

---

#### Passo 4.1: Criar WAF WebACL

#### Passo 4.1: Criar WAF WebACL

```bash
cd ../04-security
terraform init
terraform apply -auto-approve
```

**Recursos criados:** 1 (WAF WebACL)

**‚è±Ô∏è Tempo estimado:** 30 segundos

---

#### Passo 4.2: Criar Ingress Sample (provisionar√° o ALB)

> üìù **Nota:** Este passo cria uma aplica√ß√£o de exemplo apenas para demonstrar a integra√ß√£o WAF + ALB. 
> Em produ√ß√£o, voc√™ associaria o WAF aos ALBs das suas aplica√ß√µes reais.

Antes de associar o WAF ao ALB, √© necess√°rio que um ALB exista. Vamos criar um deployment de teste:

```bash
kubectl apply -f ../02-eks-cluster/samples/ingress-sample-deployment.yml
```

**Aguarde o ALB ser provisionado (~2-3 minutos):**

```bash
kubectl get ingress eks-devopsproject-ingress -n sample-app -w
```

Quando aparecer o endere√ßo do ALB na coluna `ADDRESS`, pressione Ctrl+C.

**Teste o ALB (aguarde DNS propagar ~60-90 segundos):**

```bash
ALB_URL=$(kubectl get ingress eks-devopsproject-ingress -n sample-app -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
curl -I http://$ALB_URL
```

**‚úÖ Esperado:** `HTTP/1.1 200 OK`

> üí° **Automa√ß√£o com Ansible:** Em ambientes de produ√ß√£o, recomendamos automatizar o deploy de aplica√ß√µes e associa√ß√£o do WAF usando Ansible. Veja [GUIA-IMPLEMENTACAO-ANSIBLE.md](./docs/GUIA-IMPLEMENTACAO-ANSIBLE.md) para exemplos.

---

#### Passo 4.3: Associar WAF ao ALB

Agora que o ALB existe, associe o WAF adicionando uma anota√ß√£o ao Ingress.

**Obtenha o ARN do WAF:**

```bash
cd ../04-security
WAF_ARN=$(terraform state show aws_wafv2_web_acl.this | grep "arn " | awk '{print $3}' | tr -d '"')
echo "WAF ARN: $WAF_ARN"
```

**Adicione a anota√ß√£o do WAF ao Ingress:**

```bash
kubectl annotate ingress eks-devopsproject-ingress \
  -n sample-app \
  alb.ingress.kubernetes.io/wafv2-acl-arn="$WAF_ARN" \
  --overwrite
```

**Aguarde o ALB Controller processar (~30-60 segundos):**

```bash
kubectl get ingress eks-devopsproject-ingress -n sample-app -w
```

Quando a coluna `ADDRESS` aparecer novamente (pode piscar), pressione Ctrl+C.

**‚úÖ Valida√ß√£o:**

Verifique se a associa√ß√£o foi criada:

```bash
# Obter ARN do ALB
ALB_ARN=$(aws elbv2 describe-load-balancers \
  --query "LoadBalancers[?contains(LoadBalancerName, 'k8s-sampleap')].LoadBalancerArn" \
  --output text --profile terraform)

# Verificar associa√ß√£o WAF
aws wafv2 get-web-acl-for-resource \
  --resource-arn "$ALB_ARN" \
  --region us-east-1 \
  --profile terraform \
  --query 'WebACL.Name' \
  --output text
```

**Esperado:** `waf-eks-devopsproject-webacl`

Ou verifique no AWS Console:
1. Acesse: https://console.aws.amazon.com/wafv2/home?region=us-east-1
2. Clique em **Web ACLs** ‚Üí `waf-eks-devopsproject-webacl`
3. Na aba **Associated AWS resources**, voc√™ ver√° o ALB listado

---

### ü§ñ Automatizando WAF com Ansible (Recomendado para Produ√ß√£o)

Os passos manuais acima s√£o √∫teis para **demonstra√ß√£o e aprendizado**, mas em produ√ß√£o recomendamos automatizar:

**Por que automatizar?**
- ‚úÖ Evita passos manuais repetitivos
- ‚úÖ Garante consist√™ncia entre ambientes (dev/staging/prod)
- ‚úÖ Permite CI/CD completo
- ‚úÖ Reduz erros humanos

**Como fazer:**

Crie um playbook Ansible que:
1. Deploya sua aplica√ß√£o com Ingress
2. Aguarda o ALB ser provisionado
3. Associa automaticamente o WAF ao ALB

**Exemplo b√°sico:**

```yaml
# ansible/playbooks/deploy-app-with-waf.yml
- name: Deploy aplica√ß√£o com WAF
  hosts: localhost
  tasks:
    - name: Deploy aplica√ß√£o
      kubernetes.core.k8s:
        state: present
        src: ../k8s/my-app-ingress.yml
    
    - name: Aguardar ALB ser criado
      kubernetes.core.k8s_info:
        kind: Ingress
        name: my-app-ingress
        namespace: production
      register: ingress
      until: ingress.resources[0].status.loadBalancer.ingress is defined
      retries: 30
      delay: 10
    
    - name: Obter ARN do WAF
      shell: |
        cd ../04-security
        terraform output -raw waf_arn
      register: waf_arn
    
    - name: Associar WAF ao Ingress
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: networking.k8s.io/v1
          kind: Ingress
          metadata:
            name: my-app-ingress
            namespace: production
            annotations:
              alb.ingress.kubernetes.io/wafv2-acl-arn: "{{ waf_arn.stdout }}"
```

üìñ **Para implementa√ß√£o completa, veja:** [GUIA-IMPLEMENTACAO-ANSIBLE.md](./docs/GUIA-IMPLEMENTACAO-ANSIBLE.md)

---

### Stack 05 - Monitoring (Prometheus + Grafana)

Configure Amazon Managed Prometheus e Amazon Managed Grafana para monitorar o Cluster EKS.

**ANTES DE APLICAR:**

1. Verifique se `05-monitoring/data.cluster.remote-state.tf` usa o bucket correto com seu Account ID
2. O arquivo `05-monitoring/grafana.workspace.tf` j√° est√° configurado com `authentication_providers = ["AWS_SSO"]`
   - ‚úÖ **AWS_SSO √© RECOMENDADO** (gratuito, integrado com AWS)
   - ‚ö†Ô∏è Se voc√™ usa IdP externo (Okta, Azure AD), altere para `["SAML"]` e configure federation metadata ap√≥s o deploy
3. Ap√≥s o `terraform apply`, voc√™ **deve** configurar o acesso ao Grafana (ver se√ß√£o "Configura√ß√£o do Grafana" abaixo)

```bash
cd ../05-monitoring
terraform init
terraform apply -auto-approve
```

**Recursos criados:** 7 (Prometheus Workspace, Prometheus Scraper, Grafana Workspace, IAM Roles, CloudWatch Log Group, EKS Addon)

**‚è±Ô∏è Tempo estimado:** 20-25 minutos (Prometheus Scraper ~17min, Grafana Workspace ~6min)

**‚úÖ Outputs importantes:**

```bash
terraform output
```

Voc√™ receber√°:
- `grafana_workspace_url`: URL de acesso ao Grafana
- `prometheus_workspace_endpoint`: Endpoint do Prometheus
- `grafana_workspace_id`: ID do workspace Grafana
- `prometheus_workspace_id`: ID do workspace Prometheus
- `grafana_api_key`: API Key para automa√ß√£o Ansible (sensitive)

**‚ö†Ô∏è PR√ìXIMO PASSO OBRIGAT√ìRIO:** V√° para a se√ß√£o "üìä Configura√ß√£o do Grafana" mais abaixo antes de usar o Grafana

---

## ‚úÖ Valida√ß√£o Final da Infraestrutura

Ap√≥s completar todos os stacks, valide a infraestrutura completa:

```bash
# 1. Verificar nodes do cluster
kubectl get nodes
# Esperado: 3 nodes Ready

# 2. Verificar pods de sistema
kubectl get pods -A
# Esperado: Todos Running

# 3. Verificar Karpenter
kubectl get nodepools
kubectl get ec2nodeclasses
# Esperado: Status Ready

# 4. Verificar Ingress e ALB
kubectl get ingress
# Esperado: ADDRESS preenchido

# 5. Testar acesso HTTP
ALB_URL=$(kubectl get ingress eks-devopsproject-ingress -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
curl -I http://$ALB_URL
# Esperado: HTTP/1.1 200 OK

# 6. Verificar addons EKS
aws eks list-addons --cluster-name eks-devopsproject-cluster --profile terraform
# Esperado: vpc-cni, kube-proxy, coredns, aws-ebs-csi-driver, eks-pod-identity-agent, prometheus-node-exporter
```

**üìä Resumo de Recursos Provisionados:**

| Stack | Recursos | Tempo Estimado | Notas |
|-------|----------|----------------|-------|
| 00 - Backend | 3 | < 1 min | Obrigat√≥rio |
| 01 - Networking | 21 | 2-3 min | Obrigat√≥rio |
| 02 - EKS Cluster | 21 | 15-20 min | Obrigat√≥rio |
| 03 - Karpenter | 10 | 3-5 min | Obrigat√≥rio |
| 04 - Security/WAF | 2 | 1 min | **Opcional*** |
| 05 - Monitoring | 7 | 20-25 min | Obrigat√≥rio |
| 06 - E-commerce App | 15 (K8s) | 3 min (Ansible) / 20 min (Manual) | **Opcional**‚Ä†‚Ä† |
| **TOTAL (sem Stacks opcionais)** | **62** | **~39-54 min** | Cluster funcional |
| **TOTAL (com Stack 04)** | **64** | **~40-55 min** | + WAF |
| **TOTAL (completo com app)** | **79** | **~42-58 min** | + Aplica√ß√£o demo |

> **\* Stack 04 (WAF) √© opcional** porque:
> - WAF protege ALBs, que s√≥ existem quando voc√™ deploya aplica√ß√µes com Ingress
> - Se voc√™ ainda n√£o tem apps, pode pular este stack
> - Voc√™ pode voltar e aplicar Stack 04 depois de deployar suas aplica√ß√µes
> - Para automa√ß√£o completa de apps + WAF, veja [GUIA-IMPLEMENTACAO-ANSIBLE.md](./docs/GUIA-IMPLEMENTACAO-ANSIBLE.md)
> 
> **‚Ä†‚Ä† Stack 06 (E-commerce App) √© opcional** porque:
> - √â uma aplica√ß√£o de demonstra√ß√£o para mostrar cluster em funcionamento
> - Demonstra o valor do Ansible (3 min vs 20 min manual - economia de 85%)
> - Ideal para apresenta√ß√µes e valida√ß√£o de observabilidade
> - Pode ser removida a qualquer momento sem afetar infraestrutura

---

### Stack 06 - E-commerce Application (Demonstra√ß√£o) - OPCIONAL

Deploy de uma aplica√ß√£o real (e-commerce com microservi√ßos) para demonstrar o cluster em funcionamento com observabilidade completa.

> üí° **NOVO DIFERENCIAL:** Este stack demonstra a **superioridade do Ansible** sobre processos manuais!
> 
> | Abordagem | Tempo | Comandos | Erros Poss√≠veis |
> |-----------|-------|----------|-----------------|
> | **Manual** | 15-20 min | ~15 kubectl apply + valida√ß√µes | Alta chance de erro |
> | **Ansible** | 2-3 min | 1 comando | Zero erros (idempotente) |
> | **Economia** | **~85%** | **93% menos comandos** | **100% confi√°vel** |

**Sobre a Aplica√ß√£o:**
- **7 microservi√ßos** (Frontend React + 6 APIs backend)
- Arquitetura moderna (microservices pattern)
- Imagens Docker prontas (rslim087/*)
- **Ingress com ALB** (reutiliza Stack 02)
- **Auto-scaling** (usa Karpenter da Stack 03)
- **WAF opcional** (pode usar Stack 04)
- **Monitoramento autom√°tico** (integrado com Stack 05)

**Pr√©-requisitos:**
- ‚úÖ Stacks 00-03 deployadas (obrigat√≥rio)
- ‚úÖ Stack 05 deployada (recomendado para monitoramento)
- ‚úÖ Ansible instalado (para automa√ß√£o)

---

#### Op√ß√£o A: Deploy Automatizado com Ansible (RECOMENDADO) üöÄ

```bash
# Deploy completo da aplica√ß√£o (namespace + deployments + services + ingress + valida√ß√µes)
ansible-playbook ansible/playbooks/03-deploy-ecommerce.yml
```

**O que o playbook faz automaticamente:**
1. ‚úÖ Valida conex√£o com cluster e ALB Controller
2. ‚úÖ Cria namespace `ecommerce`
3. ‚úÖ Deploy de 7 microservi√ßos (Deployments + Services)
4. ‚úÖ Aguarda pods ficarem prontos (health checks)
5. ‚úÖ Cria Ingress e provisiona ALB
6. ‚úÖ Aguarda ALB ficar acess√≠vel
7. ‚úÖ Executa testes de conectividade
8. ‚úÖ Salva informa√ß√µes de acesso em arquivo

**Tempo total:** ~3 minutos ‚è±Ô∏è

**Configurar Monitoramento (Opcional mas Recomendado):**

```bash
# Importa dashboards Grafana espec√≠ficos para monitorar a aplica√ß√£o
ansible-playbook ansible/playbooks/04-configure-ecommerce-monitoring.yml
```

**O que o playbook faz:**
1. ‚úÖ Importa 3 dashboards Grafana (Kubernetes App Metrics, Pods, Deployments)
2. ‚úÖ Cria dashboard customizado para e-commerce
3. ‚úÖ Configura queries Prometheus para m√©tricas dos microservi√ßos
4. ‚úÖ Documenta alertas recomendados

**Tempo total:** ~2 minutos ‚è±Ô∏è

---

#### Op√ß√£o B: Deploy Manual (Para Compara√ß√£o Educacional)

Se quiser ver a diferen√ßa e entender o valor do Ansible:

```bash
# 1. Criar namespace
kubectl create namespace ecommerce

# 2. Deploy dos microservi√ßos (7 arquivos)
kubectl apply -f 06-ecommerce-app/manifests/ecommerce-ui.yaml
kubectl apply -f 06-ecommerce-app/manifests/product-catalog.yaml
kubectl apply -f 06-ecommerce-app/manifests/order-management.yaml
kubectl apply -f 06-ecommerce-app/manifests/product-inventory.yaml
kubectl apply -f 06-ecommerce-app/manifests/profile-management.yaml
kubectl apply -f 06-ecommerce-app/manifests/shipping-and-handling.yaml
kubectl apply -f 06-ecommerce-app/manifests/team-contact-support.yaml

# 3. Aguardar pods ficarem prontos
kubectl wait --for=condition=ready pod --all -n ecommerce --timeout=300s

# 4. Deploy do Ingress
kubectl apply -f 06-ecommerce-app/manifests/ingress.yaml

# 5. Aguardar ALB ser provisionado (2-5 minutos)
kubectl get ingress ecommerce-ingress -n ecommerce -w

# 6. Obter URL do ALB
ALB_URL=$(kubectl get ingress ecommerce-ingress -n ecommerce -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
echo "Aplica√ß√£o dispon√≠vel em: http://$ALB_URL"

# 7. Testar acesso
curl -I http://$ALB_URL

# 8. Configurar DNS no Hostgator (manual via painel)
# CNAME: eks ‚Üí [ALB_URL]
```

**Tempo total:** ~15-20 minutos ‚è±Ô∏è

**Problemas comuns do processo manual:**
- ‚ùå Esquecer algum microservi√ßo
- ‚ùå N√£o aguardar pods ficarem prontos
- ‚ùå Testar ALB antes de propagar DNS
- ‚ùå N√£o salvar informa√ß√µes de acesso

---

#### Acessar a Aplica√ß√£o

Ap√≥s o deploy (Ansible ou manual):

**Via ALB Direto:**
```bash
# Obter URL
kubectl get ingress ecommerce-ingress -n ecommerce

# Acessar no navegador
http://[ALB-URL]
```

**Via DNS Personalizado (Recomendado):**

1. Acesse o painel DNS do Hostgator
2. Crie/Edite registro CNAME:
   - **Nome:** `eks`
   - **Tipo:** `CNAME`
   - **Destino:** `[ALB-URL]`
   - **TTL:** `300`

3. Aguarde propaga√ß√£o (~5-10 minutos)

4. Acesse: **http://eks.devopsproject.com.br**

---

#### Validar Aplica√ß√£o

```bash
# Status dos pods
kubectl get pods -n ecommerce

# Logs do frontend
kubectl logs -f deployment/ecommerce-ui -n ecommerce

# Logs de um microservi√ßo espec√≠fico
kubectl logs -f deployment/product-catalog -n ecommerce

# Informa√ß√µes do Ingress
kubectl describe ingress ecommerce-ingress -n ecommerce

# Health check
curl -I http://[ALB-URL]
```

---

#### Monitoramento no Grafana

Se voc√™ executou o playbook de monitoramento, acesse o Grafana e veja:

1. **Dashboard "Kubernetes App Metrics"**
   - CPU/Memory por microservi√ßo
   - Network I/O
   - Pod status

2. **Dashboard "E-commerce Application - Overview"**
   - M√©tricas espec√≠ficas dos 7 microservi√ßos
   - Contagem de restarts
   - Status de health checks

3. **Queries √∫teis para criar alertas:**
   ```promql
   # Pods running
   count(kube_pod_status_phase{namespace="ecommerce", phase="Running"})
   
   # CPU usage por pod
   sum(rate(container_cpu_usage_seconds_total{namespace="ecommerce"}[5m])) by (pod)
   
   # Restarts nas √∫ltimas 24h
   sum(increase(kube_pod_container_status_restarts_total{namespace="ecommerce"}[24h]))
   ```

---

#### Associar WAF ao E-commerce (Opcional)

Se voc√™ deployou Stack 04 (WAF), pode proteger a aplica√ß√£o:

```bash
# Obter ARN do WAF
cd 04-security
WAF_ARN=$(terraform output -raw waf_arn)

# Adicionar annotation ao Ingress
kubectl annotate ingress ecommerce-ingress \
  -n ecommerce \
  alb.ingress.kubernetes.io/wafv2-acl-arn="$WAF_ARN" \
  --overwrite

# Verificar associa√ß√£o
kubectl describe ingress ecommerce-ingress -n ecommerce | grep waf
```

**Prote√ß√µes ativadas:**
- ‚úÖ Rate limiting (200 req/5min por IP)
- ‚úÖ SQL Injection detection
- ‚úÖ Cross-Site Scripting (XSS) protection
- ‚úÖ Geographic blocking (se configurado)

---

#### Remover Aplica√ß√£o

**Via Ansible:**
```bash
kubectl delete namespace ecommerce
```

**Manual:**
```bash
kubectl delete -f 06-ecommerce-app/manifests/ -n ecommerce
kubectl delete namespace ecommerce
```

O ALB ser√° automaticamente removido.

---

#### üìä Comparativo Final: Ansible vs Manual

| Tarefa | Manual | Ansible | Diferen√ßa |
|--------|--------|---------|-----------|
| **Deploy aplica√ß√£o** | 15-20 min | 3 min | ‚ö° **83% mais r√°pido** |
| **Configurar monitoramento** | 15 min | 2 min | ‚ö° **87% mais r√°pido** |
| **Valida√ß√µes** | Manual (5 min) | Autom√°tico | ‚ö° **100% automatizado** |
| **Documenta√ß√£o** | Manual | Auto-gerada | ‚ö° **Zero esfor√ßo** |
| **Comandos executados** | ~15 | 1 | ‚ö° **93% menos comandos** |
| **Chance de erro** | Alta | Zero | ‚ö° **100% confi√°vel** |
| **Reprodutibilidade** | Baixa | Perfeita | ‚ö° **Idempotente** |
| **Total (deploy + monitor)** | **30-35 min** | **5 min** | ‚ö° **85% mais r√°pido** |

**Conclus√£o:** Ansible economiza ~30 minutos por deploy e elimina completamente erros humanos! üéØ

---

## ÔøΩÔøΩ Troubleshooting - Erros Comuns

### Erro 1: "the server has asked for the client to provide credentials" (kubectl)

**Causa:** Access entry da terraform-role n√£o foi criado no EKS.

**Solu√ß√£o:** 
1. Verifique se `02-eks-cluster/eks.cluster.access.tf` cont√©m o bloco terraform_role (veja se√ß√£o 5.4)
2. Reaplique Stack 02: `terraform apply -auto-approve`
3. Atualize kubeconfig: `aws eks update-kubeconfig --name eks-devopsproject-cluster --region us-east-1 --profile terraform`

---

### Erro 2: "S3 bucket eks-devopsproject-state-files does not exist"

**Causa:** Nome do bucket S3 n√£o inclui o Account ID ou n√£o foi substitu√≠do corretamente.

**Solu√ß√£o:**
1. Verifique o nome do bucket no Stack 00: `cat 00-backend/variables.tf | grep bucket`
2. Deve ser: `eks-devopsproject-state-files-<YOUR_ACCOUNT>`
3. Corrija todos os arquivos `main.tf` e `data.cluster.remote-state.tf` nos stacks 01-05
4. Execute o comando de substitui√ß√£o da se√ß√£o 5.2 novamente

---

### Erro 3: "SSO is not enabled in any region" (Grafana)

**Causa:** Tentativa de usar `AWS_SSO` como autentica√ß√£o do Grafana sem SSO configurado.

**Solu√ß√£o:**
1. Edite `05-monitoring/grafana.workspace.tf`
2. Altere: `authentication_providers = ["SAML"]`
3. Reaplique: `terraform apply -auto-approve`

---

### Erro 4: "The specified instance type is not eligible for Free Tier"

**Causa:** Conta AWS Free Tier n√£o suporta inst√¢ncias t3.medium.

**Solu√ß√£o:**
- **Op√ß√£o 1 (Recomendada):** Fa√ßa upgrade da conta AWS para Paid Plan
- **Op√ß√£o 2:** Altere em `02-eks-cluster/variables.tf`:
  ```hcl
  instance_types = ["t3.small"]  # ou ["t3.micro"]
  ```
  > ‚ö†Ô∏è **ATEN√á√ÉO:** Inst√¢ncias menores podem causar problemas de performance no cluster.

---

### Erro 5: "Error creating WAF Web ACL Association" (Stack 04)

**Causa:** Tentativa de associar WAF antes do ALB existir.

**Solu√ß√£o:** Siga a sequ√™ncia correta da se√ß√£o Stack 04:
1. Criar WAF (`terraform apply`)
2. Criar Ingress (`kubectl apply -f ingress-sample-deployment.yml`)
3. Aguardar ALB ser provisionado (`kubectl get ingress -w`)
4. Renomear arquivos `.disabled` para `.tf`
5. Aplicar associa√ß√£o (`terraform apply`)

---

### Erro 6: "InvalidParameterException: bash_user_arn not found"

**Causa:** Nome de usu√°rio IAM em `locals.tf` n√£o foi atualizado.

**Solu√ß√£o:**
1. Edite `02-eks-cluster/locals.tf`
2. Substitua `user/<YOUR_USER>` pelo nome do seu usu√°rio IAM
3. Reaplique: `terraform apply -auto-approve`

---

### Erro 7: Helm provider version conflicts

**Causa:** Incompatibilidade entre vers√µes do provider Helm.

**Solu√ß√£o:**
O projeto j√° est√° fixado no Helm provider v2.17.0. Se encontrar problemas:
```bash
cd 02-eks-cluster
terraform init -upgrade
```

---

## üóëÔ∏è Destruir Infraestrutura

Para destruir os recursos provisionados, siga **EXATAMENTE** esta ordem para evitar erros de depend√™ncia:

### Ordem de Destrui√ß√£o

```bash
# Stack 05 - Monitoring
cd ./05-monitoring
terraform destroy -auto-approve

# Stack 04 - Security (WAF)
cd ../04-security
terraform destroy -auto-approve

# Stack 03 - Karpenter
cd ../03-karpenter-auto-scaling
terraform destroy -auto-approve

# Stack 02 - EKS Cluster (ORDEM IMPORTANTE)
cd ../02-eks-cluster

# Primeiro: Destruir External DNS
terraform destroy -target=helm_release.external_dns -auto-approve

# Segundo: Destruir ALB Controller
terraform destroy -target=helm_release.load_balancer_controller -auto-approve

# Terceiro: Destruir resto do cluster
terraform destroy -auto-approve

# Stack 01 - Networking
cd ../01-networking
terraform destroy -auto-approve

# Stack 00 - Backend (OPCIONAL - mant√©m hist√≥rico de state)
# cd ../00-backend
# terraform destroy -auto-approve
```

**‚ö†Ô∏è ATEN√á√ÉO:** 
- **N√£o destrua** o Stack 00 se quiser manter o hist√≥rico de state do Terraform
- Sempre siga a ordem inversa do deployment
- Aguarde cada comando concluir antes de executar o pr√≥ximo
- Se houver erro, verifique se h√° recursos dependentes (ex: ALBs criados por Ingress) e delete-os manualmente

**‚è±Ô∏è Tempo total de destrui√ß√£o:** ~15-20 minutos

---

## üí∞ Estimativa de Custos

**Custos mensais aproximados (us-east-1):**

| Servi√ßo | Custo Estimado |
|---------|----------------|
| EKS Control Plane | $73/m√™s |
| EC2 (3x t3.medium) | ~$90/m√™s |
| NAT Gateways (2x) | ~$65/m√™s |
| EBS Volumes | ~$10/m√™s |
| ALB | ~$23/m√™s |
| Prometheus | ~$10/m√™s |
| Grafana | ~$9/m√™s |
| **TOTAL** | **~$280/m√™s** |

**üí° Economia:** Destrua os recursos quando n√£o estiver usando para economizar ~$9-10 por noite.

---


---

## üìä Configura√ß√£o do Grafana

**‚ö†Ô∏è OBRIGAT√ìRIO:** Ap√≥s aplicar a Stack 05, o Grafana Workspace √© criado **vazio** e **sem acesso configurado**. Voc√™ deve seguir esta se√ß√£o para configurar autentica√ß√£o e dashboards.

### Vis√£o Geral do Processo

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ ETAPA 1: Configurar Autentica√ß√£o SSO (OBRIGAT√ìRIA)             ‚îÇ
‚îÇ ‚è±Ô∏è Tempo: 5-10 minutos                                          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 1. Habilitar IAM Identity Center (SSO)                          ‚îÇ
‚îÇ 2. Criar usu√°rio SSO                                            ‚îÇ
‚îÇ 3. Atribuir usu√°rio ao Grafana Workspace                        ‚îÇ
‚îÇ 4. Promover usu√°rio para ADMIN (cr√≠tico!)                       ‚îÇ
‚îÇ 5. Acessar Grafana via AWS Access Portal                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ ETAPA 2: Configurar Data Source + Dashboards                   ‚îÇ
‚îÇ Escolha UMA das op√ß√µes abaixo:                                  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ OP√á√ÉO A (RECOMENDADA): Ansible Automation                      ‚îÇ
‚îÇ ‚è±Ô∏è Tempo: 2 minutos                                             ‚îÇ
‚îÇ ‚úÖ Data Source Prometheus configurado automaticamente           ‚îÇ
‚îÇ ‚úÖ Dashboard Node Exporter importado automaticamente            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ OP√á√ÉO B: Configura√ß√£o Manual                                   ‚îÇ
‚îÇ ‚è±Ô∏è Tempo: 10-15 minutos                                         ‚îÇ
‚îÇ ‚öôÔ∏è Configurar Data Source Prometheus manualmente                ‚îÇ
‚îÇ ‚öôÔ∏è Importar Dashboard 1860 (Node Exporter Full) manualmente     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## ETAPA 1: Configurar Autentica√ß√£o SSO (Obrigat√≥ria para Ambas Op√ß√µes)

### Passo 1: Habilitar AWS IAM Identity Center (SSO)

1. Acesse o console AWS: https://console.aws.amazon.com/singlesignon
2. Clique em **"Enable"** para ativar o IAM Identity Center
3. Anote o **Instance ID** que ser√° criado (formato: `ssoins-xxxxxxxxxxxx`)

### Passo 2: Criar Usu√°rio SSO

1. No IAM Identity Center, v√° em **Users** (menu lateral)
2. Clique em **"Add user"**
3. Preencha:
   - **Username**: `grafana-admin` (ou nome de sua prefer√™ncia)
   - **Email**: seu e-mail corporativo
   - **First name**: Seu nome
   - **Last name**: Seu sobrenome
4. Clique em **"Next"**
5. Em "Add user to groups": Pule esta etapa (Next)
6. Clique em **"Add user"**
7. Verifique seu e-mail e clique no link de verifica√ß√£o
8. Defina uma senha quando solicitado

### Passo 3: Obter URLs Importantes

```bash
cd 05-monitoring

# URL do Grafana Workspace
terraform output -raw grafana_workspace_url

# ID do Grafana Workspace
terraform output -raw grafana_workspace_id

# Endpoint do Prometheus (voc√™ usar√° no Passo 7)
terraform output -raw prometheus_workspace_endpoint
```

**Anote esses valores!** Voc√™ precisar√°:
- **grafana_workspace_id**: Para encontrar o workspace no console AWS
- **prometheus_workspace_endpoint**: Para configurar o Data Source no Passo 7

**Exemplo de output esperado:**
```
Grafana URL: https://g-7b4f900d4a.grafana-workspace.us-east-1.amazonaws.com/
Grafana ID: g-7b4f900d4a
Prometheus Endpoint: https://aps-workspaces.us-east-1.amazonaws.com/workspaces/ws-12345678-abcd-1234-efgh-123456789012
```

### Passo 4: Atribuir Usu√°rio ao Grafana Workspace

1. Acesse: https://console.aws.amazon.com/grafana/home?region=us-east-1
2. Clique no workspace que foi criado (ex: `g-8e1225a34f`)
3. V√° na aba **"Authentication"**
4. Na se√ß√£o **"AWS IAM Identity Center"**, clique em **"Assign new user or group"**
5. Selecione:
   - **Type**: User
   - **User**: Selecione o usu√°rio que criou (ex: `grafana-admin`)
6. Clique em **"Assign users and groups"**

### Passo 5: Alterar Permiss√£o para ADMIN ‚ö†Ô∏è OBRIGAT√ìRIO

1. Na mesma aba **"Authentication"**, localize o usu√°rio na tabela
2. Selecione o usu√°rio (marque o checkbox ao lado do nome)
3. Clique no bot√£o **"Actions"** (no topo da tabela)
4. Selecione **"Make admin"**
5. Confirme a altera√ß√£o

> ‚ö†Ô∏è **CR√çTICO:** Sem permiss√£o ADMIN, voc√™ N√ÉO conseguir√°:
> - Adicionar Data Sources (manual ou via Ansible)
> - Importar Dashboards (manual ou via Ansible)
> - Executar playbook Ansible (falhar√° com erro 403 Forbidden)

> üìù **Nota:** A interface AWS foi atualizada. Se voc√™ ainda v√™ os 3 pontinhos **[...]**, use essa op√ß√£o. Caso contr√°rio, use o bot√£o **Actions** ‚Üí **Make admin**.

---

### ‚úÖ Checkpoint: Autentica√ß√£o SSO Configurada

**Parab√©ns!** Voc√™ completou a ETAPA 1. Agora voc√™ tem:
- ‚úÖ IAM Identity Center (SSO) habilitado
- ‚úÖ Usu√°rio SSO criado e verificado
- ‚úÖ Usu√°rio atribu√≠do ao Grafana Workspace com permiss√£o ADMIN
- ‚úÖ Acesso ao Grafana via AWS Access Portal

**üéØ Pr√≥ximo Passo:** Configure o Grafana com Ansible (automa√ß√£o)

---

## ETAPA 2: Configura√ß√£o Autom√°tica com Ansible ‚≠ê

**‚è±Ô∏è Tempo:** 2 minutos  
**üìã Pr√©-requisitos:**
- ‚úÖ ETAPA 1 completa (SSO configurado com usu√°rio ADMIN)
- ‚úÖ Ansible instalado (ver [QUICK-START-ANSIBLE.md](./docs/QUICK-START-ANSIBLE.md))

**üöÄ Execu√ß√£o:**

```bash
cd ansible
ansible-playbook playbooks/01-configure-grafana.yml
```

**‚úÖ Resultado esperado:**
```
PLAY RECAP *********************************************************************
localhost : ok=3 changed=2 unreachable=0 failed=0

‚úÖ Data Source Prometheus configurado automaticamente
‚úÖ Dashboard Node Exporter Full (ID 1860) importado automaticamente
‚úÖ Grafana 100% pronto para uso
```

**üéâ Pronto!** Prossiga para a "Valida√ß√£o Final" abaixo.

---

### üîß Preferiu Configurar Manualmente?

Se voc√™ **n√£o pode** usar Ansible ou quer entender o processo passo a passo:

üìñ **Guia Completo:** [CONFIGURACAO-MANUAL-GRAFANA.md](./docs/CONFIGURACAO-MANUAL-GRAFANA.md)

**Tempo estimado:** 10-15 minutos (vs 2 minutos com Ansible)

O guia manual inclui:
- Passo a passo detalhado para configurar Data Source Prometheus
- Instru√ß√µes para importar Dashboard Node Exporter (ID 1860)
- Troubleshooting de erros comuns
- Queries PromQL para testes

---

## ‚úÖ Valida√ß√£o Final do Grafana

Ap√≥s executar o playbook Ansible, valide se tudo est√° funcionando:

**1. Verificar Data Source:**
- Menu lateral ‚Üí **Connections** ‚Üí **Data sources**
- Deve aparecer: **Prometheus** (verde, ativo)

**2. Verificar Dashboard:**
- Menu lateral ‚Üí **Dashboards**
- Deve aparecer: **Node Exporter Full**
- Clique no dashboard e verifique se os gr√°ficos est√£o mostrando dados

**3. Verificar M√©tricas:**
- No dashboard, voc√™ deve ver m√©tricas dos 3 nodes do EKS
- Gr√°ficos de CPU, Mem√≥ria, Disco devem estar populados com dados

üéâ **Sucesso!** Seu Grafana est√° 100% configurado e monitorando o cluster!

### üìä M√©tricas Dispon√≠veis no Dashboard Node Exporter Full

- üìä **CPU**: Usage, cores, idle, system, user, iowait
- üíæ **Mem√≥ria**: Total, usado, dispon√≠vel, cache, buffers
- üíø **Disco**: I/O read/write, utiliza√ß√£o, espa√ßo livre
- üåê **Rede**: Tr√°fego RX/TX, pacotes, erros, drops
- ‚ö° **Sistema**: Load average (1m, 5m, 15m), uptime, processes
- üìÅ **File System**: Inodes, mount points, file descriptors

### Troubleshooting

#### ‚ùå Grafana vazio (sem data sources, sem dashboards)
**Causa:** Isso √© **esperado**! O Terraform provisiona apenas o workspace Grafana vazio.

**Solu√ß√£o:** Voc√™ **deve** configurar manualmente:
1. **Data Source Prometheus**: Siga o Passo 7 acima
   - Menu lateral ‚Üí Connections ‚Üí Add data source ‚Üí Prometheus
   - Configure URL do Prometheus (obtido via `terraform output`)
   - Habilite SigV4 auth
2. **Dashboards**: Siga o Passo 8 acima
   - Menu lateral ‚Üí Dashboards ‚Üí New ‚Üí Import
   - Digite ID **1860** (Node Exporter Full)

**Tempo estimado:** 5 minutos para configura√ß√£o completa

---

#### ‚ùå Erro "sso.auth.access-denied" ao tentar acessar Grafana
**Causa:** Usu√°rio SSO existe, mas n√£o est√° atribu√≠do ao workspace Grafana ou tem permiss√£o VIEWER.

**Solu√ß√£o:**
1. Acesse: https://console.aws.amazon.com/grafana/home?region=us-east-1
2. Clique no workspace criado (ex: `g-7b4f900d4a`)
3. V√° na aba **"Authentication"**
4. Verifique se seu usu√°rio SSO est√° na lista
   - Se **N√ÉO**: Clique em "Assign new user or group" e adicione
   - Se **SIM**: Verifique se a role √© **ADMIN** (n√£o VIEWER)
5. Aguarde 1-2 minutos e tente novamente

---

#### ‚ùå Erro "403 Forbidden" ao executar Ansible
**Causa:** Usu√°rio SSO tem permiss√£o VIEWER ao inv√©s de ADMIN.

**Solu√ß√£o:**
1. Acesse: https://console.aws.amazon.com/grafana/home?region=us-east-1
2. Clique no workspace ‚Üí aba "Authentication"
3. Selecione o usu√°rio ‚Üí Actions ‚Üí Make admin
4. Aguarde 1-2 minutos
5. Re-execute o playbook Ansible

---

#### ‚ùå Erro 404 ao clicar "Go to connections"
**Solu√ß√£o**: Acesse diretamente via menu lateral ‚Üí Connections

#### ‚ùå Bot√£o "Add data source" desabilitado
**Solu√ß√£o**: Usu√°rio est√° com role VIEWER. Altere para ADMIN (Passo 5)

#### ‚ùå "Missing Authentication Token" ao testar Prometheus
**Solu√ß√£o**: Certifique-se de:
- Marcar **SigV4 auth**
- Preencher **Service: aps**
- URL sem barra `/` no final

#### ‚ùå "Page not found" ou "HttpNotFoundException"
**Solu√ß√£o**: Verifique se a URL do Prometheus est√° correta (sem `/api/v1/query` no final)

### Queries PromQL √öteis

Teste no **Explore** do Grafana:

```promql
# CPU usage por node
100 - (avg by (instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)

# Mem√≥ria dispon√≠vel em %
node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes * 100

# Disco usado em %
(node_filesystem_size_bytes - node_filesystem_free_bytes) / node_filesystem_size_bytes * 100

# Load average 5 minutos
node_load5

# Tr√°fego de rede (recebido)
rate(node_network_receive_bytes_total[5m])
```


## üìö Recursos Adicionais

### Testes e Valida√ß√£o

O projeto inclui arquivos de exemplo (YAML manifests) para valida√ß√£o manual dos componentes:

üìñ **Guia Completo de Testes:** [TESTES-VALIDACAO-MANUAL.md](./docs/TESTES-VALIDACAO-MANUAL.md)

O guia inclui:
- ‚úÖ Valida√ß√£o de EBS CSI Driver (Persistent Volumes)
- ‚úÖ Valida√ß√£o de ALB Ingress Controller + WAF
- ‚úÖ Valida√ß√£o de Karpenter Auto-Scaling
- ‚úÖ Valida√ß√£o de External DNS
- ‚úÖ Valida√ß√£o de Prometheus Node Exporter
- üìä Checklist completo de valida√ß√£o

> üí° **Dica:** Para ambientes de produ√ß√£o, considere automatizar estes testes com Ansible ou CI/CD pipelines ao inv√©s de execut√°-los manualmente.

### Comandos √öteis

```bash
# Verificar vers√£o do cluster
aws eks describe-cluster \
    --name eks-devopsproject-cluster \
    --query 'cluster.version' \
    --profile terraform

# Listar todos os addons instalados
aws eks list-addons \
    --cluster-name eks-devopsproject-cluster \
    --profile terraform

# Verificar logs do Karpenter
kubectl logs -n kube-system -l app.kubernetes.io/name=karpenter --tail=100 -f

# Ver detalhes do NodePool do Karpenter
kubectl describe nodepool default-node-pool

# Verificar WAF rules aplicadas
aws wafv2 list-web-acls --scope REGIONAL --region us-east-1 --profile terraform

# Acessar Grafana (ap√≥s deploy do Stack 05)
cd 05-monitoring
terraform output -raw grafana_workspace_url

# Ou obter URL do Prometheus
terraform output -raw prometheus_workspace_endpoint
```

### Links da Documenta√ß√£o Oficial

- [Amazon EKS User Guide](https://docs.aws.amazon.com/eks/latest/userguide/)
- [Karpenter Documentation](https://karpenter.sh/)
- [AWS WAF Developer Guide](https://docs.aws.amazon.com/waf/latest/developerguide/)
- [Amazon Managed Prometheus](https://docs.aws.amazon.com/prometheus/)
- [Amazon Managed Grafana](https://docs.aws.amazon.com/grafana/)

---

## ü§ù Suporte

Se encontrar problemas durante o deployment:

1. Verifique a se√ß√£o **Troubleshooting** acima
2. Confirme que seguiu **exatamente** a sequ√™ncia de deployment
3. Verifique se todas as substitui√ß√µes de vari√°veis foram feitas (Account ID, Bucket S3, IAM User)
4. Consulte os logs do Terraform: `terraform apply` sem `-auto-approve` para ver detalhes
5. Verifique se sua conta AWS tem os limites de servi√ßo adequados

---

## üìù Notas Importantes

- ‚úÖ Projeto testado e validado com Terraform 1.12.2
- ‚úÖ Compat√≠vel com EKS 1.32
- ‚úÖ Helm provider fixado em v2.17.0 para evitar breaking changes
- ‚úÖ Todos os stacks usam remote state em S3 com state locking em DynamoDB
- ‚úÖ IAM Roles seguem princ√≠pio de least privilege exceto AdministratorAccess na terraform-role
- ‚ö†Ô∏è Requer AWS Paid Plan ou cr√©ditos suficientes para inst√¢ncias t3.medium
- ‚ö†Ô∏è Custo estimado: ~$280/m√™s se mantido ligado 24/7
- üí° Economia: ~$9-10/noite destruindo recursos fora do hor√°rio de uso

---

## ‚ö†Ô∏è PROBLEMA COMUM: Esgotamento de IPs na Subnet

### üî¥ Sintoma

Ap√≥s o deploy completo, voc√™ pode receber um ou mais destes erros/avisos:

**1. Alerta no Console EKS:**
```
InsufficientFreeAddresses
One or more of the subnets associated with your cluster does not have enough 
available IP addresses for Amazon EKS to perform cluster management operations. 
Free up addresses in the subnet(s), or associate different subnets to your 
cluster using the Amazon EKS update-cluster-config API.
```

**2. Erro ao provisionar inst√¢ncias EC2:**
```
InsufficientFreeAddresses - We currently do not have sufficient IP addresses 
in the subnet subnet-xxxxxxxxx (10.0.0.96/27) to launch the instance.
```

**3. Pods travados em ContainerCreating:**
```
Failed to create pod sandbox: plugin type="aws-cni" failed (add): 
failed to assign an IP address to container
```

**Causa Raiz:** AWS VPC CNI com configura√ß√£o padr√£o pr√©-aloca ENIs com at√© 6 IPs secund√°rios por node, consumindo rapidamente os 27 IPs √∫teis de uma subnet /27.

---

### üìä An√°lise do Problema

#### Diagn√≥stico R√°pido

Verifique quantos IPs est√£o dispon√≠veis na subnet problem√°tica (normalmente `private-subnet-us-east-1b`):

```bash
# 1. Listar todas as subnets privadas
aws ec2 describe-subnets \
    --filters "Name=tag:Name,Values=*private*" \
    --query 'Subnets[].[SubnetId,CidrBlock,AvailableIpAddressCount,Tags[?Key==`Name`].Value|[0]]' \
    --output table \
    --profile terraform

# 2. Ver detalhes de uma subnet espec√≠fica
aws ec2 describe-subnets \
    --subnet-ids subnet-xxxxxxxxx \
    --query 'Subnets[0].[SubnetId,CidrBlock,AvailableIpAddressCount]' \
    --output table \
    --profile terraform

# 3. Contar ENIs e IPs secund√°rios por node
aws ec2 describe-network-interfaces \
    --filters "Name=subnet-id,Values=subnet-xxxxxxxxx" \
    --query 'NetworkInterfaces[].[NetworkInterfaceId,PrivateIpAddress,PrivateIpAddresses[].PrivateIpAddress|length(@),Description]' \
    --output table \
    --profile terraform
```

**Indicadores de problema:**
- ‚úÖ **Saud√°vel:** AvailableIpAddressCount > 10 (>40% da capacidade)
- ‚ö†Ô∏è **Aten√ß√£o:** AvailableIpAddressCount 5-10 (20-40% da capacidade)
- üî¥ **Cr√≠tico:** AvailableIpAddressCount < 5 (<20% da capacidade)

---

### üõ†Ô∏è Op√ß√µes de Solu√ß√£o

Voc√™ tem **3 op√ß√µes** para resolver este problema. Escolha baseado no seu cen√°rio:

---

#### **OP√á√ÉO 1: Otimiza√ß√£o AWS VPC CNI (RECOMENDADA)** ‚≠ê

**Quando usar:** Ambiente de desenvolvimento/testes, subnet /27, poucos nodes (2-4)

**Pr√≥s:**
- ‚úÖ **Custo:** $0 (zero investimento)
- ‚úÖ **Downtime:** Zero (configura√ß√£o online)
- ‚úÖ **Complexidade:** Baixa (5 minutos)
- ‚úÖ **Ganho:** Reduz consumo de IPs em ~15-20%
- ‚úÖ **Revers√≠vel:** Sim, facilmente

**Contras:**
- ‚ö†Ô∏è **Ganho limitado:** Libera apenas 4-5 IPs em subnet /27
- ‚ö†Ô∏è **Necessita reciclagem de nodes:** Para efeito imediato
- ‚ö†Ô∏è **N√£o escala:** Solu√ß√£o paliativa, n√£o resolve crescimento futuro

**Passo a Passo:**

```bash
# 1. Aplicar configura√ß√£o otimizada no AWS VPC CNI
kubectl set env daemonset aws-node -n kube-system \
  WARM_ENI_TARGET=0 \
  WARM_IP_TARGET=5 \
  MINIMUM_IP_TARGET=10

# 2. Verificar rollout
kubectl rollout status daemonset aws-node -n kube-system --timeout=3m

# 3. Confirmar configura√ß√£o aplicada
kubectl get daemonset aws-node -n kube-system -o jsonpath='{.spec.template.spec.containers[0].env[?(@.name=="WARM_ENI_TARGET")].value}' && echo
kubectl get daemonset aws-node -n kube-system -o jsonpath='{.spec.template.spec.containers[0].env[?(@.name=="WARM_IP_TARGET")].value}' && echo

# 4. OPCIONAL: Reciclar nodes para efeito imediato (ou aguardar libera√ß√£o natural)
# ATEN√á√ÉO: Isto causar√° recria√ß√£o dos nodes e reagendamento de todos os pods

# 4a. Obter lista de nodes
kubectl get nodes -o wide

# 4b. Obter IDs das inst√¢ncias EC2 dos nodes
aws ec2 describe-instances \
    --filters "Name=tag:eks:cluster-name,Values=eks-devopsproject-cluster-<YOUR_ACCOUNT>" \
              "Name=instance-state-name,Values=running" \
    --query 'Reservations[].Instances[].[InstanceId,PrivateIpAddress]' \
    --output table \
    --profile terraform

# 4c. Terminar as inst√¢ncias (ASG criar√° novas automaticamente com CNI otimizado)
aws ec2 terminate-instances \
    --instance-ids i-xxxxxxxxx i-yyyyyyyyy i-zzzzzzzzz \
    --profile terraform

# 4d. Aguardar novos nodes (2-3 minutos)
watch kubectl get nodes

# 5. Validar IPs liberados (ap√≥s 3-5 minutos)
aws ec2 describe-subnets \
    --subnet-ids subnet-xxxxxxxxx \
    --query 'Subnets[0].AvailableIpAddressCount' \
    --profile terraform
```

**Resultado esperado:** De 1-2 IPs dispon√≠veis para 5-7 IPs dispon√≠veis (ganho de +400%)

**Reverter (se necess√°rio):**
```bash
kubectl set env daemonset aws-node -n kube-system \
  WARM_ENI_TARGET=1 \
  WARM_IP_TARGET- \
  MINIMUM_IP_TARGET-
```

---

#### **OP√á√ÉO 2: Expandir Subnet para /26 ou /25 (SOLU√á√ÉO DEFINITIVA)** ‚≠ê‚≠ê‚≠ê

**Quando usar:** Produ√ß√£o, staging, ou qualquer ambiente que precisar√° escalar

**Pr√≥s:**
- ‚úÖ **Ganho significativo:** /26 = 59 IPs √∫teis (+118%) | /25 = 123 IPs √∫teis (+355%)
- ‚úÖ **Escalabilidade:** Suporta crescimento futuro
- ‚úÖ **Estabilidade:** Solu√ß√£o definitiva, n√£o paliativa
- ‚úÖ **Sem reconfigura√ß√µes:** N√£o precisa otimizar CNI

**Contras:**
- ‚ö†Ô∏è **Requer recria√ß√£o da subnet:** Necess√°rio destruir e recriar Stack 01 e seguintes
- ‚ö†Ô∏è **Downtime:** ~30-40 minutos (destrui√ß√£o + recria√ß√£o)
- ‚ö†Ô∏è **Trabalhoso:** Precisa recriar todas as stacks dependentes
- ‚ö†Ô∏è **Perda de dados tempor√°rios:** Pods e volumes ef√™meros s√£o perdidos

**Passo a Passo:**

```bash
# 1. Destruir stacks (ordem inversa)
# Certifique-se de estar na raiz do projeto
cd ./05-monitoring && terraform destroy -auto-approve
cd ../04-security && terraform destroy -auto-approve
cd ../03-karpenter-auto-scaling && terraform destroy -auto-approve
cd ../02-eks-cluster && terraform destroy -target=helm_release.external_dns -auto-approve
cd ../02-eks-cluster && terraform destroy -target=helm_release.load_balancer_controller -auto-approve
cd ../02-eks-cluster && terraform destroy -auto-approve
cd ../01-networking && terraform destroy -auto-approve

# 2. Editar arquivo de subnets privadas
# Abrir: 01-networking/vpc.private-subnets.tf
# Alterar os CIDRs das 3 subnets privadas:

# DE (subnet /27 = 27 IPs √∫teis):
# private-subnet-us-east-1a = "10.0.0.32/27"   # 10.0.0.32 - 10.0.0.63
# private-subnet-us-east-1b = "10.0.0.96/27"   # 10.0.0.96 - 10.0.0.127
# private-subnet-us-east-1c = "10.0.0.160/27"  # 10.0.0.160 - 10.0.0.191

# PARA /26 (59 IPs √∫teis - RECOMENDADO PARA STAGING):
# private-subnet-us-east-1a = "10.0.1.0/26"    # 10.0.1.0 - 10.0.1.63
# private-subnet-us-east-1b = "10.0.1.64/26"   # 10.0.1.64 - 10.0.1.127
# private-subnet-us-east-1c = "10.0.1.128/26"  # 10.0.1.128 - 10.0.1.191

# OU PARA /25 (123 IPs √∫teis - RECOMENDADO PARA PRODU√á√ÉO):
# private-subnet-us-east-1a = "10.0.2.0/25"    # 10.0.2.0 - 10.0.2.127
# private-subnet-us-east-1b = "10.0.2.128/25"  # 10.0.2.128 - 10.0.2.255
# private-subnet-us-east-1c = "10.0.3.0/25"    # 10.0.3.0 - 10.0.3.127

# 3. Recriar todas as stacks (seguir sequ√™ncia de deploy completa)
cd ../01-networking && terraform init && terraform apply -auto-approve
cd ../02-eks-cluster && terraform init && terraform apply -auto-approve
# ... continuar com stacks 03, 04, 05

# 4. Validar capacidade da nova subnet
aws ec2 describe-subnets \
    --filters "Name=tag:Name,Values=*private-subnet-us-east-1b*" \
    --query 'Subnets[].[SubnetId,CidrBlock,AvailableIpAddressCount]' \
    --output table \
    --profile terraform
```

**Resultado esperado:**
- /26: ~55-57 IPs dispon√≠veis (de 27 para 59 IPs √∫teis)
- /25: ~119-121 IPs dispon√≠veis (de 27 para 123 IPs √∫teis)

---

#### **OP√á√ÉO 3: Adicionar Mais Availability Zones (M√âDIA COMPLEXIDADE)**

**Quando usar:** Precisa de alta disponibilidade em m√∫ltiplas AZs, mas n√£o quer recriar subnets

**Pr√≥s:**
- ‚úÖ **Aumenta capacidade total:** Distribui carga entre mais subnets
- ‚úÖ **Alta disponibilidade:** Mais AZs = mais resili√™ncia
- ‚úÖ **Mant√©m subnets existentes:** N√£o precisa destruir stacks

**Contras:**
- ‚ö†Ô∏è **N√£o resolve subnet espec√≠fica:** Se us-east-1b est√° cheia, continua cheia
- ‚ö†Ô∏è **Custo:** +$32/m√™s por NAT Gateway adicional
- ‚ö†Ô∏è **Complexidade moderada:** Requer edi√ß√£o de m√∫ltiplos arquivos

**Passo a Passo:**

```bash
# 1. Adicionar us-east-1d, us-east-1e, ou us-east-1f em:
#    - 01-networking/vpc.private-subnets.tf
#    - 01-networking/vpc.public-subnets.tf  
#    - 01-networking/vpc.nat-gateways.tf
#    - 01-networking/vpc.private-route-tables.tf

# 2. Aplicar mudan√ßas
cd ./01-networking && terraform apply -auto-approve

# 3. Node Group do EKS automaticamente distribuir√° nodes nas novas subnets
```

**Resultado esperado:** Carga distribu√≠da, mas custo adicional de ~$32/m√™s por AZ

---

### üéØ Matriz de Decis√£o: Qual Op√ß√£o Escolher?

| Cen√°rio | Op√ß√£o Recomendada | Justificativa |
|---------|-------------------|---------------|
| **Dev/Testes com poucos pods** | Op√ß√£o 1 (CNI) | R√°pido, gr√°tis, resolve temporariamente |
| **Staging com crescimento** | Op√ß√£o 2 (/26) | Balanceia capacidade e custo |
| **Produ√ß√£o cr√≠tica** | Op√ß√£o 2 (/25) + Op√ß√£o 1 | M√°xima capacidade + otimiza√ß√£o |
| **Multi-regi√£o HA** | Op√ß√£o 3 + Op√ß√£o 1 | Resili√™ncia + efici√™ncia |
| **Or√ßamento zero** | Op√ß√£o 1 (CNI) | √önica op√ß√£o sem custo |
| **Problema urgente** | Op√ß√£o 1 (CNI) | Resolve em 5 minutos |

---

### üìã Checklist de Valida√ß√£o P√≥s-Solu√ß√£o

Ap√≥s aplicar qualquer op√ß√£o, valide:

```bash
# ‚úÖ 1. Subnet tem IPs suficientes (>10)
aws ec2 describe-subnets \
    --subnet-ids subnet-xxxxxxxxx \
    --query 'Subnets[0].AvailableIpAddressCount' \
    --profile terraform

# ‚úÖ 2. Todos os nodes est√£o Ready
kubectl get nodes

# ‚úÖ 3. Todos os pods est√£o Running (nenhum ContainerCreating)
kubectl get pods -A | grep -v Running | grep -v Completed

# ‚úÖ 4. CNI est√° configurado corretamente (se usou Op√ß√£o 1)
kubectl get daemonset aws-node -n kube-system \
    -o jsonpath='{.spec.template.spec.containers[0].env[?(@.name=="WARM_ENI_TARGET")].value}' && echo

# ‚úÖ 5. N√£o h√° alertas AWS sobre IPs
# Verificar CloudWatch ou AWS Personal Health Dashboard
```

---

### üí° Li√ß√µes Aprendidas e Melhores Pr√°ticas

1. **Dimensionamento de Subnets:**
   - Dev: /27 (27 IPs) - Suficiente para 2-3 nodes + CNI otimizado
   - Staging: /26 (59 IPs) - Suporta 5-8 nodes confortavelmente
   - Produ√ß√£o: /25 (123 IPs) - Recomendado para escalabilidade

2. **Monitoramento Proativo:**
   ```bash
   # Criar alarme CloudWatch para IPs < 10
   aws cloudwatch put-metric-alarm \
       --alarm-name subnet-low-ips \
       --metric-name AvailableIpAddressCount \
       --namespace AWS/EC2 \
       --statistic Average \
       --period 300 \
       --evaluation-periods 1 \
       --threshold 10 \
       --comparison-operator LessThanThreshold \
       --profile terraform
   ```

3. **Otimiza√ß√£o CNI como Padr√£o:**
   - Sempre aplique Op√ß√£o 1 (CNI otimizado) **mesmo** com subnets /26 ou /25
   - Reduz desperd√≠cio e aumenta efici√™ncia em qualquer cen√°rio

4. **Planejamento de Capacidade:**
   - Calcule: `(Nodes √ó 10 IPs/node) + 5 IPs reserva`
   - Exemplo: 5 nodes = m√≠nimo 55 IPs = subnet /26

---

### üîç Troubleshooting Adicional

**Problema:** Ap√≥s otimizar CNI, IPs n√£o foram liberados

**Solu√ß√£o:**
```bash
# 1. Verificar se configura√ß√£o foi aplicada
kubectl get daemonset aws-node -n kube-system -o yaml | grep -A3 "WARM_"

# 2. Restart pods aws-node
kubectl delete pods -n kube-system -l k8s-app=aws-node

# 3. Aguardar 5 minutos e verificar novamente
sleep 300
aws ec2 describe-subnets --subnet-ids subnet-xxxxxxxxx \
    --query 'Subnets[0].AvailableIpAddressCount' \
    --profile terraform

# 4. Se ainda n√£o liberou, reciclar nodes (Op√ß√£o 1, passo 4)
```

**Problema:** Pods ficam `ContainerCreating` mesmo com IPs dispon√≠veis

**Solu√ß√£o:**
```bash
# 1. Verificar eventos do pod
kubectl describe pod <pod-name> -n <namespace>

# 2. Verificar logs do aws-node no node espec√≠fico
kubectl logs -n kube-system -l k8s-app=aws-node --all-containers=true | grep ERROR

# 3. Deletar pod para for√ßar reagendamento
kubectl delete pod <pod-name> -n <namespace>
```

---

## üôè Cr√©ditos

Este projeto √© um fork do trabalho original de **[Kenerry Serain](https://github.com/kenerry-serain)**, desenvolvido como material do curso **DevOps na Nuvem**.

Agradecimentos especiais pela estrutura e conhecimento compartilhado que tornou este projeto poss√≠vel.

**Reposit√≥rio Original:** [kenerry-serain (GitHub)](https://github.com/kenerry-serain)

---

## üìÑ Licen√ßa

Este projeto √© fornecido como material educacional. Uso livre para fins de estudo e desenvolvimento pessoal.

---

**Desenvolvido com ‚ù§Ô∏è para aprendizado de DevOps e Infraestrutura como C√≥digo**

