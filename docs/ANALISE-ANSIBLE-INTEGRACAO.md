# ğŸ” AnÃ¡lise Completa: IntegraÃ§Ã£o Ansible no Projeto EKS

## ğŸ“Š Executive Summary

ApÃ³s anÃ¡lise detalhada das 6 stacks do projeto, identifiquei **5 Ã¡reas estratÃ©gicas** onde Ansible agrega valor real seguindo **prÃ¡ticas de mercado modernas**. Esta anÃ¡lise foi feita considerando:

- âœ… **GitOps e IaC best practices** (Terraform para infraestrutura, Ansible para configuraÃ§Ã£o)
- âœ… **SeparaÃ§Ã£o de responsabilidades** (provisioning vs configuration management)
- âœ… **AutomaÃ§Ã£o de tarefas manuais repetitivas**
- âœ… **Reprodutibilidade em mÃºltiplos ambientes**
- âœ… **Realidade do mercado** (como empresas modernas estÃ£o trabalhando em 2024-2025)

---

## ğŸ¯ Ãreas Identificadas para Ansible

### **1ï¸âƒ£ ConfiguraÃ§Ã£o do Grafana (Stack 05 - Monitoring)** â­â­â­â­â­

**Status Atual:** 100% manual (8 passos), ~15-20 minutos  
**Prioridade:** ğŸ”´ **CRÃTICA** - Maior ganho de automaÃ§Ã£o  

#### **Problema Atual**
```bash
# ApÃ³s terraform apply da Stack 05, usuÃ¡rio precisa:
1. Habilitar AWS IAM Identity Center (SSO) manualmente via console
2. Criar usuÃ¡rio SSO manualmente
3. Atribuir usuÃ¡rio ao Grafana Workspace via console AWS
4. Alterar permissÃ£o para ADMIN via console AWS
5. Acessar Grafana via AWS Access Portal
6. Configurar Data Source Prometheus manualmente (URL, SigV4 auth)
7. Importar Dashboard 1860 (Node Exporter) manualmente
8. Validar mÃ©tricas manualmente
```

**â±ï¸ Tempo:** 15-20 minutos  
**âŒ Problemas:**
- Propenso a erros humanos (URL errada, autenticaÃ§Ã£o incorreta)
- NÃ£o reprodutÃ­vel (cada ambiente precisa reconfiguraÃ§Ã£o manual)
- NÃ£o versionado (mudanÃ§as no Grafana nÃ£o sÃ£o rastreadas)
- Onboarding lento (novos ambientes demoram para configurar)

#### **SoluÃ§Ã£o com Ansible**

```yaml
# ansible/playbooks/configure-grafana.yml
---
- name: ConfiguraÃ§Ã£o completa do Grafana Workspace
  hosts: localhost
  gather_facts: false
  
  tasks:
    # 1. Configurar Data Source Prometheus automaticamente
    - name: Adicionar Data Source Prometheus
      community.grafana.grafana_datasource:
        grafana_url: "{{ grafana_workspace_url }}"
        grafana_api_key: "{{ grafana_api_key }}"
        name: "Prometheus"
        ds_type: "prometheus"
        ds_url: "{{ prometheus_endpoint }}"
        access: "proxy"
        additional_json_data:
          httpMethod: "POST"
          sigV4Auth: true
          sigV4AuthType: "workspace-iam-role"
          sigV4Region: "us-east-1"
        state: present

    # 2. Importar Dashboard Node Exporter (1860) automaticamente
    - name: Importar Dashboard Node Exporter Full
      community.grafana.grafana_dashboard:
        grafana_url: "{{ grafana_workspace_url }}"
        grafana_api_key: "{{ grafana_api_key }}"
        dashboard_id: 1860
        dashboard_revision: 37
        overwrite: true
        state: present

    # 3. Importar Dashboards customizados (opcional)
    - name: Importar Dashboard Kubernetes Cluster Monitoring
      community.grafana.grafana_dashboard:
        grafana_url: "{{ grafana_workspace_url }}"
        grafana_api_key: "{{ grafana_api_key }}"
        dashboard_id: 7249
        dashboard_revision: 1
        overwrite: true
        state: present

    # 4. Criar Alertas customizados
    - name: Configurar Alerta - High CPU Usage
      uri:
        url: "{{ grafana_workspace_url }}/api/v1/provisioning/alert-rules"
        method: POST
        headers:
          Authorization: "Bearer {{ grafana_api_key }}"
          Content-Type: "application/json"
        body_format: json
        body:
          title: "High CPU Usage on EKS Nodes"
          condition: "A"
          data:
            - refId: "A"
              queryType: "prometheus"
              datasourceUid: "prometheus"
              expr: '100 - (avg(rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80'
          for: "5m"
          annotations:
            description: "CPU usage above 80% for 5 minutes"
```

**âœ… Ganhos:**
- â±ï¸ **Tempo:** 15-20 min â†’ **2 min** (automaÃ§Ã£o completa)
- ğŸ”„ **Reprodutibilidade:** 100% idempotente (mesmo resultado sempre)
- ğŸ“ **Versionamento:** Dashboards e alertas como cÃ³digo
- ğŸš€ **Onboarding:** Novos ambientes em 1 comando
- ğŸ›¡ï¸ **Conformidade:** ConfiguraÃ§Ã£o padronizada entre dev/staging/prod

---

### **2ï¸âƒ£ Deploy de AplicaÃ§Ãµes de Exemplo/DemonstraÃ§Ã£o (Stack 02)** â­â­â­â­

**Status Atual:** 100% manual via kubectl  
**Prioridade:** ğŸŸ¡ **ALTA** - DemonstraÃ§Ã£o didÃ¡tica  
**PrÃ¡ticas de Mercado:** âœ… Comum em pipelines CI/CD

#### **Problema Atual**
```bash
# UsuÃ¡rio precisa manualmente aplicar YAMLs de exemplo:
kubectl apply -f 02-eks-cluster/samples/ingress-sample-deployment.yml
kubectl apply -f 02-eks-cluster/samples/csi-sample-deployment.yml
kubectl apply -f 03-karpenter-auto-scaling/samples/karpenter-nginx-deployment.yml

# E depois configurar manualmente:
# - Anotar Ingress com WAF ARN
# - Aguardar ALB ser provisionado
# - Testar endpoints manualmente
```

**âŒ Problemas:**
- Arquivos de exemplo espalhados em mÃºltiplas stacks
- Sem validaÃ§Ã£o automÃ¡tica de deployment
- ConfiguraÃ§Ã£o WAF manual (anotaÃ§Ãµes)
- Sem rollback automatizado

#### **SoluÃ§Ã£o com Ansible**

```yaml
# ansible/playbooks/deploy-sample-apps.yml
---
- name: Deploy de aplicaÃ§Ãµes de demonstraÃ§Ã£o
  hosts: localhost
  gather_facts: false
  
  tasks:
    # 1. Deploy Nginx com Ingress + ALB
    - name: Deploy Nginx Sample App
      kubernetes.core.k8s:
        kubeconfig: "{{ kubeconfig_path }}"
        state: present
        definition:
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            name: nginx-sample
            namespace: sample-app
          spec:
            replicas: 3
            selector:
              matchLabels:
                app: nginx
            template:
              metadata:
                labels:
                  app: nginx
              spec:
                containers:
                  - name: nginx
                    image: nginx:1.27
                    ports:
                      - containerPort: 80

    # 2. Configurar Ingress com WAF automaticamente
    - name: Obter ARN do WAF
      command: >
        aws wafv2 list-web-acls 
        --scope REGIONAL 
        --region us-east-1 
        --query 'WebACLs[?Name==`waf-eks-devopsproject-webacl`].ARN' 
        --output text
      register: waf_arn
      changed_when: false

    - name: Criar Ingress com WAF annotation
      kubernetes.core.k8s:
        kubeconfig: "{{ kubeconfig_path }}"
        state: present
        definition:
          apiVersion: networking.k8s.io/v1
          kind: Ingress
          metadata:
            name: nginx-ingress
            namespace: sample-app
            annotations:
              alb.ingress.kubernetes.io/scheme: internet-facing
              alb.ingress.kubernetes.io/target-type: ip
              alb.ingress.kubernetes.io/wafv2-acl-arn: "{{ waf_arn.stdout }}"
          spec:
            ingressClassName: alb
            rules:
              - http:
                  paths:
                    - path: /
                      pathType: Prefix
                      backend:
                        service:
                          name: nginx-sample
                          port:
                            number: 80

    # 3. Aguardar ALB ser provisionado
    - name: Aguardar Ingress ter endereÃ§o ALB
      kubernetes.core.k8s_info:
        kubeconfig: "{{ kubeconfig_path }}"
        kind: Ingress
        name: nginx-ingress
        namespace: sample-app
      register: ingress_status
      until: ingress_status.resources[0].status.loadBalancer.ingress is defined
      retries: 30
      delay: 10

    # 4. Validar endpoint automaticamente
    - name: Testar endpoint ALB
      uri:
        url: "http://{{ ingress_status.resources[0].status.loadBalancer.ingress[0].hostname }}"
        method: GET
        status_code: 200
      register: alb_test
      retries: 5
      delay: 10
      until: alb_test.status == 200

    - name: Exibir URL do ALB
      debug:
        msg: "âœ… ALB disponÃ­vel em: http://{{ ingress_status.resources[0].status.loadBalancer.ingress[0].hostname }}"
```

**âœ… Ganhos:**
- ğŸ”„ **Deploy idempotente:** Pode reexecutar sem efeitos colaterais
- ğŸ›¡ï¸ **WAF automÃ¡tico:** AnotaÃ§Ã£o aplicada automaticamente
- âœ… **ValidaÃ§Ã£o automÃ¡tica:** Testa endpoint HTTP antes de concluir
- ğŸ“Š **Healthcheck:** Verifica se pods estÃ£o Running antes de prosseguir

---

### **3ï¸âƒ£ ConfiguraÃ§Ã£o de Karpenter Resources (Stack 03)** â­â­â­

**Status Atual:** Semi-automÃ¡tico (terraform + shell scripts)  
**Prioridade:** ğŸŸ¢ **MÃ‰DIA** - Melhoria de orquestraÃ§Ã£o  
**PrÃ¡ticas de Mercado:** âœ… Ansible mais idiomÃ¡tico que shell scripts

#### **Problema Atual**

```bash
# Stack 03 usa terraform_data + shell scripts:
# karpenter.resources.tf
resource "terraform_data" "karpenter_resources" {
  provisioner "local-exec" {
    command = "${path.module}/cli/karpenter-resources-create.sh"
    when    = create
    environment = {
      REGION              = var.region
      CLUSTER_NAME        = local.eks_cluster_name
      KARPENTER_NODE_ROLE = local.karpenter_node_role_name
    }
  }
}

# cli/karpenter-resources-create.sh
#!/bin/bash
kubectl apply -f resources/karpenter-node-pool.yml
kubectl apply -f resources/karpenter-node-class.yml
```

**âŒ Problemas:**
- Shell scripts nÃ£o sÃ£o idempotentes (sem validaÃ§Ã£o de estado)
- Sem rollback automÃ¡tico em caso de falha
- DifÃ­cil validar sintaxe antes de executar
- NÃ£o valida se CRDs existem antes de aplicar resources

#### **SoluÃ§Ã£o com Ansible**

```yaml
# ansible/roles/karpenter-resources/tasks/main.yml
---
- name: Verificar CRDs do Karpenter existem
  kubernetes.core.k8s_info:
    kind: CustomResourceDefinition
    name: "{{ item }}"
  register: crd_check
  failed_when: crd_check.resources | length == 0
  loop:
    - nodepools.karpenter.sh
    - ec2nodeclasses.karpenter.k8s.aws

- name: Aplicar Karpenter NodePool
  kubernetes.core.k8s:
    state: present
    definition: "{{ lookup('template', 'karpenter-node-pool.yml.j2') }}"
    validate:
      fail_on_error: true
      strict: true

- name: Aplicar Karpenter EC2NodeClass
  kubernetes.core.k8s:
    state: present
    definition: "{{ lookup('template', 'karpenter-node-class.yml.j2') }}"
    validate:
      fail_on_error: true
      strict: true

- name: Aguardar NodePool estar Ready
  kubernetes.core.k8s_info:
    kind: NodePool
    name: default
  register: nodepool_status
  until: nodepool_status.resources[0].status.conditions | selectattr('type', 'equalto', 'Ready') | list | length > 0
  retries: 10
  delay: 5

- name: Validar Karpenter estÃ¡ provisionando nodes
  kubernetes.core.k8s_info:
    kind: Pod
    namespace: kube-system
    label_selectors:
      - app.kubernetes.io/name=karpenter
  register: karpenter_pods
  failed_when: karpenter_pods.resources | length == 0
```

**Templates dinÃ¢micos:**
```yaml
# ansible/roles/karpenter-resources/templates/karpenter-node-pool.yml.j2
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: {{ karpenter_nodepool_name | default('default') }}
spec:
  template:
    spec:
      requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values: {{ karpenter_capacity_types | to_json }}  # ['on-demand', 'spot']
        - key: karpenter.k8s.aws/instance-category
          operator: In
          values: {{ karpenter_instance_categories | to_json }}  # ['m', 't', 'c']
      nodeClassRef:
        group: karpenter.k8s.aws
        kind: EC2NodeClass
        name: {{ karpenter_nodeclass_name | default('default') }}
      expireAfter: {{ karpenter_expire_after | default('8h') }}
  limits:
    cpu: {{ karpenter_cpu_limit | default(1000) }}
  disruption:
    consolidationPolicy: {{ karpenter_consolidation_policy | default('WhenEmptyOrUnderutilized') }}
    consolidateAfter: {{ karpenter_consolidate_after | default('1m') }}
```

**âœ… Ganhos:**
- âœ… **ValidaÃ§Ã£o prÃ©via:** Verifica CRDs antes de aplicar resources
- ğŸ”„ **IdempotÃªncia:** kubernetes.core.k8s Ã© totalmente idempotente
- ğŸ“ **Templates dinÃ¢micos:** ConfiguraÃ§Ãµes variÃ¡veis por ambiente
- ğŸ›¡ï¸ **Rollback automÃ¡tico:** Ansible reverte em caso de falha
- ğŸ›ï¸ **Controle fino:** VariÃ¡veis para dev (spot) vs prod (on-demand)

---

### **4ï¸âƒ£ ConfiguraÃ§Ã£o de Secrets e ConfigMaps para AplicaÃ§Ãµes** â­â­â­â­

**Status Atual:** NÃ£o existe no projeto atual  
**Prioridade:** ğŸ”´ **CRÃTICA** - SeguranÃ§a e melhores prÃ¡ticas  
**PrÃ¡ticas de Mercado:** âœ… **OBRIGATÃ“RIO** em ambientes corporativos

#### **CenÃ¡rio Real**

Empresas modernas **NUNCA** commitam secrets em Git. O fluxo correto Ã©:

```
1. Secrets armazenados no AWS Secrets Manager / Parameter Store
2. Ansible busca secrets do AWS
3. Ansible cria Kubernetes Secrets no cluster
4. AplicaÃ§Ãµes consomem secrets via volumes ou env vars
```

#### **SoluÃ§Ã£o com Ansible**

```yaml
# ansible/playbooks/configure-secrets.yml
---
- name: Configurar Secrets e ConfigMaps
  hosts: localhost
  gather_facts: false
  
  tasks:
    # 1. Buscar secrets do AWS Secrets Manager
    - name: Obter Database Password do AWS Secrets Manager
      amazon.aws.secretsmanager_secret:
        name: "/eks-devopsproject/{{ env }}/database/password"
        region: us-east-1
      register: db_password

    - name: Obter API Key de serviÃ§o externo
      amazon.aws.secretsmanager_secret:
        name: "/eks-devopsproject/{{ env }}/external-api/key"
        region: us-east-1
      register: api_key

    # 2. Criar Kubernetes Secret a partir de AWS Secrets Manager
    - name: Criar Secret para Database Credentials
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: v1
          kind: Secret
          metadata:
            name: database-credentials
            namespace: production-app
          type: Opaque
          stringData:
            DB_PASSWORD: "{{ db_password.secret }}"
            DB_USER: "admin"
            DB_HOST: "{{ rds_endpoint }}"
            DB_NAME: "application_db"

    # 3. Criar ConfigMap para configuraÃ§Ãµes nÃ£o-sensÃ­veis
    - name: Criar ConfigMap de configuraÃ§Ã£o da aplicaÃ§Ã£o
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: app-config
            namespace: production-app
          data:
            APP_ENV: "{{ env }}"
            LOG_LEVEL: "{{ log_level | default('info') }}"
            FEATURE_FLAG_NEW_UI: "{{ feature_new_ui | default('false') }}"
            CACHE_TTL: "3600"
            MAX_CONNECTIONS: "100"

    # 4. Criar Secret TLS para Ingress (certificado SSL)
    - name: Buscar certificado SSL do ACM
      command: >
        aws acm get-certificate 
        --certificate-arn {{ acm_certificate_arn }} 
        --region us-east-1 
        --query 'Certificate' 
        --output text
      register: ssl_cert
      changed_when: false

    - name: Criar TLS Secret para Ingress
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: v1
          kind: Secret
          metadata:
            name: tls-certificate
            namespace: production-app
          type: kubernetes.io/tls
          stringData:
            tls.crt: "{{ ssl_cert.stdout }}"
            tls.key: "{{ ssl_private_key }}"

    # 5. Criar ServiceAccount com IRSA (IAM Roles for Service Accounts)
    - name: Criar ServiceAccount com anotaÃ§Ã£o IRSA
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: v1
          kind: ServiceAccount
          metadata:
            name: app-service-account
            namespace: production-app
            annotations:
              eks.amazonaws.com/role-arn: "{{ app_iam_role_arn }}"
```

**âœ… Ganhos:**
- ğŸ” **SeguranÃ§a:** Secrets NUNCA no Git (buscados do AWS em runtime)
- ğŸ”„ **RotaÃ§Ã£o automÃ¡tica:** Update secrets sem rebuild de imagens
- ğŸ›ï¸ **ConfiguraÃ§Ã£o por ambiente:** Dev usa RDS staging, prod usa RDS prod
- âœ… **Conformidade:** Atende SOC2, ISO 27001, PCI-DSS
- ğŸ“ **AuditÃ¡vel:** Ansible Tower/AWX registra quem aplicou quais secrets

**ğŸ’¡ Exemplo de uso na aplicaÃ§Ã£o:**
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: backend-api
spec:
  template:
    spec:
      serviceAccountName: app-service-account  # IRSA
      containers:
        - name: api
          image: myapp:v1.2.3
          envFrom:
            - configMapRef:
                name: app-config  # ConfiguraÃ§Ãµes nÃ£o-sensÃ­veis
            - secretRef:
                name: database-credentials  # Secrets sensÃ­veis
```

---

### **5ï¸âƒ£ ValidaÃ§Ã£o e Testes PÃ³s-Deployment (Quality Gates)** â­â­â­â­â­

**Status Atual:** 100% manual  
**Prioridade:** ğŸ”´ **CRÃTICA** - Confiabilidade em produÃ§Ã£o  
**PrÃ¡ticas de Mercado:** âœ… **OBRIGATÃ“RIO** em CI/CD pipelines

#### **Problema Atual**

```bash
# UsuÃ¡rio precisa manualmente validar:
kubectl get nodes  # Nodes estÃ£o Ready?
kubectl get pods -A  # Todos pods Running?
kubectl get ingress  # ALB foi criado?
curl http://ALB_URL  # Endpoint responde?

# Nenhuma validaÃ§Ã£o automÃ¡tica de:
# - Karpenter estÃ¡ funcionando?
# - Prometheus estÃ¡ coletando mÃ©tricas?
# - Grafana tem dashboards?
# - WAF estÃ¡ bloqueando requests maliciosos?
```

**âŒ Problemas:**
- Sem garantia que infraestrutura estÃ¡ saudÃ¡vel
- Problemas descobertos tarde (quando usuÃ¡rio acessa)
- Sem SLA de deployment (quanto tempo atÃ© estar pronto?)

#### **SoluÃ§Ã£o com Ansible**

```yaml
# ansible/playbooks/validate-cluster.yml
---
- name: ValidaÃ§Ã£o completa do cluster EKS
  hosts: localhost
  gather_facts: false
  
  tasks:
    # ============================================
    # PHASE 1: INFRASTRUCTURE VALIDATION
    # ============================================
    - name: "[INFRA] Validar todos nodes estÃ£o Ready"
      kubernetes.core.k8s_info:
        kind: Node
      register: nodes
      failed_when: >
        nodes.resources | selectattr('status.conditions', 'defined') 
        | selectattr('status.conditions', 'selectattr', 'type', 'equalto', 'Ready') 
        | list | length != (nodes.resources | length)

    - name: "[INFRA] Validar EBS CSI Driver estÃ¡ instalado"
      kubernetes.core.k8s_info:
        kind: DaemonSet
        name: ebs-csi-node
        namespace: kube-system
      register: ebs_csi
      failed_when: ebs_csi.resources | length == 0

    # ============================================
    # PHASE 2: NETWORKING VALIDATION
    # ============================================
    - name: "[NETWORK] Validar AWS Load Balancer Controller estÃ¡ Running"
      kubernetes.core.k8s_info:
        kind: Deployment
        name: aws-load-balancer-controller
        namespace: kube-system
      register: alb_controller
      failed_when: >
        alb_controller.resources[0].status.readyReplicas != 
        alb_controller.resources[0].spec.replicas

    - name: "[NETWORK] Validar CoreDNS estÃ¡ respondendo"
      command: kubectl run -it --rm dns-test --image=busybox --restart=Never -- nslookup kubernetes.default
      register: dns_test
      changed_when: false
      failed_when: "'kubernetes.default.svc.cluster.local' not in dns_test.stdout"

    # ============================================
    # PHASE 3: KARPENTER VALIDATION
    # ============================================
    - name: "[KARPENTER] Validar Karpenter Controller estÃ¡ Running"
      kubernetes.core.k8s_info:
        kind: Pod
        namespace: kube-system
        label_selectors:
          - app.kubernetes.io/name=karpenter
      register: karpenter_pods
      failed_when: >
        karpenter_pods.resources | selectattr('status.phase', 'equalto', 'Running') 
        | list | length == 0

    - name: "[KARPENTER] Testar auto-scaling (criar deployment de teste)"
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            name: karpenter-test
            namespace: default
          spec:
            replicas: 10  # ForÃ§a Karpenter a criar novos nodes
            selector:
              matchLabels:
                app: karpenter-test
            template:
              metadata:
                labels:
                  app: karpenter-test
              spec:
                containers:
                  - name: pause
                    image: k8s.gcr.io/pause:3.9
                    resources:
                      requests:
                        cpu: 100m
                        memory: 128Mi

    - name: "[KARPENTER] Aguardar Karpenter provisionar novos nodes"
      kubernetes.core.k8s_info:
        kind: Node
      register: nodes_after_scaling
      until: nodes_after_scaling.resources | length > nodes.resources | length
      retries: 20
      delay: 10

    - name: "[KARPENTER] Cleanup deployment de teste"
      kubernetes.core.k8s:
        state: absent
        kind: Deployment
        name: karpenter-test
        namespace: default

    # ============================================
    # PHASE 4: SECURITY VALIDATION (WAF)
    # ============================================
    - name: "[SECURITY] Obter URL do ALB"
      kubernetes.core.k8s_info:
        kind: Ingress
        name: nginx-ingress
        namespace: sample-app
      register: ingress

    - name: "[SECURITY] Testar request legÃ­timo (deve passar)"
      uri:
        url: "http://{{ ingress.resources[0].status.loadBalancer.ingress[0].hostname }}"
        method: GET
        status_code: 200
      register: legitimate_request

    - name: "[SECURITY] Testar SQL Injection (WAF deve bloquear)"
      uri:
        url: "http://{{ ingress.resources[0].status.loadBalancer.ingress[0].hostname }}?id=1' OR '1'='1"
        method: GET
        status_code: 403  # WAF deve retornar 403 Forbidden
      register: sql_injection_test
      failed_when: sql_injection_test.status != 403

    - name: "[SECURITY] Testar XSS Attack (WAF deve bloquear)"
      uri:
        url: "http://{{ ingress.resources[0].status.loadBalancer.ingress[0].hostname }}?search=<script>alert('XSS')</script>"
        method: GET
        status_code: 403
      register: xss_test
      failed_when: xss_test.status != 403

    # ============================================
    # PHASE 5: MONITORING VALIDATION
    # ============================================
    - name: "[MONITORING] Validar Prometheus estÃ¡ coletando mÃ©tricas"
      uri:
        url: "{{ prometheus_endpoint }}/api/v1/query?query=up"
        method: GET
        headers:
          Authorization: "AWS4-HMAC-SHA256 {{ aws_signature }}"
        status_code: 200
      register: prometheus_test

    - name: "[MONITORING] Validar Grafana tem dashboards configurados"
      uri:
        url: "{{ grafana_url }}/api/dashboards/uid/rYdddlPWk"  # Node Exporter Full (1860)
        method: GET
        headers:
          Authorization: "Bearer {{ grafana_api_key }}"
        status_code: 200
      register: dashboard_test

    - name: "[MONITORING] Validar mÃ©tricas de nodes estÃ£o sendo coletadas"
      uri:
        url: "{{ prometheus_endpoint }}/api/v1/query?query=node_cpu_seconds_total"
        method: GET
        status_code: 200
      register: node_metrics
      failed_when: node_metrics.json.data.result | length == 0

    # ============================================
    # PHASE 6: APPLICATION HEALTH
    # ============================================
    - name: "[APP] Validar todos pods crÃ­ticos estÃ£o Running"
      kubernetes.core.k8s_info:
        kind: Pod
        namespace: "{{ item }}"
      register: pods
      failed_when: >
        pods.resources | selectattr('status.phase', 'ne', 'Running') 
        | list | length > 0
      loop:
        - kube-system
        - sample-app
        - production-app

    # ============================================
    # PHASE 7: REPORT FINAL
    # ============================================
    - name: "Gerar relatÃ³rio de validaÃ§Ã£o"
      debug:
        msg: |
          ========================================
          âœ… VALIDAÃ‡ÃƒO COMPLETA - CLUSTER SAUDÃVEL
          ========================================
          
          ğŸ“Š INFRAESTRUTURA:
            - Nodes Ready: {{ nodes.resources | length }}
            - EBS CSI Driver: âœ… Instalado
          
          ğŸŒ NETWORKING:
            - ALB Controller: âœ… Running ({{ alb_controller.resources[0].status.readyReplicas }}/{{ alb_controller.resources[0].spec.replicas }})
            - CoreDNS: âœ… Respondendo
          
          ğŸš€ KARPENTER:
            - Controller: âœ… Running
            - Auto-scaling: âœ… Testado (provisionou {{ nodes_after_scaling.resources | length - nodes.resources | length }} novos nodes)
          
          ğŸ›¡ï¸ SECURITY (WAF):
            - Request legÃ­timo: âœ… 200 OK
            - SQL Injection: âœ… Bloqueado (403)
            - XSS Attack: âœ… Bloqueado (403)
          
          ğŸ“ˆ MONITORING:
            - Prometheus: âœ… Coletando mÃ©tricas
            - Grafana: âœ… Dashboards configurados
            - Node Exporter: âœ… {{ node_metrics.json.data.result | length }} mÃ©tricas coletadas
          
          ğŸƒ APLICAÃ‡Ã•ES:
            - Pods kube-system: âœ… Todos Running
            - Pods sample-app: âœ… Todos Running
          
          ğŸŒ ENDPOINTS:
            - ALB URL: http://{{ ingress.resources[0].status.loadBalancer.ingress[0].hostname }}
            - Grafana: {{ grafana_url }}
          
          â±ï¸ Tempo total de validaÃ§Ã£o: {{ ansible_play_duration }} segundos
          ========================================
```

**âœ… Ganhos:**
- âœ… **ConfianÃ§a em produÃ§Ã£o:** Deploy sÃ³ Ã© marcado como sucesso apÃ³s validaÃ§Ãµes
- ğŸš¨ **DetecÃ§Ã£o precoce:** Problemas identificados antes de afetar usuÃ¡rios
- ğŸ“Š **MÃ©tricas de SLA:** Tempo exato atÃ© cluster estar pronto
- ğŸ”„ **CI/CD integration:** Jenkins/GitLab CI pode executar validaÃ§Ãµes

---

**Desenvolvido com â¤ï¸ para educaÃ§Ã£o DevOps de qualidade**
