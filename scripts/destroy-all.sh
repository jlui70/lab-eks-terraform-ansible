#!/bin/bash

# Script para destruir todos os recursos na ordem correta
# VersÃ£o: 3.3
# Data: 02 de Dezembro de 2025
# Stacks: 00-backend atÃ© 06-ecommerce-app (Terraform + Kubernetes resources)
# Changelog v3.3: DocumentaÃ§Ã£o atualizada (Stack 06 jÃ¡ estava sendo deletada via namespace ecommerce)
# Changelog v3.2: Limpeza IAM dinÃ¢mica (lÃª nomes do Terraform state - suporta nomes customizados)
# Changelog v3.1: Limpeza IAM automÃ¡tica (previne erro EntityAlreadyExists)
# Changelog v3.0: RemoÃ§Ã£o automÃ¡tica de resources Ã³rfÃ£os do state (Stack 04, 03, 02)

set -e  # Para em caso de erro

echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘     ğŸ—‘ï¸  DESTRUINDO INFRAESTRUTURA EKS - 6 STACKS               â•‘"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""

PROJECT_ROOT="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

# FunÃ§Ã£o para destruir uma stack
destroy_stack() {
    local stack_name=$1
    local stack_path=$2
    
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo "ğŸ—‘ï¸  Destruindo: $stack_name"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    
    cd "$PROJECT_ROOT/$stack_path"
    
    if [ -f "terraform.tfstate" ] || terraform state list &>/dev/null; then
        terraform destroy -auto-approve || {
            echo "âš ï¸  Erro ao destruir $stack_name, tentando remover state Ã³rfÃ£o..."
            terraform state list 2>/dev/null | while read resource; do
                terraform state rm "$resource" 2>/dev/null || true
            done
            echo "âœ… $stack_name limpo (recursos jÃ¡ removidos)"
        }
        echo "âœ… $stack_name destruÃ­do com sucesso!"
    else
        echo "âš ï¸  $stack_name: Nenhum recurso para destruir"
    fi
    
    echo ""
}

# IMPORTANTE: Primeiro deletar recursos Kubernetes que criam recursos AWS
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "ğŸ§¹ PASSO 1: Deletando recursos Kubernetes (Ingress â†’ ALB)"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""

# Verificar se kubectl consegue acessar o cluster
if kubectl cluster-info &>/dev/null; then
    echo "  âœ… Cluster acessÃ­vel via kubectl"
    
    # Deletar namespace ecommerce (aplicaÃ§Ã£o com 7 microserviÃ§os + Ingress â†’ ALB)
    if kubectl get namespace ecommerce &>/dev/null; then
        echo "  ğŸ—‘ï¸  Deletando namespace ecommerce (7 microserviÃ§os + ALB)..."
        kubectl delete namespace ecommerce --timeout=90s 2>/dev/null || true
    fi
    
    # Deletar recursos do namespace sample-app (se existir)
    if kubectl get namespace sample-app &>/dev/null; then
        echo "  ğŸ—‘ï¸  Deletando namespace sample-app..."
        kubectl delete ingress eks-devopsproject-ingress -n sample-app --ignore-not-found=true 2>/dev/null || true
        kubectl delete service nginx -n sample-app --ignore-not-found=true 2>/dev/null || true
        kubectl delete deployment nginx -n sample-app --ignore-not-found=true 2>/dev/null || true
        kubectl delete namespace sample-app --timeout=90s 2>/dev/null || true
    fi
    
    # Deletar kube-state-metrics se existir (instalado manualmente via Helm)
    if helm list -n kube-system | grep -q kube-state-metrics; then
        echo "  ğŸ—‘ï¸  Desinstalando kube-state-metrics..."
        helm uninstall kube-state-metrics -n kube-system 2>/dev/null || true
    fi

    echo "  â³ Aguardando ALB(s) serem deletados pela AWS (45s)..."
    sleep 45
    echo "  âœ… Recursos Kubernetes deletados"
else
    echo "  âš ï¸  Cluster inaccessÃ­vel via kubectl (pode jÃ¡ ter sido destruÃ­do)"
    echo "  â„¹ï¸  Prosseguindo com destroy do Terraform (limparÃ¡ ALB se existir)"
fi
echo ""

# Ordem correta de destruiÃ§Ã£o (REVERSA da criaÃ§Ã£o: 05 â†’ 00)
echo "ğŸ“‹ Ordem de destruiÃ§Ã£o: 05-monitoring â†’ 04-security â†’ 03-karpenter â†’ 02-eks â†’ 01-networking â†’ 00-backend"
echo ""

destroy_stack "Stack 05 - Monitoring (Grafana + Prometheus)" "05-monitoring"

# CRÃTICO: Aguardar ENIs do Prometheus Scraper serem liberadas pela AWS
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "â³ Aguardando liberaÃ§Ã£o de ENIs do Prometheus Scraper..."
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "â„¹ï¸  O Prometheus Scraper cria ENIs gerenciadas que levam ~5min para"
echo "   serem liberadas pela AWS apÃ³s o terraform destroy."
echo ""

MAX_WAIT=600  # 10 minutos
INTERVAL=15   # Verificar a cada 15 segundos
elapsed=0

while [ $elapsed -lt $MAX_WAIT ]; do
    # Verificar ENIs com tipo amp_collector (Prometheus)
    ENI_COUNT=$(aws ec2 describe-network-interfaces \
        --filters "Name=interface-type,Values=amp_collector" \
        --query 'length(NetworkInterfaces)' \
        --output text \
        --profile terraform 2>/dev/null || echo "0")
    
    if [ "$ENI_COUNT" = "0" ]; then
        echo "âœ… Todas as ENIs do Prometheus foram liberadas!"
        break
    fi
    
    echo "  â³ Ainda hÃ¡ $ENI_COUNT ENI(s) do Prometheus... aguardando ${elapsed}s/${MAX_WAIT}s"
    sleep $INTERVAL
    elapsed=$((elapsed + INTERVAL))
done

if [ $elapsed -ge $MAX_WAIT ]; then
    echo "âš ï¸  TIMEOUT: ENIs ainda nÃ£o foram liberadas apÃ³s ${MAX_WAIT}s"
    echo "   Prosseguindo mesmo assim (pode causar erro na Stack 01 - VPC)"
    echo "   Se VPC nÃ£o deletar, aguarde mais 5min e execute:"
    echo "   â†’ ./cleanup-vpc-final.sh"
else
    echo "âœ… Pronto para deletar recursos de rede!"
fi
echo ""

# Stack 04: Remover WAF association do state (ALB jÃ¡ foi deletado via kubectl)
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "ğŸ§¹ Stack 04: Limpando state de WAF association Ã³rfÃ£..."
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
cd "$PROJECT_ROOT/04-security"
terraform state rm aws_wafv2_web_acl_association.alb 2>/dev/null && echo "  âœ… WAF association removida do state" || echo "  â„¹ï¸  WAF association jÃ¡ removida ou nÃ£o existe"
terraform state rm data.aws_lb.eks 2>/dev/null && echo "  âœ… Data source ALB removido do state" || echo "  â„¹ï¸  Data source jÃ¡ removido"
echo ""

destroy_stack "Stack 04 - Security (WAF)" "04-security"

# Stack 03: Remover helm release do state (pode estar Ã³rfÃ£o se cluster foi destruÃ­do)
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "ğŸ§¹ Stack 03: Limpando state de Karpenter helm release Ã³rfÃ£o..."
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
cd "$PROJECT_ROOT/03-karpenter-auto-scaling"
terraform state rm helm_release.karpenter 2>/dev/null && echo "  âœ… Karpenter helm release removido do state" || echo "  â„¹ï¸  Helm release jÃ¡ removido ou nÃ£o existe"
echo ""

destroy_stack "Stack 03 - Karpenter (Auto-scaling)" "03-karpenter-auto-scaling"

# Stack 02: Remover helm releases do state (cluster inacessÃ­vel apÃ³s addons destruÃ­dos)
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "ğŸ§¹ Stack 02: Limpando state de helm releases Ã³rfÃ£os..."
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
cd "$PROJECT_ROOT/02-eks-cluster"
terraform state rm helm_release.load_balancer_controller 2>/dev/null && echo "  âœ… ALB Controller helm release removido do state" || echo "  â„¹ï¸  ALB Controller jÃ¡ removido ou nÃ£o existe"
terraform state rm helm_release.external_dns 2>/dev/null && echo "  âœ… External DNS helm release removido do state" || echo "  â„¹ï¸  External DNS jÃ¡ removido ou nÃ£o existe"
echo ""

destroy_stack "Stack 02 - EKS Cluster" "02-eks-cluster"

# IMPORTANTE: Limpar IAM roles/policies Ã³rfÃ£s que o Terraform pode nÃ£o ter deletado
# Isso evita erro "EntityAlreadyExists" em reinstalaÃ§Ãµes
# VERSÃƒO DINÃ‚MICA v3.2: LÃª nomes reais do Terraform state (funciona mesmo se usuÃ¡rio alterar variables.tf)
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "ğŸ§¹ Limpando IAM Roles/Policies Ã³rfÃ£s (prevenÃ§Ã£o de conflitos)..."
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

# FunÃ§Ã£o auxiliar para deletar role IAM (detach policies primeiro)
delete_iam_role() {
    local role_name=$1
    
    if [ -z "$role_name" ]; then
        return 0
    fi
    
    if aws iam get-role --role-name "$role_name" --profile terraform &>/dev/null; then
        echo "  ğŸ—‘ï¸  Deletando role: $role_name"
        
        # Detach managed policies
        ATTACHED_POLICIES=$(aws iam list-attached-role-policies \
            --role-name "$role_name" \
            --profile terraform \
            --query 'AttachedPolicies[].PolicyArn' \
            --output text 2>/dev/null || echo "")
        
        for policy_arn in $ATTACHED_POLICIES; do
            aws iam detach-role-policy \
                --role-name "$role_name" \
                --policy-arn "$policy_arn" \
                --profile terraform 2>/dev/null || true
        done
        
        # Delete inline policies
        INLINE_POLICIES=$(aws iam list-role-policies \
            --role-name "$role_name" \
            --profile terraform \
            --query 'PolicyNames' \
            --output text 2>/dev/null || echo "")
        
        for policy_name in $INLINE_POLICIES; do
            aws iam delete-role-policy \
                --role-name "$role_name" \
                --policy-name "$policy_name" \
                --profile terraform 2>/dev/null || true
        done
        
        # Remove from instance profiles AND delete the profiles
        INSTANCE_PROFILES=$(aws iam list-instance-profiles-for-role \
            --role-name "$role_name" \
            --profile terraform \
            --query 'InstanceProfiles[].InstanceProfileName' \
            --output text 2>/dev/null || echo "")
        
        for profile_name in $INSTANCE_PROFILES; do
            echo "    â†’ Removendo role do instance profile: $profile_name"
            aws iam remove-role-from-instance-profile \
                --instance-profile-name "$profile_name" \
                --role-name "$role_name" \
                --profile terraform 2>/dev/null || true
            
            # Deletar o instance profile (Ã³rfÃ£o criado pelo EKS)
            echo "    â†’ Deletando instance profile Ã³rfÃ£o: $profile_name"
            aws iam delete-instance-profile \
                --instance-profile-name "$profile_name" \
                --profile terraform 2>/dev/null || true
        done
        
        # Delete role
        aws iam delete-role --role-name "$role_name" --profile terraform 2>/dev/null && \
            echo "    âœ… Role $role_name deletada" || \
            echo "    âš ï¸  Role $role_name nÃ£o pÃ´de ser deletada"
    fi
}

# FunÃ§Ã£o auxiliar para extrair nome de role do Terraform state
get_role_name_from_state() {
    local stack_path=$1
    local resource_address=$2
    
    cd "$PROJECT_ROOT/$stack_path"
    
    # Tentar obter nome da role do state
    local role_name=$(terraform state show "$resource_address" 2>/dev/null | grep -E "^\s+name\s+=" | head -1 | awk -F'"' '{print $2}')
    
    echo "$role_name"
}

# FunÃ§Ã£o auxiliar para extrair nome de policy do Terraform state
get_policy_name_from_state() {
    local stack_path=$1
    local resource_address=$2
    
    cd "$PROJECT_ROOT/$stack_path"
    
    # Tentar obter nome da policy do state
    local policy_name=$(terraform state show "$resource_address" 2>/dev/null | grep -E "^\s+name\s+=" | head -1 | awk -F'"' '{print $2}')
    
    echo "$policy_name"
}

# Obter account ID dinamicamente
ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text --profile terraform 2>/dev/null || echo "")

if [ -z "$ACCOUNT_ID" ]; then
    echo "  âš ï¸  NÃ£o foi possÃ­vel obter Account ID, pulando limpeza de IAM"
else
    echo "  ğŸ“Š Account ID: $ACCOUNT_ID"
    echo "  ğŸ” Lendo nomes reais das roles do Terraform state..."
    echo ""
    
    # ======================================================================
    # STACK 02 - EKS CLUSTER ROLES (lendo dinamicamente do state)
    # ======================================================================
    echo "  ğŸ—‚ï¸  Stack 02 - EKS Cluster"
    
    ROLE_CSI=$(get_role_name_from_state "02-eks-cluster" "aws_iam_role.container_storage_interface")
    ROLE_ALB=$(get_role_name_from_state "02-eks-cluster" "aws_iam_role.load_balancer_controller")
    ROLE_NODE=$(get_role_name_from_state "02-eks-cluster" "aws_iam_role.eks_cluster_node_group")
    ROLE_CLUSTER=$(get_role_name_from_state "02-eks-cluster" "aws_iam_role.eks_cluster")
    ROLE_DNS=$(get_role_name_from_state "02-eks-cluster" "aws_iam_role.external_dns")
    
    POLICY_ALB=$(get_policy_name_from_state "02-eks-cluster" "aws_iam_policy.load_balancer_controller")
    
    [ -n "$ROLE_CSI" ] && delete_iam_role "$ROLE_CSI" || delete_iam_role "AmazonEKS_EFS_CSI_DriverRole"
    [ -n "$ROLE_ALB" ] && delete_iam_role "$ROLE_ALB" || delete_iam_role "aws-load-balancer-controller"
    [ -n "$ROLE_NODE" ] && delete_iam_role "$ROLE_NODE" || delete_iam_role "eks-devopsproject-node-group-role"
    [ -n "$ROLE_CLUSTER" ] && delete_iam_role "$ROLE_CLUSTER" || delete_iam_role "eks-devopsproject-cluster-role"
    [ -n "$ROLE_DNS" ] && delete_iam_role "$ROLE_DNS" || delete_iam_role "external-dns-irsa-role"
    
    # Deletar policy ALB Controller
    if [ -n "$POLICY_ALB" ]; then
        POLICY_ARN="arn:aws:iam::${ACCOUNT_ID}:policy/${POLICY_ALB}"
    else
        POLICY_ARN="arn:aws:iam::${ACCOUNT_ID}:policy/AWSLoadBalancerControllerIAMPolicy"
    fi
    
    if aws iam get-policy --policy-arn "$POLICY_ARN" --profile terraform &>/dev/null; then
        echo "  ğŸ—‘ï¸  Deletando policy: $(basename $POLICY_ARN)"
        aws iam delete-policy --policy-arn "$POLICY_ARN" --profile terraform 2>/dev/null && \
            echo "    âœ… Policy deletada" || \
            echo "    âš ï¸  Policy nÃ£o pÃ´de ser deletada (pode estar attached)"
    fi
    echo ""
    
    # ======================================================================
    # STACK 03 - KARPENTER ROLES (lendo dinamicamente do state)
    # ======================================================================
    echo "  ğŸ—‚ï¸  Stack 03 - Karpenter"
    
    ROLE_KARPENTER=$(get_role_name_from_state "03-karpenter-auto-scaling" "aws_iam_role.karpenter_controller")
    POLICY_KARPENTER=$(get_policy_name_from_state "03-karpenter-auto-scaling" "aws_iam_policy.karpenter_controller")
    
    [ -n "$ROLE_KARPENTER" ] && delete_iam_role "$ROLE_KARPENTER" || delete_iam_role "KarpenterControllerRole"
    
    # Deletar policy Karpenter
    if [ -n "$POLICY_KARPENTER" ]; then
        POLICY_ARN="arn:aws:iam::${ACCOUNT_ID}:policy/${POLICY_KARPENTER}"
    else
        POLICY_ARN="arn:aws:iam::${ACCOUNT_ID}:policy/KarpenterControllerPolicy"
    fi
    
    if aws iam get-policy --policy-arn "$POLICY_ARN" --profile terraform &>/dev/null; then
        echo "  ğŸ—‘ï¸  Deletando policy: $(basename $POLICY_ARN)"
        aws iam delete-policy --policy-arn "$POLICY_ARN" --profile terraform 2>/dev/null && \
            echo "    âœ… Policy deletada" || \
            echo "    âš ï¸  Policy nÃ£o pÃ´de ser deletada"
    fi
    echo ""
    
    # ======================================================================
    # STACK 05 - MONITORING ROLES (lendo dinamicamente do state)
    # ======================================================================
    echo "  ğŸ—‚ï¸  Stack 05 - Monitoring"
    
    ROLE_GRAFANA=$(get_role_name_from_state "05-monitoring" "aws_iam_role.grafana")
    
    [ -n "$ROLE_GRAFANA" ] && delete_iam_role "$ROLE_GRAFANA" || delete_iam_role "GrafanaWorkspaceRole"
    echo ""
    
    echo "  âœ… Limpeza de IAM concluÃ­da (modo dinÃ¢mico v3.2)"
fi
echo ""

destroy_stack "Stack 01 - Networking (VPC)" "01-networking"

# Backend por Ãºltimo
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "ğŸ—‘ï¸  Destruindo: Stack 00 - Backend (S3 + DynamoDB)"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
read -p "âš ï¸  Destruir backend tambÃ©m? Isso removerÃ¡ o state remoto! (s/N): " destroy_backend

if [[ $destroy_backend =~ ^[Ss]$ ]]; then
    cd "$PROJECT_ROOT/00-backend"
    
    # Obter nome do bucket do terraform
    BUCKET_NAME=$(terraform output -raw s3_bucket_name 2>/dev/null)
    
    if [ -z "$BUCKET_NAME" ]; then
        echo "âš ï¸  NÃ£o foi possÃ­vel obter nome do bucket. Tentando detectar..."
        ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text 2>/dev/null)
        BUCKET_NAME="eks-devopsproject-state-files-${ACCOUNT_ID}"
        echo "  â†’ Bucket detectado: $BUCKET_NAME"
    fi
    
    echo "ğŸ§¹ Esvaziando bucket S3: $BUCKET_NAME"
    
    # Verificar se bucket existe antes de tentar esvaziar
    if aws s3 ls "s3://$BUCKET_NAME" --profile terraform &>/dev/null; then
        echo "  â†’ Removendo todos os objetos e versÃµes do bucket..."
        
        # MÃ©todo 1: Usar aws s3 rm com --recursive (mais simples e confiÃ¡vel)
        aws s3 rm "s3://$BUCKET_NAME" --recursive --profile terraform 2>/dev/null || true
        
        # MÃ©todo 2: Deletar versÃµes antigas (versionamento habilitado)
        echo "  â†’ Verificando versÃµes antigas..."
        VERSIONS=$(aws s3api list-object-versions \
            --bucket "$BUCKET_NAME" \
            --profile terraform \
            --output json \
            --query '{Objects: Versions[].{Key:Key,VersionId:VersionId}}' 2>/dev/null)
        
        if [ "$VERSIONS" != "null" ] && [ "$VERSIONS" != "" ] && [ "$VERSIONS" != "{}" ]; then
            echo "  â†’ Removendo versÃµes de objetos..."
            aws s3api delete-objects \
                --bucket "$BUCKET_NAME" \
                --profile terraform \
                --delete "$VERSIONS" 2>/dev/null || true
        fi
        
        # MÃ©todo 3: Deletar delete markers
        echo "  â†’ Verificando delete markers..."
        MARKERS=$(aws s3api list-object-versions \
            --bucket "$BUCKET_NAME" \
            --profile terraform \
            --output json \
            --query '{Objects: DeleteMarkers[].{Key:Key,VersionId:VersionId}}' 2>/dev/null)
        
        if [ "$MARKERS" != "null" ] && [ "$MARKERS" != "" ] && [ "$MARKERS" != "{}" ]; then
            echo "  â†’ Removendo delete markers..."
            aws s3api delete-objects \
                --bucket "$BUCKET_NAME" \
                --profile terraform \
                --delete "$MARKERS" 2>/dev/null || true
        fi
        
        echo "  âœ… Bucket esvaziado completamente"
    else
        echo "  â„¹ï¸  Bucket nÃ£o encontrado ou jÃ¡ foi deletado"
    fi
    echo ""
    
    # Agora destruir o backend (com force_destroy = true, mesmo se houver objetos restantes)
    terraform destroy -auto-approve
    echo "âœ… Stack 00 - Backend destruÃ­do"
else
    echo "â¸ï¸  Stack 00 - Backend preservado (state remoto mantido)"
fi
echo ""

echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘              âœ… DESTRUIÃ‡ÃƒO COMPLETA!                            â•‘"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
echo "ğŸ“Š Recursos destruÃ­dos:"
echo "  âœ… Namespace ecommerce + ALB (via kubectl)"
echo "  âœ… Namespace sample-app (se existia)"
echo "  âœ… kube-state-metrics (se existia)"
echo "  âœ… Stack 05: Grafana + Prometheus"
echo "  âœ… Stack 04: WAF Web ACL + Association"
echo "  âœ… Stack 03: Karpenter + IAM Roles + Resources"
echo "  âœ… Stack 02: EKS Cluster + Node Group + ALB Controller + External DNS"
echo "  âœ… Stack 01: VPC + Subnets + NAT Gateways + EIPs"
if [[ $destroy_backend =~ ^[Ss]$ ]]; then
echo "  âœ… Stack 00: Backend (S3 + DynamoDB)"
else
echo "  â¸ï¸  Stack 00: Backend preservado"
fi
echo ""
echo "ğŸ’° Custos AWS agora: ~$0/mÃªs"
if [[ ! $destroy_backend =~ ^[Ss]$ ]]; then
echo "   (S3 + DynamoDB do backend: <$1/mÃªs)"
fi
echo ""
echo "ğŸ”„ Para recriar tudo: ./rebuild-all.sh"
echo ""
